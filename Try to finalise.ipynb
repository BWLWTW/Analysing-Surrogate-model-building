{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcc4e5b-9e8a-41d7-833d-9c0d6bda0cef",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f24f54-ae70-460a-be8c-13724240b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "# import ML related libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from bayes_opt import BayesianOptimization\n",
    "# import plotting related libraries\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50080ecc-0feb-4117-91c8-20af33d435ac",
   "metadata": {},
   "source": [
    "# Things that we can change:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4546278b-7e2c-4cde-b87c-163225c35502",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapathway=r\"C:\\Users\\wingt\\Downloads\\S11_along_BD_205.csv\"\n",
    "xstart=3            #column of xstart\n",
    "xstop=5            #column of xend\n",
    "ystart=6           #column of ystart\n",
    "yname='Stress'      # name of the ultimate output array\n",
    "yparameter='Position'       #position here but it will be what is the parameter to be tuned\n",
    "xcolumn=['Travel_length', 'Welding_speed', 'Net_energy_input'] #name of inputs\n",
    "act_list=[tf.nn.relu,tf.nn.tanh,tf.nn.sigmoid,tf.nn.elu,tf.nn.leaky_relu,tf.nn.softsign,tf.nn.softplus,tf.nn.softmax]\n",
    "optimizer_search_list=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "optimizer_tuning_number_of_full_update=200\n",
    "optimizer_tuning_number_sample_used_in_one_training=10\n",
    "grid_search_tuning_other_parameter_tuning_dict= {'N_hidden_nodes':[100, 500, 1000, 1500, 2000], 'l_rate':[0.0001, 0.001, 0.01], 'Batch_size':[1, 8, 16, 32], 'epochs':[1000]}\n",
    "BO_turing_other_params_nn ={\n",
    "     'N_hidden_nodes': (10, 2000),\n",
    "     'l_rate':(0.0001, 0.01),\n",
    "     'Batch_size':(1, 64),\n",
    "     'epochs':(50, 1000),\n",
    " }\n",
    "no_iter_before_conclude=5\n",
    "no_iter_conclude=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9e405-21c9-448d-840c-d91ab062b096",
   "metadata": {},
   "source": [
    "### Get Plot\n",
    "It is a function plotting the actual simulation result and the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1dbb1-a89b-4729-9a5a-1304e83edd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(oringin_data, predict_data, i):\n",
    "    '''\n",
    "    Plot target outputs against predicted outputs\n",
    "    inputs:\n",
    "        oringin_data: target outputs\n",
    "        predict_data: predicted outputs\n",
    "        i: plot index\n",
    "        root: root for saving figure\n",
    "    '''\n",
    "    root = r'ANN_results/BD/test/test--' + str(i) + '.jpg'\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    x = y_label_new\n",
    "    oringin_data = oringin_data \n",
    "    predict_data = predict_data \n",
    "    p1 = ax.plot(x,oringin_data.ravel(),'r--', label = 'Target stress')\n",
    "    p2 = ax.plot(x,predict_data.ravel(),'g--',label = 'Predict stress')\n",
    "    ax.set_title(\"Test-Set\" + str(i))\n",
    "    # ax.set_xticks(x)\n",
    "    ax.set_ylabel('Logitudinal stress (MPa)')\n",
    "    ax.set_xlabel('Distance from top surface Z(mm)')\n",
    "    \n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_tick_params(direction='out')\n",
    "\n",
    "    ax2 = plt.twinx()\n",
    "    difference = predict_data.ravel() - oringin_data.ravel()\n",
    "    difference=np.array(list(map(lambda x,y:x/y,difference,oringin_data.ravel())))\n",
    "\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='red', lw=2, label='Simulation result'),\n",
    "                       Line2D([0], [0], color='green', lw=2, label='ANN prediction')\n",
    "                      ]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "    plt.savefig(root)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b65e3-ae89-435b-af42-5735d0a1b1a9",
   "metadata": {},
   "source": [
    "# Step 3: Tune the Optimizer\n",
    "a function to tune the hyperparameter of the ANN machine-> optimizer\n",
    "model used is from KerasRegressor\n",
    "Used gridsearch cross validation\n",
    "printing the best optimizer name and the score of this machine\n",
    "printing the mean_test_score, std_test_score, parameter for all the optimizer tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a886f-9fde-4b32-9623-e3ccb51f7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_optimizer():\n",
    "    # callbacks = [\n",
    "    #     EarlyStopping(monitor='mse', patience=100, verbose=0),\n",
    "    # ]\n",
    "    model = KerasRegressor(build_fn=create_model, nb_epoch=optimizer_tuning_number_of_full_update, batch_size=optimizer_tuning_number_sample_used_in_one_training, verbose=0) \n",
    "    optimizer = optimizer_search_list # 200 hidden nodes\n",
    "    param_grid = dict(optimizer=optimizer) \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], grid_result.cv_results_['std_test_score'][index], grid_result.cv_results_['params'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd070c15-5a69-4813-902d-46aa50c8a8ea",
   "metadata": {},
   "source": [
    "# Step 4: Tuning other parameters\n",
    "a function tuning the other hyperparameter, including number of hidden nodes, learning rate, batch size,\n",
    "set number of epochs to 1000\n",
    "still use GridSearch CV and print the same thing as step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c19638-ea6c-402d-86af-b234a58bd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_other_para():\n",
    "    model = KerasRegressor(build_fn=create_model_2, verbose=1) \n",
    "    param_grid = grid_search_tuning_other_parameter_tuning_dict\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], grid_result.cv_results_['std_test_score'][index], grid_result.cv_results_['params'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a220b9-eb7f-4b52-b372-a4d173b44752",
   "metadata": {},
   "source": [
    "### Plot_history\n",
    "plot learning curve\n",
    "make learning history(generated later after building a model) a dataframe and insert a column called epoch in it\n",
    "plot MSE against epoch\n",
    "plot val_mse (should be validation mse) against no of epoch (validation error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66632a-f4ca-415d-88b8-a403aaf91988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''\n",
    "    Plot learning curve using NN training history info\n",
    "    '''\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch',fontsize=20)\n",
    "    plt.ylabel('Mean Square Error [$MPG^2$]',fontsize=20)\n",
    "    plt.plot(hist['epoch'], hist['mse'],\n",
    "            label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mse'],\n",
    "            label = 'Val Error')\n",
    "    plt.ylim([0,0.4])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960f30b-0024-433a-b949-e115c73310e0",
   "metadata": {},
   "source": [
    "### import data + preprocessing(select the suitable columns)\n",
    "import the data using pd.read_csv\n",
    "only get columns required\n",
    "assign column names\n",
    "get y data for training as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383156d4-0b1b-4e58-a1a2-04050456de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(csv_file_name):\n",
    "    '''\n",
    "    used to import dataset and split training and test dataset\n",
    "    csv_file_name is the dateset root\n",
    "    reture traing and test datasets\n",
    "    '''\n",
    "    raw_data = pd.read_csv(csv_file_name,header=None).dropna()\n",
    "    x = raw_data.iloc[:,xstart:(xstop+1)]\n",
    "    #x.columns = xcolumn\n",
    "    y = raw_data.iloc[:,ystart:]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b25972-ff6a-480a-8333-82e6c0bd99e3",
   "metadata": {},
   "source": [
    "### Neural Network training function for Bayesian Optimisation\n",
    "use a function for defning the machine name and parameters for the machine\n",
    "use KerasRegressor to build the machine with specified hyperparameters\n",
    "use processed training input and output (will define function to make them later) for training the model\n",
    "history is the name of the model\n",
    "return a scoe showing cross validation mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee36b20-498f-4940-b0c3-6597f2f5c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cl_bo(N_hidden_nodes, l_rate, Batch_size, epochs):\n",
    "    '''\n",
    "    Using certain number of neurons, learning rate and Batch size\n",
    "    to train a NN and return its cross validation MSE\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    output:\n",
    "        score: cross validation MSE\n",
    "    '''\n",
    "    N_hidden_nodes = round(N_hidden_nodes)\n",
    "    Batch_size = round(Batch_size)\n",
    "    def nn_cl_fun():\n",
    "        nn = keras.Sequential([\n",
    "            keras.layers.Dense(N_hidden_nodes, activation=tf.nn.relu, input_shape=(4,)),\n",
    "        # keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        nn.compile(loss='mse',\n",
    "                    optimizer=tf.keras.optimizers.Adamax(learning_rate=l_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0),\n",
    "                    metrics=['mse'])\n",
    "        return nn\n",
    "    nn = KerasRegressor(build_fn=nn_cl_fun, epochs=epochs, batch_size=Batch_size,\n",
    "                        verbose=0)\n",
    "    history = nn.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=1000, \n",
    "                        verbose=0, validation_split=0.1)\n",
    "    score = -history.history['val_loss'][-1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60193e0-0d01-4b42-be77-8e605e8d7149",
   "metadata": {},
   "source": [
    "### function for creating a model for tuning optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fd0870-1d0b-4b17-846d-8e54132f641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(200, activation=tf.nn.leaky_relu, input_shape=(4,)),\n",
    "        keras.layers.Dense(21,activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de5226-6576-4d7e-a1ac-6a107ecacd7b",
   "metadata": {},
   "source": [
    "### function for creating a model for tuning other hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc9b3c-559f-433f-91bf-72262261a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(N_hidden_nodes, l_rate, Batch_size, epochs):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=tf.nn.leaky_relu, input_shape=(4,)),\n",
    "        keras.layers.Dense(21,activation='linear')\n",
    "    ])\n",
    "    optimizer=tf.keras.optimizers.Adamax(learning_rate=l_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=epochs, \n",
    "                        verbose=0, validation_split=0.1)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efea17f-28d9-4604-85fa-fb28389f159f",
   "metadata": {},
   "source": [
    "### function for creating a model for all tuned hyperparameter(actual model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880454b-cb7d-4bc7-b7c2-5e71d3c4abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(N_hidden_nodes, input_dim, N_outputs, l_rate, Batch_size, Epochs):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mse', patience=100, verbose=2),\n",
    "        ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=tf.nn.leaky_relu, input_shape=(input_dim,)),\n",
    "    # keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(N_outputs,activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=tf.keras.optimizers.Adamax(learning_rate=l_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0),\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=0.1, callbacks=callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b37c9-3461-40c4-97cd-c1ae358227a4",
   "metadata": {},
   "source": [
    "### Uniform Spaced Sampling(only for this dataset of weld residue stress, if we got other data, this might need to be deleted or modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389e884-c838-45ee-8257-7a4415d0555c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniformly_spaced_sampling(y_label, y_label_new, y):\n",
    "    '''\n",
    "    fit and interpolate\n",
    "    '''\n",
    "    f = interpolate.interp1d(y_label, y, kind='linear') # linear interpolation function use first row of y and all of y -> linear interpolate location and y value\n",
    "    ynew=pd.DataFrame(f(y_label_new)) #first row of y (linear distributed) -> linearly interpolate(predict) when in evenly distributed location, what r y\n",
    "    return ynew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fc473-f9e8-41bb-bef7-9f5f5551103e",
   "metadata": {},
   "source": [
    "### assign column names to data set \n",
    "can see only one column of y here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa92f38-e177-4227-a397-b9edc4145bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_melt(x, y):\n",
    "    '''\n",
    "    Add position as input\n",
    "    input:\n",
    "        x: old input \n",
    "        y: old output\n",
    "    output:\n",
    "        x_new: new input \n",
    "        y_new: new output\n",
    "    '''\n",
    "    y_label_str = [str(x) for x in y_label_new] # make a list of string of y_label_new\n",
    "    dataset = pd.concat([x, y],axis=1, ignore_index=True)\n",
    "    col_names = xcolumn + y_label_str\n",
    "    dataset.columns = col_names\n",
    "    dataset = dataset.melt(id_vars=xcolumn, \n",
    "        var_name=yparameter, \n",
    "        value_name=yname) #for given welding parameter, in a given location, the stress is in this dataset\n",
    "    x_new = dataset.iloc[:, 0:(len(xcolumn)+1)]\n",
    "    y_new = dataset.iloc[:, (len(xcolumn)+1)]\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cf4b0-f4b4-436f-9fb6-7c104aad0a55",
   "metadata": {},
   "source": [
    "### define mean square error function MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd031e64-35a7-4714-91ef-af04c4cb7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(testY, predicY):\n",
    "    '''\n",
    "    Get MSE fun\n",
    "    '''\n",
    "    MSE=np.sum(np.power((testY - predicY),2))/testY.shape[1]/testY.shape[0]\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c530b6-af40-49e7-8a66-fc18e24d4bf6",
   "metadata": {},
   "source": [
    "### Define a function getting number of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c15b5-d56e-41f5-a848-4241b0527302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_N_output(Y_train):\n",
    "    '''\n",
    "    Get the number of outputs\n",
    "    '''\n",
    "    if Y_train.ndim == 1:\n",
    "        N_outputs = 1\n",
    "    else:\n",
    "        N_outputs = Y_train.shape[1]\n",
    "    return N_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc98e1e2-4c77-42ba-8f2d-aa99cbec5b04",
   "metadata": {},
   "source": [
    "### Define a function getting y(output) label\n",
    "use y in the csv\n",
    "the first row of that csv is y_lab\n",
    "y_lab_new is min to max of y_lab in length of y_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d966a7f-cc89-4296-a3dd-d2bb2ccff1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_y_lable(root):\n",
    "    '''\n",
    "    Create an array for uniform interval depth \n",
    "    (Used biased mesh in simulaiton, so the sample point depth is not uniform) \n",
    "    input:\n",
    "        root: file root contains the sample point depth information\n",
    "    outputs:\n",
    "        y_lab: old sample point position array\n",
    "        y_lab_new: new sample point position array\n",
    "    '''\n",
    "    x, y = data_import(root)\n",
    "    y_lab = y.iloc[0, :]\n",
    "    y_lab_new = np.linspace(round(min(y_lab),2), \n",
    "                              round(max(y_lab),2), \n",
    "                              round(len(y_lab),2))\n",
    "    return y_lab, y_lab_new\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08708d5c-00d0-42ab-96fd-a8538a9a8119",
   "metadata": {},
   "source": [
    "### Prepocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272701b5-e0a1-4e41-a3b8-0d50ec6e04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(model_type, x, y):\n",
    "    '''\n",
    "    Data preprocessing (Uniformly spaced sampling, normalisation, train test split)\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)        \n",
    "        x: a dataframe of inputs of the whole dataset\n",
    "        y: a dataframe of outputs of the whole dataset\n",
    "    outputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        Y_test: Test output\n",
    "    '''\n",
    "      # uniformly_spaced_sampling\n",
    "    global y_label, y_label_new\n",
    "    y_label, y_label_new = get_y_lable(r'C:\\Users\\wingt\\Downloads\\benchmark_BD1.csv')\n",
    "    y = uniformly_spaced_sampling(y_label, y_label_new, y)\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x,y,test_size=1/125.0, random_state=3)\n",
    "    X_train=pd.DataFrame(X_train[:].values)\n",
    "    X_test =pd.DataFrame(X_test[:].values)\n",
    "    Y_train=pd.DataFrame(Y_train[:].values)\n",
    "    Y_test=pd.DataFrame(Y_test[:].values)\n",
    "    Y_train = uniformly_spaced_sampling(y_label, y_label_new, Y_train)\n",
    "    Y_test = uniformly_spaced_sampling(y_label, y_label_new, Y_test)\n",
    "    # data reconstruction\n",
    "    if 'ANN2' in model_type:\n",
    "        X_train, Y_train = data_melt(X_train, Y_train)\n",
    "        X_test, Y_test_1 = data_melt(X_test, Y_test)\n",
    "\n",
    "    # Normalization\n",
    "    global scaler_X, scaler_Y\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_Y = StandardScaler()\n",
    "    scaled_train_X = scaler_X.fit_transform(X_train.to_numpy())\n",
    "    scaled_test_X = scaler_X.transform(X_test)\n",
    "\n",
    "    if Y_train.ndim == 1:\n",
    "      Y_train = np.array(Y_train).reshape(-1,1)#column array\n",
    "    scaled_train_Y = scaler_Y.fit_transform(Y_train)\n",
    "    Proc_X_train = scaled_train_X\n",
    "    Proc_Y_train = scaled_train_Y\n",
    "    Proc_X_test = scaled_test_X\n",
    "\n",
    "    return model_type, Proc_X_train, Proc_Y_train, Proc_X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc516a76-61b1-41dd-8ac2-4a233df90024",
   "metadata": {},
   "source": [
    "### Get Result function\n",
    "build a model based on preprocessed data and tuned hyperparameter\n",
    "output the model, predicted scalar transformed y, predicted y in 21 columns, history of fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6656fb-1ba1-4c13-99c3-e138824b1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(model_type, Proc_X_train, Proc_Y_train, Proc_X_test, N_neurons, Batch_size, Epochs, l_rate):\n",
    "    \n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    N_inputs = Proc_X_train.shape[1]\n",
    "    N_outputs = get_N_output(Proc_Y_train)\n",
    "    model, history = build_model(N_neurons, N_inputs, N_outputs, l_rate, Batch_size, Epochs)\n",
    "    # Predict\n",
    "    \n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (21, -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16200cb-6212-474d-a2fb-be7d67b9a95f",
   "metadata": {},
   "source": [
    "### fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf2a92d-d8c3-4a8c-880b-6cb97d04439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_tensorflow(seed):\n",
    "    '''\n",
    "    Fix ramdom seed\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa94d0c-d4f3-4993-b24c-f0777935c519",
   "metadata": {},
   "source": [
    "### actual import and train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8350cc6-c3f8-45b0-9e48-195797e89d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix ramdom seed\n",
    "warnings.filterwarnings('ignore')\n",
    "seed_tensorflow(42)\n",
    "\n",
    "# Import dataset\n",
    "x, y = data_import(r'C:\\Users\\wingt\\Downloads\\S11_along_BD_205.csv')\n",
    "\n",
    "# Pre-processing\n",
    "model_type, Proc_X_train, Proc_Y_train, Proc_X_test, Y_test = pre_processing('ANN2', x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2ff88-ab80-4970-92cf-918ce37f5762",
   "metadata": {},
   "source": [
    "### function of tuning(timed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb09dc-5896-44c0-9098-2f91f6d89106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning(tuning=False): # Run Bayesian Optimization: find maximum value of unknown function by a few iteration\n",
    "    if tuning==True:\n",
    "        import time\n",
    "        t1=time.time\n",
    "        tune_optimizer()\n",
    "        t2=time.time\n",
    "        nn_bo = BayesianOptimization(nn_cl_bo, BO_turing_other_params_nn, random_state=5)\n",
    "        nn_bo.maximize(init_points=no_iter_before_conclude, n_iter=no_iter_conclude)\n",
    "        params_nn_ = nn_bo.max['params']\n",
    "        print(params_nn_)\n",
    "        print(nn_bo)\n",
    "        t3=time.time\n",
    "        tune_other_para()\n",
    "        t4=time.time\n",
    "        print('required Optimizer tuning time:',t2-t1)\n",
    "        print('required BO time:',t3-t2)\n",
    "        print('required Gridsearch others time:',t4-t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f0424-c30a-40cf-a7f5-56a3248b6b39",
   "metadata": {},
   "source": [
    "### The actual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb2c0eae-f666-48cd-aaa3-1e74bb1ac6d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2823809950.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 17\u001b[1;36m\u001b[0m\n\u001b[1;33m    get_plot(Y_test.iloc[i, :], predict_test[i], i)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# Build model and Predict\n",
    "tuning()\n",
    "ANN, predict_test_scal,predict_test, history = get_result(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,\n",
    "                                      1000, 8, 1000, 0.001)\n",
    "Y_test_scal = scaler_Y.transform(Y_test.values.reshape(-1,1)).reshape(-1, 21)\n",
    "MSE_R_scal = MSE(Y_test_scal, predict_test_scal)\n",
    "MSE_R = MSE(Y_test.values, predict_test)\n",
    "print(MSE_R_scal)\n",
    "print(MSE_R)\n",
    "plot_history(history)\n",
    "# save the model\n",
    "# ANN.save(r'data_analysis/saved_model/ANN.h5')\n",
    "# pickle.dump(scaler_X, open('scaler_X.pkl','wb'))\n",
    "# pickle.dump(scaler_Y, open('scaler_Y.pkl','wb'))\n",
    "\n",
    "#for i in range(len(predict_test)):\n",
    "get_plot(Y_test.iloc[i, :], predict_test[i], i)\n",
    "\n",
    "pred_results = []\n",
    "sim_results = []\n",
    "for i in range(len(predict_test)):\n",
    "    for j in range(len(predict_test[0])):\n",
    "        sim_results.append(Y_test.iloc[i, j])\n",
    "        pred_results.append(predict_test[i][j])\n",
    "\n",
    "max_results = max(max(pred_results,sim_results))\n",
    "min_results = min(min(pred_results,sim_results))\n",
    "fig,ax = plt.subplots(figsize=(12,12))\n",
    "ax.plot([min_results, max_results], [min_results, max_results],'b', linewidth=3)\n",
    "ax.scatter(pred_results, sim_results,s=10, c='r')\n",
    "ax.set_xlabel(\"Surrogate model predicted residual stress (MPa)\", fontsize=24)\n",
    "ax.set_ylabel(\"Finite element simulated residual stress (MPa)\", fontsize=24)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c4618cb4-2a9b-4640-b29b-6d5400441d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 200)               1000      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21)                4221      \n",
      "=================================================================\n",
      "Total params: 5,221\n",
      "Trainable params: 5,221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "427/427 [==============================] - 0s 664us/step - loss: 0.1822 - mse: 0.1822\n",
      "Best: -0.162955 using {'activation': 'relu'}\n",
      "-0.162955 (0.099280) with: {'activation': 'relu'}\n",
      "-0.191986 (0.142865) with: {'activation': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001B49BFA2850>}\n",
      "-0.372672 (0.322990) with: {'activation': 'hard_sigmoid'}\n",
      "-0.481071 (0.316380) with: {'activation': 'elu'}\n",
      "-0.483844 (0.313194) with: {'activation': 'linear'}\n",
      "-0.414365 (0.295534) with: {'activation': 'sigmoid'}\n",
      "-1.117318 (0.764708) with: {'activation': 'softmax'}\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LeakyReLU\n",
    "act_tun_number_sample_used_in_one_training=10\n",
    "act_tun_number_of_full_update=200\n",
    "activation_function=['relu',LeakyReLU(alpha=0.01),'hard_sigmoid','elu','linear','sigmoid','softmax']\n",
    "param_grid = dict(activation_function=activation_function)\n",
    "def tune_act_funct():\n",
    "    # callbacks = [\n",
    "    #     EarlyStopping(monitor='mse', patience=100, verbose=0),\n",
    "    # ]\n",
    "    model = KerasRegressor(build_fn=create_model_for_tune_act, \n",
    "                           nb_epoch=act_tun_number_of_full_update, \n",
    "                           batch_size=act_tun_number_sample_used_in_one_training, \n",
    "                           verbose=1) \n",
    "    activation = activation_function_list # 200 hidden nodes\n",
    "    param_grid = dict(activation=activation) \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], \n",
    "                                    grid_result.cv_results_['std_test_score'][index], \n",
    "                                    grid_result.cv_results_['params'][index]))\n",
    "def create_model_for_tune_act(activation='relu'):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(200, activation=activation, input_shape=(4,)),\n",
    "        keras.layers.Dense(21,activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer='adam',\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model        \n",
    "tune_act_funct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0b259-8ec0-44c0-af11-af94072ad797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
