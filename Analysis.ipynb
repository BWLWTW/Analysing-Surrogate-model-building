{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcc4e5b-9e8a-41d7-833d-9c0d6bda0cef",
   "metadata": {},
   "source": [
    "# Step 1: Import library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9e405-21c9-448d-840c-d91ab062b096",
   "metadata": {},
   "source": [
    "# Step 2: Get Plot\n",
    "It is a function plotting the actual simulation result and the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4f1dbb1-a89b-4729-9a5a-1304e83edd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(oringin_data, predict_data, i):\n",
    "    '''\n",
    "    Plot target outputs against predicted outputs\n",
    "    inputs:\n",
    "        oringin_data: target outputs\n",
    "        predict_data: predicted outputs\n",
    "        i: plot index\n",
    "        root: root for saving figure\n",
    "    '''\n",
    "    root = r'ANN_results/BD/test/test--' + str(i) + '.jpg'\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    x = y_label_new\n",
    "    oringin_data = oringin_data \n",
    "    predict_data = predict_data \n",
    "    p1 = ax.plot(x,oringin_data.ravel(),'r--', label = 'Target stress')\n",
    "    p2 = ax.plot(x,predict_data.ravel(),'g--',label = 'Predict stress')\n",
    "    ax.set_title(\"Test-Set\" + str(i))\n",
    "    # ax.set_xticks(x)\n",
    "    ax.set_ylabel('Logitudinal stress (MPa)')\n",
    "    ax.set_xlabel('Distance from top surface Z(mm)')\n",
    "    \n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_tick_params(direction='out')\n",
    "\n",
    "    ax2 = plt.twinx()\n",
    "    difference = predict_data.ravel() - oringin_data.ravel()\n",
    "    difference=np.array(list(map(lambda x,y:x/y,difference,oringin_data.ravel())))\n",
    "\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='red', lw=2, label='Simulation result'),\n",
    "                       Line2D([0], [0], color='green', lw=2, label='ANN prediction')\n",
    "                      ]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "    plt.savefig(root)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b65e3-ae89-435b-af42-5735d0a1b1a9",
   "metadata": {},
   "source": [
    "# Step 3: Tune the Optimizer\n",
    "a function to tune the hyperparameter of the ANN machine-> optimizer\n",
    "model used is from KerasRegressor\n",
    "Used gridsearch cross validation\n",
    "printing the best optimizer name and the score of this machine\n",
    "printing the mean_test_score, std_test_score, parameter for all the optimizer tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a886f-9fde-4b32-9623-e3ccb51f7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_optimizer():\n",
    "    # callbacks = [\n",
    "    #     EarlyStopping(monitor='mse', patience=100, verbose=0),\n",
    "    # ]\n",
    "    model = KerasRegressor(build_fn=create_model, nb_epoch=200, batch_size=10, verbose=0) \n",
    "    optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'] # 200 hidden nodes\n",
    "    param_grid = dict(optimizer=optimizer) \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], grid_result.cv_results_['std_test_score'][index], grid_result.cv_results_['params'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd070c15-5a69-4813-902d-46aa50c8a8ea",
   "metadata": {},
   "source": [
    "# Step 4: Tuning other parameters\n",
    "a function tuning the other hyperparameter, including number of hidden nodes, learning rate, batch size,\n",
    "set number of epochs to 1000\n",
    "still use GridSearch CV and print the same thing as step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c19638-ea6c-402d-86af-b234a58bd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_other_para():\n",
    "    model = KerasRegressor(build_fn=create_model_2, verbose=0) \n",
    "    param_grid = {'N_hidden_nodes':[100, 500, 1000, 1500, 2000], 'l_rate':[0.0001, 0.001, 0.01], 'Batch_size':[1, 8, 16, 32], 'epochs':[1000]}\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], grid_result.cv_results_['std_test_score'][index], grid_result.cv_results_['params'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a220b9-eb7f-4b52-b372-a4d173b44752",
   "metadata": {},
   "source": [
    "# Step 5: Plot_history\n",
    "plot learning curve\n",
    "make learning history(generated later after building a model) a dataframe and insert a column called epoch in it\n",
    "plot MSE against epoch\n",
    "plot val_mse (should be validation mse) against no of epoch (validation error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d66632a-f4ca-415d-88b8-a403aaf91988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''\n",
    "    Plot learning curve using NN training history info\n",
    "    '''\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$MPG^2$]')\n",
    "    plt.plot(hist['epoch'], hist['mse'],\n",
    "            label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mse'],\n",
    "            label = 'Val Error')\n",
    "    plt.ylim([0,0.4])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960f30b-0024-433a-b949-e115c73310e0",
   "metadata": {},
   "source": [
    "# Step 6: import data + preprocessing\n",
    "import the data using pd.read_csv\n",
    "only get columns required\n",
    "assign column names\n",
    "get y data for training as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383156d4-0b1b-4e58-a1a2-04050456de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(csv_file_name):\n",
    "    '''\n",
    "    used to import dataset and split training and test dataset\n",
    "    csv_file_name is the dateset root\n",
    "    reture traing and test datasets\n",
    "    '''\n",
    "    raw_data = pd.read_csv(csv_file_name,header=None).dropna()\n",
    "    x = raw_data.iloc[:,3:6]\n",
    "    # x.columns = ['Travel_length', 'Welding_speed', 'Net_energy_input']\n",
    "    y = raw_data.iloc[:,6:]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b25972-ff6a-480a-8333-82e6c0bd99e3",
   "metadata": {},
   "source": [
    "# Step 7: train a neural network using hyperparameter specified (we should have tuned them alr)\n",
    "preprocess input\n",
    "use a function for defning the machine name and parameters for the machine\n",
    "use KerasRegressor to build the machine with specified hyperparameters\n",
    "use processed training input and output (will define function to make them later) for training the model\n",
    "history is the name of the model\n",
    "return a scoe showing cross validation mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee36b20-498f-4940-b0c3-6597f2f5c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cl_bo(N_hidden_nodes, l_rate, Batch_size, epochs):\n",
    "    '''\n",
    "    Using certain number of neurons, learning rate and Batch size\n",
    "    to train a NN and return its cross validation MSE\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    output:\n",
    "        score: cross validation MSE\n",
    "    '''\n",
    "    N_hidden_nodes = round(N_hidden_nodes)\n",
    "    Batch_size = round(Batch_size)\n",
    "    def nn_cl_fun():\n",
    "        nn = keras.Sequential([\n",
    "            keras.layers.Dense(N_hidden_nodes, activation=tf.nn.relu, input_shape=(4,)),\n",
    "        # keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        nn.compile(loss='mse',\n",
    "                    optimizer=tf.keras.optimizers.Adamax(learning_rate=l_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0),\n",
    "                    metrics=['mse'])\n",
    "        return nn\n",
    "    nn = KerasRegressor(build_fn=nn_cl_fun, epochs=epochs, batch_size=Batch_size,\n",
    "                        verbose=0)\n",
    "    history = nn.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=1000, \n",
    "                        verbose=0, validation_split=0.1)\n",
    "    score = -history.history['val_loss'][-1]\n",
    "    return score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
