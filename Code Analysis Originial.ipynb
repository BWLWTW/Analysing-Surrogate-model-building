{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182cad17-fd1f-419c-995e-7bc748e616ac",
   "metadata": {},
   "source": [
    "# Origninal Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42c21e7-c272-4bdd-b264-1e0d74ea4d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "# import ML related libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from bayes_opt import BayesianOptimization\n",
    "# import plotting related libraries\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "\n",
    "def get_plot(oringin_data, predict_data, i):\n",
    "    '''\n",
    "    Plot target outputs against predicted outputs\n",
    "    inputs:\n",
    "        oringin_data: target outputs\n",
    "        predict_data: predicted outputs\n",
    "        i: plot index\n",
    "        root: root for saving figure\n",
    "    '''\n",
    "    root = r'ANN_results/BD/test/test--' + str(i) + '.jpg'\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    x = y_label_new\n",
    "    oringin_data = oringin_data \n",
    "    predict_data = predict_data \n",
    "    p1 = ax.plot(x,oringin_data.ravel(),'r--', label = 'Target stress')\n",
    "    p2 = ax.plot(x,predict_data.ravel(),'g--',label = 'Predict stress')\n",
    "    ax.set_title(\"Test-Set\" + str(i))\n",
    "    # ax.set_xticks(x)\n",
    "    ax.set_ylabel('Logitudinal stress (MPa)')\n",
    "    ax.set_xlabel('Distance from top surface Z(mm)')\n",
    "    \n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_tick_params(direction='out')\n",
    "\n",
    "    ax2 = plt.twinx()\n",
    "    difference = predict_data.ravel() - oringin_data.ravel()\n",
    "    difference=np.array(list(map(lambda x,y:x/y,difference,oringin_data.ravel())))\n",
    "\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='red', lw=2, label='Simulation result'),\n",
    "                       Line2D([0], [0], color='green', lw=2, label='ANN prediction')\n",
    "                      ]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "    plt.savefig(root)\n",
    "    plt.close()\n",
    "\n",
    "def tune_optimizer():\n",
    "    # callbacks = [\n",
    "    #     EarlyStopping(monitor='mse', patience=100, verbose=0),\n",
    "    # ]\n",
    "    model = KerasRegressor(build_fn=create_model, nb_epoch=200, batch_size=10, verbose=0) \n",
    "    optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam'] # 200 hidden nodes\n",
    "    param_grid = dict(optimizer=optimizer) \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], grid_result.cv_results_['std_test_score'][index], grid_result.cv_results_['params'][index]))\n",
    "\n",
    "def tune_other_para():\n",
    "    model = KerasRegressor(build_fn=create_model_2, verbose=0) \n",
    "    param_grid = {'N_hidden_nodes':[100, 500, 1000, 1500, 2000], 'l_rate':[0.0001, 0.001, 0.01], 'Batch_size':[1, 8, 16, 32], 'epochs':[1000]}\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], grid_result.cv_results_['std_test_score'][index], grid_result.cv_results_['params'][index]))\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    '''\n",
    "    Plot learning curve using NN training history info\n",
    "    '''\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Square Error [$MPG^2$]')\n",
    "    plt.plot(hist['epoch'], hist['mse'],\n",
    "            label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mse'],\n",
    "            label = 'Val Error')\n",
    "    plt.ylim([0,0.4])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def data_import(csv_file_name):\n",
    "    '''\n",
    "    used to import dataset and split training and test dataset\n",
    "    csv_file_name is the dateset root\n",
    "    reture traing and test datasets\n",
    "    '''\n",
    "    raw_data = pd.read_csv(csv_file_name,header=None).dropna()\n",
    "    x = raw_data.iloc[:,3:6]\n",
    "    # x.columns = ['Travel_length', 'Welding_speed', 'Net_energy_input']\n",
    "    y = raw_data.iloc[:,6:]\n",
    "    return x, y\n",
    "\n",
    "def nn_cl_bo(N_hidden_nodes, l_rate, Batch_size, epochs):\n",
    "    '''\n",
    "    Using certain number of neurons, learning rate and Batch size\n",
    "    to train a NN and return its cross validation MSE\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    output:\n",
    "        score: cross validation MSE\n",
    "    '''\n",
    "    N_hidden_nodes = round(N_hidden_nodes)\n",
    "    Batch_size = round(Batch_size)\n",
    "    def nn_cl_fun():\n",
    "        nn = keras.Sequential([\n",
    "            keras.layers.Dense(N_hidden_nodes, activation=tf.nn.relu, input_shape=(4,)),\n",
    "        # keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(1, activation='linear')\n",
    "        ])\n",
    "        nn.compile(loss='mse',\n",
    "                    optimizer=tf.keras.optimizers.Adamax(learning_rate=l_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0),\n",
    "                    metrics=['mse'])\n",
    "        return nn\n",
    "    nn = KerasRegressor(build_fn=nn_cl_fun, epochs=epochs, batch_size=Batch_size,\n",
    "                        verbose=0)\n",
    "    history = nn.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=1000, \n",
    "                        verbose=0, validation_split=0.1)\n",
    "    score = -history.history['val_loss'][-1]\n",
    "    return score\n",
    "\n",
    "def create_model(optimizer='adam'):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(200, activation=tf.nn.leaky_relu, input_shape=(4,)),\n",
    "        keras.layers.Dense(21,activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def create_model_2(N_hidden_nodes, l_rate, Batch_size, epochs):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=tf.nn.leaky_relu, input_shape=(4,)),\n",
    "        keras.layers.Dense(21,activation='linear')\n",
    "    ])\n",
    "    optimizer=tf.keras.optimizers.Adamax(learning_rate=l_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=epochs, \n",
    "                        verbose=0, validation_split=0.1)\n",
    "    return model\n",
    "\n",
    "def build_model(N_hidden_nodes, input_dim, N_outputs, l_rate, Batch_size, Epochs):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mse', patience=100, verbose=2),\n",
    "        ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=tf.nn.leaky_relu, input_shape=(input_dim,)),\n",
    "    # keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(N_outputs,activation='linear')\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=tf.keras.optimizers.Adamax(learning_rate=l_rate, beta_1=0.9, beta_2=0.99, epsilon=1e-08, decay=0.0),\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=0.1, callbacks=callbacks)\n",
    "    return model, history\n",
    "\n",
    "def uniformly_spaced_sampling(y_label, y_label_new, y):\n",
    "    '''\n",
    "    fit and interpolate\n",
    "    '''\n",
    "    f = interpolate.interp1d(y_label, y, kind='linear')\n",
    "    ynew=pd.DataFrame(f(y_label_new))\n",
    "    return ynew\n",
    "\n",
    "def data_melt(x, y):\n",
    "    '''\n",
    "    Add position as input\n",
    "    input:\n",
    "        x: old input \n",
    "        y: old output\n",
    "    output:\n",
    "        x_new: new input \n",
    "        y_new: new output\n",
    "    '''\n",
    "    y_label_str = [str(x) for x in y_label_new]\n",
    "    dataset = pd.concat([x, y],axis=1, ignore_index=True)\n",
    "    col_names = ['Travel_length', 'Welding_speed', 'Net_energy_input'] + y_label_str\n",
    "    dataset.columns = col_names\n",
    "    dataset = dataset.melt(id_vars=['Travel_length', 'Welding_speed', 'Net_energy_input'], \n",
    "        var_name=\"Position\", \n",
    "        value_name=\"Stress\")\n",
    "    x_new = dataset.iloc[:, 0:4]\n",
    "    y_new = dataset.iloc[:, 4]\n",
    "    return x_new, y_new\n",
    "\n",
    "def MSE(testY, predicY):\n",
    "    '''\n",
    "    Get MSE fun\n",
    "    '''\n",
    "    MSE=np.sum(np.power((testY - predicY),2))/testY.shape[1]/testY.shape[0]\n",
    "    return MSE\n",
    "\n",
    "def get_N_output(Y_train):\n",
    "    '''\n",
    "    Get the number of outputs\n",
    "    '''\n",
    "    if Y_train.ndim == 1:\n",
    "        N_outputs = 1\n",
    "    else:\n",
    "        N_outputs = Y_train.shape[1]\n",
    "    return N_outputs\n",
    "\n",
    "def get_y_lable(root):\n",
    "    '''\n",
    "    Create an array for uniform interval depth \n",
    "    (Used biased mesh in simulaiton, so the sample point depth is not uniform) \n",
    "    input:\n",
    "        root: file root contains the sample point depth information\n",
    "    outputs:\n",
    "        y_lab: old sample point position array\n",
    "        y_lab_new: new sample point position array\n",
    "    '''\n",
    "    x, y = data_import(root)\n",
    "    y_lab = y.iloc[0, :]\n",
    "    y_lab_new = np.linspace(round(min(y_lab),2), \n",
    "                              round(max(y_lab),2), \n",
    "                              round(len(y_lab),2))\n",
    "    return y_lab, y_lab_new\n",
    "    \n",
    "def pre_processing(model_type, x, y):\n",
    "    '''\n",
    "    Data preprocessing (Uniformly spaced sampling, normalisation, train test split)\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)        \n",
    "        x: a dataframe of inputs of the whole dataset\n",
    "        y: a dataframe of outputs of the whole dataset\n",
    "    outputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        Y_test: Test output\n",
    "    '''\n",
    "      # uniformly_spaced_sampling\n",
    "    global y_label, y_label_new\n",
    "    y_label, y_label_new = get_y_lable(r'extracted_data/benchmark_BD1.csv')\n",
    "    y = uniformly_spaced_sampling(y_label, y_label_new, y)\n",
    "\n",
    "    # Split dataset\n",
    "    # X_train, X_test, Y_train, Y_test = train_test_split(x,y,test_size=1/125.0, random_state=3)\n",
    "    X_train, Y_train = data_import(r'extracted_data/S11_along_BD_125.csv')\n",
    "    Y_train = uniformly_spaced_sampling(y_label, y_label_new, Y_train)\n",
    "    X_test, Y_test = data_import(r'extracted_data/S11_along_BD_80.csv')\n",
    "    Y_test = uniformly_spaced_sampling(y_label, y_label_new, Y_test)\n",
    "    # data reconstruction\n",
    "    if 'ANN2' in model_type:\n",
    "        X_train, Y_train = data_melt(X_train, Y_train)\n",
    "        X_test, Y_test_1 = data_melt(X_test, Y_test)\n",
    "\n",
    "    # Normalization\n",
    "    global scaler_X, scaler_Y\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_Y = StandardScaler()\n",
    "    scaled_train_X = scaler_X.fit_transform(X_train.to_numpy())\n",
    "    scaled_test_X = scaler_X.transform(X_test)\n",
    "\n",
    "    if Y_train.ndim == 1:\n",
    "      Y_train = np.array(Y_train).reshape(-1,1)\n",
    "    scaled_train_Y = scaler_Y.fit_transform(Y_train)\n",
    "    Proc_X_train = scaled_train_X\n",
    "    Proc_Y_train = scaled_train_Y\n",
    "    Proc_X_test = scaled_test_X\n",
    "\n",
    "    return model_type, Proc_X_train, Proc_Y_train, Proc_X_test, Y_test\n",
    "\n",
    "def get_result(model_type, Proc_X_train, Proc_Y_train, Proc_X_test, N_neurons, Batch_size, Epochs, l_rate):\n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    N_inputs = Proc_X_train.shape[1]\n",
    "    N_outputs = get_N_output(Proc_Y_train)\n",
    "    model, history = build_model(N_neurons, N_inputs, N_outputs, l_rate, Batch_size, Epochs)\n",
    "    # Predict\n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (21, -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history\n",
    "\n",
    "def seed_tensorflow(seed):\n",
    "    '''\n",
    "    Fix ramdom seed\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "# Fix ramdom seed\n",
    "warnings.filterwarnings('ignore')\n",
    "seed_tensorflow(42)\n",
    "\n",
    "# Import dataset\n",
    "x, y = data_import(r'extracted_data/S11_along_BD_205.csv')\n",
    "\n",
    "# Pre-processing\n",
    "model_type, Proc_X_train, Proc_Y_train, Proc_X_test, Y_test = pre_processing('ANN2', x, y)\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "# params_nn ={\n",
    "#     'N_hidden_nodes': (10, 2000),\n",
    "#     'l_rate':(0.0001, 0.01),\n",
    "#     'Batch_size':(1, 64),\n",
    "#     'epochs':(50, 1000),\n",
    "# }\n",
    "# print(nn_cl_bo(723, 0.000111, 27))\n",
    "# nn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=5)\n",
    "# nn_bo.maximize(init_points=60, n_iter=15)\n",
    "# params_nn_ = nn_bo.max['params']\n",
    "# print(nn_bo)\n",
    "\n",
    "# Grid search\n",
    "# tune_optimizer()\n",
    "# tune_other_para()\n",
    "\n",
    "# Build model and Predict\n",
    "ANN, predict_test_scal,predict_test, history = get_result(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,\n",
    "                                      1000, 8, 1000, 0.001)\n",
    "Y_test_scal = scaler_Y.transform(Y_test.values.reshape(-1,1)).reshape(-1, 21)\n",
    "MSE_R_scal = MSE(Y_test_scal, predict_test_scal)\n",
    "MSE_R = MSE(Y_test.values, predict_test)\n",
    "print(MSE_R_scal)\n",
    "print(MSE_R)\n",
    "# plot_history(history)\n",
    "\n",
    "# save the model\n",
    "# ANN.save(r'data_analysis/saved_model/ANN.h5')\n",
    "# pickle.dump(scaler_X, open('scaler_X.pkl','wb'))\n",
    "# pickle.dump(scaler_Y, open('scaler_Y.pkl','wb'))\n",
    "\n",
    "# for i in range(len(predict_test)):\n",
    "#     get_plot(Y_test.iloc[i, :], predict_test[i], i)\n",
    "\n",
    "pred_results = []\n",
    "sim_results = []\n",
    "for i in range(len(predict_test)):\n",
    "    for j in range(len(predict_test[0])):\n",
    "        sim_results.append(Y_test.iloc[i, j])\n",
    "        pred_results.append(predict_test[i][j])\n",
    "\n",
    "max_results = max(max(pred_results,sim_results))\n",
    "min_results = min(min(pred_results,sim_results))\n",
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot([min_results, max_results], [min_results, max_results],'b', linewidth=3)\n",
    "ax.scatter(pred_results, sim_results,s=10, c='r')\n",
    "ax.set_xlabel(\"Surrogate model predicted residual stress (MPa)\", fontsize=12)\n",
    "ax.set_ylabel(\"Finite element simulated residual stress (MPa)\", fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
