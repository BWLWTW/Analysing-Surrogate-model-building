{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfcc4e5b-9e8a-41d7-833d-9c0d6bda0cef",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91f24f54-ae70-460a-be8c-13724240b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "# import ML related libraries\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from bayes_opt import BayesianOptimization\n",
    "# import plotting related libraries\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interpolate\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50080ecc-0feb-4117-91c8-20af33d435ac",
   "metadata": {},
   "source": [
    "# Things that we can change:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4546278b-7e2c-4cde-b87c-163225c35502",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapathway=r\"C:\\Users\\wingt\\OneDrive\\Desktop\\DataForIPML\\dataset_1a_db2.csv\"\n",
    "xstart=1        #column of xstart\n",
    "xstop=5            #column of xend\n",
    "ystart=6 #column of ystart\n",
    "yname='Thermo-couple'      # name of the ultimate output array (HSCP value)\n",
    "yparameter='Near or Far'       #position here but it will be what is the parameter to be tuned (HSCP_name)\n",
    "xcolumn=['heat', 'speed', 'ra','rl','rv'] #name of inputs\n",
    "ycolumn=['Near Field Thermo-Couple'] #name of outputs (HSCP_name)\n",
    "optimizer_search_list=['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "optimizer_tuning_number_of_full_update=200\n",
    "optimizer_tuning_number_sample_used_in_one_training=10\n",
    "\n",
    "BO_turing_other_params_nn ={'learning_rate':list(np.linspace(0.0,1.0,21)),\n",
    "                                                 'momentum':list(np.linspace(0.0,10.0,21)),\n",
    "                                                 'nesterov':[True,False],\n",
    "                                                 'N_hidden_nodes':[100, 500, 1000, 1500, 2000],\n",
    "                                                 'Batch_size':[1, 8, 16, 32], \n",
    "                                                 'epochs':[1000,2000,3000]}\n",
    "no_iter_before_conclude=5\n",
    "no_iter_conclude=3\n",
    "nn_v_size=0.1\n",
    "act_tun_number_sample_used_in_one_training=10\n",
    "act_tun_number_of_full_update=200\n",
    "activation_list1=['relu',LeakyReLU(alpha=0.01),'hard_sigmoid','elu','linear','sigmoid','softmax']\n",
    "activation_list2=['relu',LeakyReLU(alpha=0.01),'hard_sigmoid','elu','linear','sigmoid','softmax']\n",
    "train_test_split_test_size=0.2\n",
    "validation_split=0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b9e405-21c9-448d-840c-d91ab062b096",
   "metadata": {},
   "source": [
    "### Get Plot\n",
    "It is a function plotting the actual simulation result and the prediction result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4f1dbb1-a89b-4729-9a5a-1304e83edd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(oringin_data, predict_data, i):\n",
    "    '''\n",
    "    Plot target outputs against predicted outputs\n",
    "    inputs:\n",
    "        oringin_data: target outputs\n",
    "        predict_data: predicted outputs\n",
    "        i: plot index\n",
    "        root: root for saving figure\n",
    "    '''\n",
    "    root = r'ANN_results/BD/test/test--' + str(i) + '.jpg'\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    x = y_label_new\n",
    "    oringin_data = oringin_data \n",
    "    predict_data = predict_data \n",
    "    p1 = ax.plot(x,oringin_data.ravel(),'r--', label = 'Target stress')\n",
    "    p2 = ax.plot(x,predict_data.ravel(),'g--',label = 'Predict stress')\n",
    "    ax.set_title(\"Test-Set\" + str(i))\n",
    "    # ax.set_xticks(x)\n",
    "    ax.set_ylabel('Logitudinal stress (MPa)')\n",
    "    ax.set_xlabel('Distance from top surface Z(mm)')\n",
    "    \n",
    "    ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n",
    "    ax.yaxis.set_tick_params(direction='out')\n",
    "\n",
    "    ax2 = plt.twinx()\n",
    "    difference = predict_data.ravel() - oringin_data.ravel()\n",
    "    difference=np.array(list(map(lambda x,y:x/y,difference,oringin_data.ravel())))\n",
    "\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='red', lw=2, label='Simulation result'),\n",
    "                       Line2D([0], [0], color='green', lw=2, label='ANN prediction')\n",
    "                      ]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "    plt.savefig(root)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35487277-42de-4f51-96d9-e442b84e55d0",
   "metadata": {},
   "source": [
    "### Function of tuning activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f28d3a3d-27f8-438a-9837-e201022d52ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_act_funct():\n",
    "    # callbacks = [\n",
    "    #     EarlyStopping(monitor='mse', patience=100, verbose=0),\n",
    "    # ]\n",
    "    model = KerasRegressor(build_fn=create_model_for_tune_act, \n",
    "                           nb_epoch=act_tun_number_of_full_update, \n",
    "                           batch_size=act_tun_number_sample_used_in_one_training, \n",
    "                           verbose=1)  # 200 hidden nodes\n",
    "    param_grid = dict(activation1=activation_list1, activation2=activation_list2) \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], \n",
    "                                    grid_result.cv_results_['std_test_score'][index], \n",
    "                                    grid_result.cv_results_['params'][index]))\n",
    "def create_model_for_tune_act(activation1='relu',activation2='relu'):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(200, activation=activation1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation=activation2)\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer='adam',\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98b65e3-ae89-435b-af42-5735d0a1b1a9",
   "metadata": {},
   "source": [
    "### Optimizer tuning function\n",
    "a function to tune the hyperparameter of the ANN machine-> optimizer\n",
    "model used is from KerasRegressor\n",
    "Used gridsearch cross validation\n",
    "printing the best optimizer name and the score of this machine\n",
    "printing the mean_test_score, std_test_score, parameter for all the optimizer tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fe4a886f-9fde-4b32-9623-e3ccb51f7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_optimizer():\n",
    "    # callbacks = [\n",
    "    #     EarlyStopping(monitor='mse', patience=100, verbose=0),\n",
    "    # ]\n",
    "    model = KerasRegressor(build_fn=create_model, nb_epoch=optimizer_tuning_number_of_full_update, batch_size=optimizer_tuning_number_sample_used_in_one_training, verbose=0) \n",
    "    optimizer = optimizer_search_list # 200 hidden nodes\n",
    "    param_grid = dict(optimizer=optimizer) \n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], grid_result.cv_results_['std_test_score'][index], grid_result.cv_results_['params'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60193e0-0d01-4b42-be77-8e605e8d7149",
   "metadata": {},
   "source": [
    "### function for creating a model for tuning optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f0fd0870-1d0b-4b17-846d-8e54132f641d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(optimizer='adam'):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(200, activation=BEST_ACT1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation=BEST_ACT2)\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b25972-ff6a-480a-8333-82e6c0bd99e3",
   "metadata": {},
   "source": [
    "### Neural Network training function for Bayesian Optimisation\n",
    "use a function for defning the machine name and parameters for the machine\n",
    "use KerasRegressor to build the machine with specified hyperparameters\n",
    "use processed training input and output (will define function to make them later) for training the model\n",
    "history is the name of the model\n",
    "return a scoe showing cross validation mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5ee36b20-498f-4940-b0c3-6597f2f5c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_cl_bo(N_hidden_nodes,learning_rate, Batch_size, epochs,momentum, nesterov):\n",
    "    '''\n",
    "    Using certain number of neurons, learning rate and Batch size\n",
    "    to train a NN and return its cross validation MSE\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    output:\n",
    "        score: cross validation MSE\n",
    "    '''\n",
    "    N_hidden_nodes = round(N_hidden_nodes)\n",
    "    Batch_size = round(Batch_size)\n",
    "    def nn_cl_fun():\n",
    "        nn = keras.Sequential([\n",
    "            keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        # keras.layers.Dropout(0.5),\n",
    "            keras.layers.Dense(get_N_output(Proc_Y_train), activation=BEST_ACT2)\n",
    "        ])\n",
    "        nn.compile(loss='mse',\n",
    "                    optimizer=keras.optimizers.SGD(learning_rate=learning_rate,momentum=momentum,nesterov=nesterov),\n",
    "                    metrics=['mse'])\n",
    "        return nn\n",
    "    nn = KerasRegressor(build_fn=nn_cl_fun, epochs=epochs, batch_size=Batch_size,\n",
    "                        verbose=0)\n",
    "    history = nn.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=epochs, \n",
    "                        verbose=0, validation_split=nn_v_size)\n",
    "    score = -history.history['val_loss'][-1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd070c15-5a69-4813-902d-46aa50c8a8ea",
   "metadata": {},
   "source": [
    "### Other hyperparameters tuning function\n",
    "a function tuning the other hyperparameter, including number of hidden nodes, learning rate, batch size,\n",
    "set number of epochs to 1000\n",
    "still use GridSearch CV and print the same thing as step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c3c19638-ea6c-402d-86af-b234a58bd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_other_para():\n",
    "    model = KerasRegressor(build_fn=create_model_2, verbose=2) \n",
    "    param_grid = grid_search_tuning_other_parameter_tuning_dict\n",
    "    grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=10,verbose=10)\n",
    "    grid_result = grid.fit(Proc_X_train, Proc_Y_train)\n",
    "    print(param_grid)\n",
    "    print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "    #for index, value in enumerate(grid_result.cv_results_['mean_test_score']):\n",
    "        #print(\"%f (%f) with: %r\" % (grid_result.cv_results_['mean_test_score'][index], grid_result.cv_results_['std_test_score'][index], grid_result.cv_results_['params'][index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de5226-6576-4d7e-a1ac-6a107ecacd7b",
   "metadata": {},
   "source": [
    "### function for creating a model for tuning other hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2adc9b3c-559f-433f-91bf-72262261a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_2(N_hidden_nodes,learning_rate, Batch_size, epochs,momentum, nesterov):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT1, input_shape=(Proc_X_train.shape[1],)),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation=BEST_ACT2)\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=learning_rate,momentum=momentum,nesterov=nesterov),\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=epochs, \n",
    "                        verbose=2, validation_split=validation_split)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a220b9-eb7f-4b52-b372-a4d173b44752",
   "metadata": {},
   "source": [
    "### Plot_history\n",
    "plot learning curve\n",
    "make learning history(generated later after building a model) a dataframe and insert a column called epoch in it\n",
    "plot MSE against epoch\n",
    "plot val_mse (should be validation mse) against no of epoch (validation error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d66632a-f4ca-415d-88b8-a403aaf91988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    '''\n",
    "    Plot learning curve using NN training history info\n",
    "    '''\n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.xlabel('Epoch',fontsize=20)\n",
    "    plt.ylabel('Mean Square Error [$MPG^2$]',fontsize=20)\n",
    "    plt.plot(hist['epoch'], hist['mse'],\n",
    "            label='Train Error')\n",
    "    plt.plot(hist['epoch'], hist['val_mse'],\n",
    "            label = 'Val Error')\n",
    "\n",
    "    plt.ylim([0,10*10**-4])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960f30b-0024-433a-b949-e115c73310e0",
   "metadata": {},
   "source": [
    "### import data + preprocessing(select the suitable columns)\n",
    "import the data using pd.read_csv\n",
    "only get columns required\n",
    "assign column names\n",
    "get y data for training as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "383156d4-0b1b-4e58-a1a2-04050456de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_import(csv_file_name):\n",
    "    '''\n",
    "    used to import dataset and split training and test dataset\n",
    "    csv_file_name is the dateset root\n",
    "    reture traing and test datasets\n",
    "    '''\n",
    "    raw_data = pd.read_csv(csv_file_name,header=None).dropna()\n",
    "    x = raw_data.iloc[:,xstart:(xstop+1)].astype(float)\n",
    "    y = raw_data.iloc[:,ystart:].astype(float)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efea17f-28d9-4604-85fa-fb28389f159f",
   "metadata": {},
   "source": [
    "### function for creating a model for all tuned hyperparameter(actual model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e880454b-cb7d-4bc7-b7c2-5e71d3c4abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(N_hidden_nodes,learning_rate, Batch_size, epochs,momentum, nesterov):\n",
    "    '''\n",
    "    Build a NN\n",
    "    inputs:\n",
    "        N_hidden_nodes: number of neurons\n",
    "        input_dim: number of inputs\n",
    "        N_outputs: number of outputs\n",
    "        l_rate: learning rate\n",
    "        Batch_size: Batch size\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        history: training history\n",
    "    '''\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_mse', patience=100, verbose=2),\n",
    "        ModelCheckpoint('best_model.h5', monitor='val_mse', save_best_only=True, verbose=0)\n",
    "    ]\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(N_hidden_nodes, activation=BEST_ACT1, input_shape=(Proc_X_train.shape[1],)),\n",
    "    # keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(get_N_output(Proc_Y_train),activation=BEST_ACT2)\n",
    "    ])\n",
    "    model.compile(loss='mse',\n",
    "                optimizer=keras.optimizers.SGD(learning_rate=learning_rate,momentum=momentum,nesterov=nesterov),\n",
    "                metrics=['mse'])\n",
    "    model.summary()\n",
    "    history = model.fit(Proc_X_train, Proc_Y_train, batch_size=Batch_size, epochs=Epochs, \n",
    "                        verbose=1, validation_split=validation_split, callbacks=callbacks)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fc473-f9e8-41bb-bef7-9f5f5551103e",
   "metadata": {},
   "source": [
    "### assign column names to data set \n",
    "can see only one column of y here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3aa92f38-e177-4227-a397-b9edc4145bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_melt(x, y):\n",
    "    '''\n",
    "    Add position as input\n",
    "    input:\n",
    "        x: old input \n",
    "        y: old output\n",
    "    output:\n",
    "        x_new: new input \n",
    "        y_new: new output\n",
    "    '''\n",
    "    y_label_str = [str(x) for x in ycolumn] # make a list of string of y_label_new\n",
    "    dataset = pd.concat([x, y],axis=1, ignore_index=True)\n",
    "    col_names = xcolumn + y_label_str\n",
    "    dataset.columns = col_names\n",
    "    dataset = dataset.melt(id_vars=xcolumn, \n",
    "        var_name=yparameter, \n",
    "        value_name=yname) #for given welding parameter, in a given location, the stress is in this dataset\n",
    "    x_new = dataset.iloc[:, 0:(len(xcolumn)+1)]\n",
    "    y_new = dataset.iloc[:, (len(xcolumn)+1)]\n",
    "    print('DATASET')\n",
    "    print(dataset)\n",
    "    return x_new, y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cf4b0-f4b4-436f-9fb6-7c104aad0a55",
   "metadata": {},
   "source": [
    "### define mean square error function MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cd031e64-35a7-4714-91ef-af04c4cb7dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(testY, predicY):\n",
    "    '''\n",
    "    Get MSE fun\n",
    "    '''\n",
    "    MSE=np.sum(np.power((testY - predicY),2))/testY.shape[1]/testY.shape[0]\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c530b6-af40-49e7-8a66-fc18e24d4bf6",
   "metadata": {},
   "source": [
    "### Define a function getting number of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f04c15b5-d56e-41f5-a848-4241b0527302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_N_output(Y_train):\n",
    "    '''\n",
    "    Get the number of outputs\n",
    "    '''\n",
    "    if Y_train.ndim == 1:\n",
    "        N_outputs = 1\n",
    "    else:\n",
    "        N_outputs = Y_train.shape[1]\n",
    "    return N_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08708d5c-00d0-42ab-96fd-a8538a9a8119",
   "metadata": {},
   "source": [
    "### Prepocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "272701b5-e0a1-4e41-a3b8-0d50ec6e04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(model_type, x, y):\n",
    "    '''\n",
    "    Data preprocessing (Uniformly spaced sampling, normalisation, train test split)\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)        \n",
    "        x: a dataframe of inputs of the whole dataset\n",
    "        y: a dataframe of outputs of the whole dataset\n",
    "    outputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        Y_test: Test output\n",
    "    '''\n",
    "    # Split dataset\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(x,y,test_size=train_test_split_test_size, random_state=3)\n",
    "    X_train=pd.DataFrame(X_train[:].values)\n",
    "    X_test =pd.DataFrame(X_test[:].values)\n",
    "    Y_train=pd.DataFrame(Y_train[:].values)\n",
    "    Y_test=pd.DataFrame(Y_test[:].values)\n",
    "    # data reconstruction\n",
    "    if 'ANN2' in model_type:\n",
    "        X_train, Y_train = data_melt(X_train, Y_train)\n",
    "        X_test, Y_test_1 = data_melt(X_test, Y_test)\n",
    "\n",
    "    # Normalization\n",
    "    global scaler_X, scaler_Y\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_Y = StandardScaler()\n",
    "    scaled_train_X = scaler_X.fit_transform(X_train.to_numpy())\n",
    "    scaled_test_X = scaler_X.transform(X_test)\n",
    "\n",
    "    if Y_train.ndim == 1:\n",
    "      Y_train = np.array(Y_train).reshape(-1,1)#column array\n",
    "    scaled_train_Y = scaler_Y.fit_transform(Y_train)\n",
    "    Proc_X_train = scaled_train_X\n",
    "    Proc_Y_train = scaled_train_Y\n",
    "    Proc_X_test = scaled_test_X\n",
    "\n",
    "    return model_type, Proc_X_train, Proc_Y_train, Proc_X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc516a76-61b1-41dd-8ac2-4a233df90024",
   "metadata": {},
   "source": [
    "### Get Result function\n",
    "build a model based on preprocessed data and tuned hyperparameter\n",
    "output the model, predicted scalar transformed y, predicted y in 21 columns, history of fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fa6656fb-1ba1-4c13-99c3-e138824b1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(model_type, Proc_X_train, Proc_Y_train, Proc_X_test, N_hidden_nodes,learning_rate, Batch_size, epochs,momentum, nesterov):\n",
    "    \n",
    "    '''\n",
    "    Get the trained model, prediction results, and NN training history\n",
    "    inputs:\n",
    "        model_type: 'ANN1' the first architecture (3 inputs and 21 outputs)\n",
    "                    'ANN2' the second architecture (4 inputs and 1 output)\n",
    "        Proc_X_train: Processed Training input\n",
    "        Proc_Y_train: Processed Training output\n",
    "        Proc_X_test: Processed Test input\n",
    "        N_neurons: Number of Neurons in the hidden layer\n",
    "        l_rate: Learning rate\n",
    "    outputs:\n",
    "        model: Trained model\n",
    "        predict_test: predicted results\n",
    "        history: training history\n",
    "    '''\n",
    "    #build model\n",
    "    model, history = build_model(N_hidden_nodes,learning_rate, Batch_size, epochs,momentum, nesterov)\n",
    "    # Predict\n",
    "    \n",
    "    predict_test_scal = model.predict(Proc_X_test)\n",
    "    if 'ANN2' in model_type:\n",
    "      predict_test_scal = np.reshape(predict_test_scal, (get_N_output(Y_test), -1)).T\n",
    "    \n",
    "    predict_test = scaler_Y.inverse_transform(predict_test_scal)\n",
    "    return  model, predict_test_scal, predict_test, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16200cb-6212-474d-a2fb-be7d67b9a95f",
   "metadata": {},
   "source": [
    "### fix random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0cf2a92d-d8c3-4a8c-880b-6cb97d04439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_tensorflow(seed):\n",
    "    '''\n",
    "    Fix ramdom seed\n",
    "    '''\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa94d0c-d4f3-4993-b24c-f0777935c519",
   "metadata": {},
   "source": [
    "### actual import and train test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c8350cc6-c3f8-45b0-9e48-195797e89d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(583, 5) \n",
      " (583, 1) \n",
      " (146, 5) \n",
      " (146, 1)\n"
     ]
    }
   ],
   "source": [
    "# Fix ramdom seed\n",
    "warnings.filterwarnings('ignore')\n",
    "seed_tensorflow(42)\n",
    "\n",
    "# Import dataset\n",
    "x, y = data_import(datapathway)\n",
    "\n",
    "# Pre-processing\n",
    "model_type, Proc_X_train, Proc_Y_train, Proc_X_test, Y_test = pre_processing('ANN1', x, y)\n",
    "print(Proc_X_train.shape,'\\n', Proc_Y_train.shape,'\\n',Proc_X_test.shape,'\\n', Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bf593e-aec2-4061-bac3-0d94c785c662",
   "metadata": {},
   "source": [
    "### Activation Function Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ec3be75f-79ac-4359-afe7-35901db6362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 200)               1200      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,401\n",
      "Trainable params: 1,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "59/59 [==============================] - 0s 655us/step - loss: 0.1623 - mse: 0.1623\n",
      "Best: -0.008311 using {'activation1': 'linear', 'activation2': 'linear'}\n",
      "-0.543606 (0.097375) with: {'activation1': 'relu', 'activation2': 'relu'}\n",
      "-0.522110 (0.085466) with: {'activation1': 'relu', 'activation2': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12AF0>}\n",
      "-0.840930 (0.110785) with: {'activation1': 'relu', 'activation2': 'hard_sigmoid'}\n",
      "-0.217840 (0.057100) with: {'activation1': 'relu', 'activation2': 'elu'}\n",
      "-0.107732 (0.045179) with: {'activation1': 'relu', 'activation2': 'linear'}\n",
      "-0.855602 (0.129402) with: {'activation1': 'relu', 'activation2': 'sigmoid'}\n",
      "-2.000246 (0.132473) with: {'activation1': 'relu', 'activation2': 'softmax'}\n",
      "-0.537339 (0.060331) with: {'activation1': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12F40>, 'activation2': 'relu'}\n",
      "-0.519228 (0.090637) with: {'activation1': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12F40>, 'activation2': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12AF0>}\n",
      "-0.834740 (0.127382) with: {'activation1': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12F40>, 'activation2': 'hard_sigmoid'}\n",
      "-0.219950 (0.048595) with: {'activation1': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12F40>, 'activation2': 'elu'}\n",
      "-0.116199 (0.031246) with: {'activation1': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12F40>, 'activation2': 'linear'}\n",
      "-0.855026 (0.126043) with: {'activation1': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12F40>, 'activation2': 'sigmoid'}\n",
      "-2.000246 (0.132473) with: {'activation1': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12F40>, 'activation2': 'softmax'}\n",
      "-0.941271 (0.150222) with: {'activation1': 'hard_sigmoid', 'activation2': 'relu'}\n",
      "-0.926468 (0.191539) with: {'activation1': 'hard_sigmoid', 'activation2': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12AF0>}\n",
      "-0.945276 (0.148093) with: {'activation1': 'hard_sigmoid', 'activation2': 'hard_sigmoid'}\n",
      "-0.633385 (0.098581) with: {'activation1': 'hard_sigmoid', 'activation2': 'elu'}\n",
      "-0.638154 (0.102902) with: {'activation1': 'hard_sigmoid', 'activation2': 'linear'}\n",
      "-0.982237 (0.156374) with: {'activation1': 'hard_sigmoid', 'activation2': 'sigmoid'}\n",
      "-2.000246 (0.132473) with: {'activation1': 'hard_sigmoid', 'activation2': 'softmax'}\n",
      "-0.499900 (0.075111) with: {'activation1': 'elu', 'activation2': 'relu'}\n",
      "-0.484452 (0.082033) with: {'activation1': 'elu', 'activation2': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12AF0>}\n",
      "-0.705568 (0.094095) with: {'activation1': 'elu', 'activation2': 'hard_sigmoid'}\n",
      "-0.100878 (0.036748) with: {'activation1': 'elu', 'activation2': 'elu'}\n",
      "-0.008694 (0.001272) with: {'activation1': 'elu', 'activation2': 'linear'}\n",
      "-0.715659 (0.096404) with: {'activation1': 'elu', 'activation2': 'sigmoid'}\n",
      "-2.000246 (0.132473) with: {'activation1': 'elu', 'activation2': 'softmax'}\n",
      "-0.493390 (0.085991) with: {'activation1': 'linear', 'activation2': 'relu'}\n",
      "-0.485042 (0.082022) with: {'activation1': 'linear', 'activation2': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12AF0>}\n",
      "-0.679098 (0.092333) with: {'activation1': 'linear', 'activation2': 'hard_sigmoid'}\n",
      "-0.098054 (0.030601) with: {'activation1': 'linear', 'activation2': 'elu'}\n",
      "-0.008311 (0.003225) with: {'activation1': 'linear', 'activation2': 'linear'}\n",
      "-0.718862 (0.096325) with: {'activation1': 'linear', 'activation2': 'sigmoid'}\n",
      "-2.000246 (0.132473) with: {'activation1': 'linear', 'activation2': 'softmax'}\n",
      "-0.905632 (0.259746) with: {'activation1': 'sigmoid', 'activation2': 'relu'}\n",
      "-0.922518 (0.161067) with: {'activation1': 'sigmoid', 'activation2': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12AF0>}\n",
      "-0.917001 (0.152117) with: {'activation1': 'sigmoid', 'activation2': 'hard_sigmoid'}\n",
      "-0.621603 (0.091151) with: {'activation1': 'sigmoid', 'activation2': 'elu'}\n",
      "-0.615090 (0.144148) with: {'activation1': 'sigmoid', 'activation2': 'linear'}\n",
      "-0.970360 (0.153380) with: {'activation1': 'sigmoid', 'activation2': 'sigmoid'}\n",
      "-2.000246 (0.132473) with: {'activation1': 'sigmoid', 'activation2': 'softmax'}\n",
      "-0.996784 (0.159584) with: {'activation1': 'softmax', 'activation2': 'relu'}\n",
      "-0.990998 (0.155302) with: {'activation1': 'softmax', 'activation2': <tensorflow.python.keras.layers.advanced_activations.LeakyReLU object at 0x000001F6BAB12AF0>}\n",
      "-1.231117 (0.146076) with: {'activation1': 'softmax', 'activation2': 'hard_sigmoid'}\n",
      "-0.989892 (0.160295) with: {'activation1': 'softmax', 'activation2': 'elu'}\n",
      "-0.988238 (0.159841) with: {'activation1': 'softmax', 'activation2': 'linear'}\n",
      "-1.230107 (0.146934) with: {'activation1': 'softmax', 'activation2': 'sigmoid'}\n",
      "-2.000246 (0.132473) with: {'activation1': 'softmax', 'activation2': 'softmax'}\n"
     ]
    }
   ],
   "source": [
    "tune_act_funct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb063f-cf9f-427c-ac63-3dfa18387281",
   "metadata": {},
   "source": [
    "### Optimizer Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "81b492e5-4d30-47c9-af4e-814052632599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 200)               1200      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 1,401\n",
      "Trainable params: 1,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Best: -0.004913 using {'optimizer': 'SGD'}\n",
      "-0.004913 (0.001021) with: {'optimizer': 'SGD'}\n",
      "-0.005992 (0.001835) with: {'optimizer': 'RMSprop'}\n",
      "-0.474915 (0.115476) with: {'optimizer': 'Adagrad'}\n",
      "-1.195776 (0.434279) with: {'optimizer': 'Adadelta'}\n",
      "-0.008322 (0.000881) with: {'optimizer': 'Adam'}\n",
      "-0.037297 (0.032593) with: {'optimizer': 'Adamax'}\n",
      "-0.017697 (0.019093) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "BEST_ACT1='linear'\n",
    "BEST_ACT2='linear'\n",
    "tune_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a2ff88-ab80-4970-92cf-918ce37f5762",
   "metadata": {},
   "source": [
    "### function of tuning other parameter (timed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb09dc-5896-44c0-9098-2f91f6d89106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuningHyper(tuningh=False): # Run Bayesian Optimization: find maximum value of unknown function by a few iteration\n",
    "    if tuningh==True:\n",
    "        import time\n",
    "        t1=time.time\n",
    "        nn_bo = BayesianOptimization(nn_cl_bo, BO_turing_other_params_nn, random_state=5)\n",
    "        nn_bo.maximize(init_points=no_iter_before_conclude, n_iter=no_iter_conclude)\n",
    "        params_nn_ = nn_bo.max['params']\n",
    "        print(params_nn_)\n",
    "        print(nn_bo)\n",
    "        t2=time.time\n",
    "        tune_other_para()\n",
    "        t3=time.time\n",
    "        print('required BO time:',t2-t1)\n",
    "        print('required Gridsearch others time:',t3-t2)\n",
    "tuningHyper(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b9c46b-587d-4ed1-b876-ba3cfa81e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "NHN=100 #, 500, 1000, 1500, 2000\n",
    "BS=1 #8,16,32\n",
    "EEE=1000 #2000,3000\n",
    "grid_search_tuning_other_parameter_tuning_dict= {'learning_rate':[0.0,0.05], #,0.1,0.15,0.2]\n",
    "                                                 'momentum':[0.0,0.2],#0.4,0.6,0.8,1.0]\n",
    "                                                 'nesterov':[True,False],\n",
    "                                                 'N_hidden_nodes':[NHN],\n",
    "                                                 'Batch_size':[BS], \n",
    "                                                 'epochs':[EEE]}\n",
    "tune_other_para()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f0424-c30a-40cf-a7f5-56a3248b6b39",
   "metadata": {},
   "source": [
    "### The actual implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fb2c0eae-f666-48cd-aaa3-1e74bb1ac6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 1000)              6000      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 1001      \n",
      "=================================================================\n",
      "Total params: 7,001\n",
      "Trainable params: 7,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2304 - mse: 0.2304 - val_loss: 0.0050 - val_mse: 0.0050\n",
      "Epoch 2/1000\n",
      "66/66 [==============================] - 0s 619us/step - loss: 0.0025 - mse: 0.0025 - val_loss: 0.0013 - val_mse: 0.0013\n",
      "Epoch 3/1000\n",
      "66/66 [==============================] - 0s 606us/step - loss: 6.3793e-04 - mse: 6.3793e-04 - val_loss: 4.5565e-04 - val_mse: 4.5565e-04\n",
      "Epoch 4/1000\n",
      "66/66 [==============================] - 0s 532us/step - loss: 3.9436e-04 - mse: 3.9436e-04 - val_loss: 4.7880e-04 - val_mse: 4.7880e-04\n",
      "Epoch 5/1000\n",
      "66/66 [==============================] - 0s 642us/step - loss: 3.2794e-04 - mse: 3.2794e-04 - val_loss: 3.7622e-04 - val_mse: 3.7622e-04\n",
      "Epoch 6/1000\n",
      "66/66 [==============================] - 0s 633us/step - loss: 3.7715e-04 - mse: 3.7715e-04 - val_loss: 3.0076e-04 - val_mse: 3.0076e-04\n",
      "Epoch 7/1000\n",
      "66/66 [==============================] - 0s 484us/step - loss: 3.7098e-04 - mse: 3.7098e-04 - val_loss: 5.9584e-04 - val_mse: 5.9584e-04\n",
      "Epoch 8/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 3.4613e-04 - mse: 3.4613e-04 - val_loss: 4.4355e-04 - val_mse: 4.4355e-04\n",
      "Epoch 9/1000\n",
      "66/66 [==============================] - 0s 498us/step - loss: 1.9886e-04 - mse: 1.9886e-04 - val_loss: 3.0130e-04 - val_mse: 3.0130e-04\n",
      "Epoch 10/1000\n",
      "66/66 [==============================] - 0s 560us/step - loss: 2.7669e-04 - mse: 2.7669e-04 - val_loss: 2.0819e-04 - val_mse: 2.0819e-04\n",
      "Epoch 11/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 2.0984e-04 - mse: 2.0984e-04 - val_loss: 3.8817e-04 - val_mse: 3.8817e-04\n",
      "Epoch 12/1000\n",
      "66/66 [==============================] - 0s 481us/step - loss: 2.2939e-04 - mse: 2.2939e-04 - val_loss: 2.1321e-04 - val_mse: 2.1321e-04\n",
      "Epoch 13/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 1.9108e-04 - mse: 1.9108e-04 - val_loss: 3.1384e-04 - val_mse: 3.1384e-04\n",
      "Epoch 14/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 2.8594e-04 - mse: 2.8594e-04 - val_loss: 3.7479e-04 - val_mse: 3.7479e-04\n",
      "Epoch 15/1000\n",
      "66/66 [==============================] - 0s 663us/step - loss: 1.9158e-04 - mse: 1.9158e-04 - val_loss: 1.8310e-04 - val_mse: 1.8310e-04\n",
      "Epoch 16/1000\n",
      "66/66 [==============================] - 0s 605us/step - loss: 2.3135e-04 - mse: 2.3135e-04 - val_loss: 5.9127e-04 - val_mse: 5.9127e-04\n",
      "Epoch 17/1000\n",
      "66/66 [==============================] - 0s 508us/step - loss: 3.1190e-04 - mse: 3.1190e-04 - val_loss: 4.8793e-04 - val_mse: 4.8793e-04\n",
      "Epoch 18/1000\n",
      "66/66 [==============================] - 0s 516us/step - loss: 1.9055e-04 - mse: 1.9055e-04 - val_loss: 2.6226e-04 - val_mse: 2.6226e-04\n",
      "Epoch 19/1000\n",
      "66/66 [==============================] - 0s 606us/step - loss: 2.6737e-04 - mse: 2.6737e-04 - val_loss: 1.5157e-04 - val_mse: 1.5157e-04\n",
      "Epoch 20/1000\n",
      "66/66 [==============================] - 0s 559us/step - loss: 1.3517e-04 - mse: 1.3517e-04 - val_loss: 1.5677e-04 - val_mse: 1.5677e-04\n",
      "Epoch 21/1000\n",
      "66/66 [==============================] - 0s 697us/step - loss: 2.8062e-04 - mse: 2.8062e-04 - val_loss: 1.7692e-04 - val_mse: 1.7692e-04\n",
      "Epoch 22/1000\n",
      "66/66 [==============================] - 0s 599us/step - loss: 1.4069e-04 - mse: 1.4069e-04 - val_loss: 1.5186e-04 - val_mse: 1.5186e-04\n",
      "Epoch 23/1000\n",
      "66/66 [==============================] - 0s 561us/step - loss: 1.3251e-04 - mse: 1.3251e-04 - val_loss: 3.0802e-04 - val_mse: 3.0802e-04\n",
      "Epoch 24/1000\n",
      "66/66 [==============================] - 0s 698us/step - loss: 1.4669e-04 - mse: 1.4669e-04 - val_loss: 9.0309e-05 - val_mse: 9.0309e-05\n",
      "Epoch 25/1000\n",
      "66/66 [==============================] - 0s 561us/step - loss: 1.2669e-04 - mse: 1.2669e-04 - val_loss: 1.8394e-04 - val_mse: 1.8394e-04\n",
      "Epoch 26/1000\n",
      "66/66 [==============================] - 0s 530us/step - loss: 1.9477e-04 - mse: 1.9477e-04 - val_loss: 2.4610e-04 - val_mse: 2.4610e-04\n",
      "Epoch 27/1000\n",
      "66/66 [==============================] - 0s 493us/step - loss: 2.0162e-04 - mse: 2.0162e-04 - val_loss: 2.0591e-04 - val_mse: 2.0591e-04\n",
      "Epoch 28/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 1.6629e-04 - mse: 1.6629e-04 - val_loss: 1.6557e-04 - val_mse: 1.6557e-04\n",
      "Epoch 29/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 1.3554e-04 - mse: 1.3554e-04 - val_loss: 2.4861e-04 - val_mse: 2.4861e-04\n",
      "Epoch 30/1000\n",
      "66/66 [==============================] - 0s 527us/step - loss: 1.3035e-04 - mse: 1.3035e-04 - val_loss: 1.2762e-04 - val_mse: 1.2762e-04\n",
      "Epoch 31/1000\n",
      "66/66 [==============================] - 0s 560us/step - loss: 1.4922e-04 - mse: 1.4922e-04 - val_loss: 1.6254e-04 - val_mse: 1.6254e-04\n",
      "Epoch 32/1000\n",
      "66/66 [==============================] - 0s 501us/step - loss: 1.7693e-04 - mse: 1.7693e-04 - val_loss: 2.8979e-04 - val_mse: 2.8979e-04\n",
      "Epoch 33/1000\n",
      "66/66 [==============================] - 0s 479us/step - loss: 1.8047e-04 - mse: 1.8047e-04 - val_loss: 3.1893e-04 - val_mse: 3.1893e-04\n",
      "Epoch 34/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 1.0252e-04 - mse: 1.0252e-04 - val_loss: 3.1798e-04 - val_mse: 3.1798e-04\n",
      "Epoch 35/1000\n",
      "66/66 [==============================] - 0s 456us/step - loss: 1.5392e-04 - mse: 1.5392e-04 - val_loss: 2.1642e-04 - val_mse: 2.1642e-04\n",
      "Epoch 36/1000\n",
      "66/66 [==============================] - 0s 510us/step - loss: 8.1489e-05 - mse: 8.1489e-05 - val_loss: 2.2743e-04 - val_mse: 2.2743e-04\n",
      "Epoch 37/1000\n",
      "66/66 [==============================] - 0s 622us/step - loss: 1.5904e-04 - mse: 1.5904e-04 - val_loss: 1.6764e-04 - val_mse: 1.6764e-04\n",
      "Epoch 38/1000\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 1.1153e-04 - mse: 1.1153e-04 - val_loss: 4.3772e-04 - val_mse: 4.3772e-04\n",
      "Epoch 39/1000\n",
      "66/66 [==============================] - 0s 758us/step - loss: 1.5079e-04 - mse: 1.5079e-04 - val_loss: 4.9614e-04 - val_mse: 4.9614e-04\n",
      "Epoch 40/1000\n",
      "66/66 [==============================] - 0s 565us/step - loss: 2.3616e-04 - mse: 2.3616e-04 - val_loss: 1.1206e-04 - val_mse: 1.1206e-04\n",
      "Epoch 41/1000\n",
      "66/66 [==============================] - 0s 529us/step - loss: 1.3696e-04 - mse: 1.3696e-04 - val_loss: 1.2133e-04 - val_mse: 1.2133e-04\n",
      "Epoch 42/1000\n",
      "66/66 [==============================] - 0s 482us/step - loss: 2.4702e-04 - mse: 2.4702e-04 - val_loss: 2.1684e-04 - val_mse: 2.1684e-04\n",
      "Epoch 43/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 1.4696e-04 - mse: 1.4696e-04 - val_loss: 2.1772e-04 - val_mse: 2.1772e-04\n",
      "Epoch 44/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.6433e-04 - mse: 2.6433e-04 - val_loss: 2.2137e-04 - val_mse: 2.2137e-04\n",
      "Epoch 45/1000\n",
      "66/66 [==============================] - 0s 481us/step - loss: 1.0985e-04 - mse: 1.0985e-04 - val_loss: 1.0872e-04 - val_mse: 1.0872e-04\n",
      "Epoch 46/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 1.1492e-04 - mse: 1.1492e-04 - val_loss: 1.8029e-04 - val_mse: 1.8029e-04\n",
      "Epoch 47/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 1.9392e-04 - mse: 1.9392e-04 - val_loss: 2.6851e-04 - val_mse: 2.6851e-04\n",
      "Epoch 48/1000\n",
      "66/66 [==============================] - 0s 633us/step - loss: 1.3653e-04 - mse: 1.3653e-04 - val_loss: 2.3135e-04 - val_mse: 2.3135e-04\n",
      "Epoch 49/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.2897e-04 - mse: 3.2897e-04 - val_loss: 1.5924e-04 - val_mse: 1.5924e-04\n",
      "Epoch 50/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 1.1492e-04 - mse: 1.1492e-04 - val_loss: 8.4520e-05 - val_mse: 8.4520e-05\n",
      "Epoch 51/1000\n",
      "66/66 [==============================] - 0s 557us/step - loss: 1.3448e-04 - mse: 1.3448e-04 - val_loss: 8.2748e-05 - val_mse: 8.2748e-05\n",
      "Epoch 52/1000\n",
      "66/66 [==============================] - 0s 591us/step - loss: 1.7483e-04 - mse: 1.7483e-04 - val_loss: 2.2992e-04 - val_mse: 2.2992e-04\n",
      "Epoch 53/1000\n",
      "66/66 [==============================] - 0s 514us/step - loss: 1.3218e-04 - mse: 1.3218e-04 - val_loss: 2.0062e-04 - val_mse: 2.0062e-04\n",
      "Epoch 54/1000\n",
      "66/66 [==============================] - 0s 506us/step - loss: 1.0459e-04 - mse: 1.0459e-04 - val_loss: 1.3694e-04 - val_mse: 1.3694e-04\n",
      "Epoch 55/1000\n",
      "66/66 [==============================] - 0s 531us/step - loss: 1.8276e-04 - mse: 1.8276e-04 - val_loss: 2.5879e-04 - val_mse: 2.5879e-04\n",
      "Epoch 56/1000\n",
      "66/66 [==============================] - 0s 585us/step - loss: 1.3240e-04 - mse: 1.3240e-04 - val_loss: 7.9622e-05 - val_mse: 7.9622e-05\n",
      "Epoch 57/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 1.7762e-04 - mse: 1.7762e-04 - val_loss: 1.6626e-04 - val_mse: 1.6626e-04\n",
      "Epoch 58/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 1.4535e-04 - mse: 1.4535e-04 - val_loss: 1.6268e-04 - val_mse: 1.6268e-04\n",
      "Epoch 59/1000\n",
      "66/66 [==============================] - 0s 498us/step - loss: 1.3515e-04 - mse: 1.3515e-04 - val_loss: 2.0788e-04 - val_mse: 2.0788e-04\n",
      "Epoch 60/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 1.3538e-04 - mse: 1.3538e-04 - val_loss: 2.3266e-04 - val_mse: 2.3266e-04\n",
      "Epoch 61/1000\n",
      "66/66 [==============================] - 0s 562us/step - loss: 1.5440e-04 - mse: 1.5440e-04 - val_loss: 2.0486e-04 - val_mse: 2.0486e-04\n",
      "Epoch 62/1000\n",
      "66/66 [==============================] - 0s 571us/step - loss: 2.1611e-04 - mse: 2.1611e-04 - val_loss: 1.8039e-04 - val_mse: 1.8039e-04\n",
      "Epoch 63/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 1.2321e-04 - mse: 1.2321e-04 - val_loss: 1.3831e-04 - val_mse: 1.3831e-04\n",
      "Epoch 64/1000\n",
      "66/66 [==============================] - 0s 543us/step - loss: 1.7115e-04 - mse: 1.7115e-04 - val_loss: 2.0206e-04 - val_mse: 2.0206e-04\n",
      "Epoch 65/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 1.1307e-04 - mse: 1.1307e-04 - val_loss: 1.4151e-04 - val_mse: 1.4151e-04\n",
      "Epoch 66/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 1.6275e-04 - mse: 1.6275e-04 - val_loss: 2.3359e-04 - val_mse: 2.3359e-04\n",
      "Epoch 67/1000\n",
      "66/66 [==============================] - 0s 618us/step - loss: 1.4255e-04 - mse: 1.4255e-04 - val_loss: 1.0546e-04 - val_mse: 1.0546e-04\n",
      "Epoch 68/1000\n",
      "66/66 [==============================] - 0s 623us/step - loss: 6.6398e-05 - mse: 6.6398e-05 - val_loss: 2.3767e-04 - val_mse: 2.3767e-04\n",
      "Epoch 69/1000\n",
      "66/66 [==============================] - 0s 586us/step - loss: 1.6131e-04 - mse: 1.6131e-04 - val_loss: 1.6429e-04 - val_mse: 1.6429e-04\n",
      "Epoch 70/1000\n",
      "66/66 [==============================] - 0s 575us/step - loss: 1.0632e-04 - mse: 1.0632e-04 - val_loss: 2.5713e-04 - val_mse: 2.5713e-04\n",
      "Epoch 71/1000\n",
      "66/66 [==============================] - 0s 591us/step - loss: 1.3474e-04 - mse: 1.3474e-04 - val_loss: 2.1473e-04 - val_mse: 2.1473e-04\n",
      "Epoch 72/1000\n",
      "66/66 [==============================] - 0s 645us/step - loss: 1.7666e-04 - mse: 1.7666e-04 - val_loss: 2.4758e-04 - val_mse: 2.4758e-04\n",
      "Epoch 73/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 1.5804e-04 - mse: 1.5804e-04 - val_loss: 9.1719e-05 - val_mse: 9.1719e-05\n",
      "Epoch 74/1000\n",
      "66/66 [==============================] - 0s 721us/step - loss: 8.2272e-05 - mse: 8.2272e-05 - val_loss: 1.9552e-04 - val_mse: 1.9552e-04\n",
      "Epoch 75/1000\n",
      "66/66 [==============================] - 0s 607us/step - loss: 1.4121e-04 - mse: 1.4121e-04 - val_loss: 1.4624e-04 - val_mse: 1.4624e-04\n",
      "Epoch 76/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 1.1374e-04 - mse: 1.1374e-04 - val_loss: 1.3566e-04 - val_mse: 1.3566e-04\n",
      "Epoch 77/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 9.5219e-05 - mse: 9.5219e-05 - val_loss: 8.2754e-05 - val_mse: 8.2754e-05\n",
      "Epoch 78/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.0955e-04 - mse: 1.0955e-04 - val_loss: 3.2601e-04 - val_mse: 3.2601e-04\n",
      "Epoch 79/1000\n",
      "66/66 [==============================] - 0s 411us/step - loss: 1.1727e-04 - mse: 1.1727e-04 - val_loss: 8.7978e-05 - val_mse: 8.7978e-05\n",
      "Epoch 80/1000\n",
      "66/66 [==============================] - 0s 421us/step - loss: 6.8620e-05 - mse: 6.8620e-05 - val_loss: 1.4625e-04 - val_mse: 1.4625e-04\n",
      "Epoch 81/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 1.0372e-04 - mse: 1.0372e-04 - val_loss: 1.7658e-04 - val_mse: 1.7658e-04\n",
      "Epoch 82/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 1.2385e-04 - mse: 1.2385e-04 - val_loss: 1.3789e-04 - val_mse: 1.3789e-04\n",
      "Epoch 83/1000\n",
      "66/66 [==============================] - 0s 552us/step - loss: 9.2839e-05 - mse: 9.2839e-05 - val_loss: 1.9137e-04 - val_mse: 1.9137e-04\n",
      "Epoch 84/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 1.0916e-04 - mse: 1.0916e-04 - val_loss: 2.0482e-04 - val_mse: 2.0482e-04\n",
      "Epoch 85/1000\n",
      "66/66 [==============================] - 0s 517us/step - loss: 8.4341e-05 - mse: 8.4341e-05 - val_loss: 1.4805e-04 - val_mse: 1.4805e-04\n",
      "Epoch 86/1000\n",
      "66/66 [==============================] - 0s 534us/step - loss: 1.4778e-04 - mse: 1.4778e-04 - val_loss: 3.9016e-04 - val_mse: 3.9016e-04\n",
      "Epoch 87/1000\n",
      "66/66 [==============================] - 0s 532us/step - loss: 1.5262e-04 - mse: 1.5262e-04 - val_loss: 9.5962e-05 - val_mse: 9.5962e-05\n",
      "Epoch 88/1000\n",
      "66/66 [==============================] - 0s 486us/step - loss: 1.0406e-04 - mse: 1.0406e-04 - val_loss: 2.4197e-04 - val_mse: 2.4197e-04\n",
      "Epoch 89/1000\n",
      "66/66 [==============================] - 0s 419us/step - loss: 2.1784e-04 - mse: 2.1784e-04 - val_loss: 2.7456e-04 - val_mse: 2.7456e-04\n",
      "Epoch 90/1000\n",
      "66/66 [==============================] - 0s 576us/step - loss: 1.1962e-04 - mse: 1.1962e-04 - val_loss: 6.9672e-05 - val_mse: 6.9672e-05\n",
      "Epoch 91/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 9.2851e-05 - mse: 9.2851e-05 - val_loss: 1.8856e-04 - val_mse: 1.8856e-04\n",
      "Epoch 92/1000\n",
      "66/66 [==============================] - 0s 389us/step - loss: 1.3700e-04 - mse: 1.3700e-04 - val_loss: 1.3412e-04 - val_mse: 1.3412e-04\n",
      "Epoch 93/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 1.4068e-04 - mse: 1.4068e-04 - val_loss: 2.4156e-04 - val_mse: 2.4156e-04\n",
      "Epoch 94/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.6057e-04 - mse: 1.6057e-04 - val_loss: 1.2223e-04 - val_mse: 1.2223e-04\n",
      "Epoch 95/1000\n",
      "66/66 [==============================] - 0s 396us/step - loss: 1.1076e-04 - mse: 1.1076e-04 - val_loss: 2.4051e-04 - val_mse: 2.4051e-04\n",
      "Epoch 96/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 1.2049e-04 - mse: 1.2049e-04 - val_loss: 1.0009e-04 - val_mse: 1.0009e-04\n",
      "Epoch 97/1000\n",
      "66/66 [==============================] - 0s 561us/step - loss: 1.0910e-04 - mse: 1.0910e-04 - val_loss: 1.6057e-04 - val_mse: 1.6057e-04\n",
      "Epoch 98/1000\n",
      "66/66 [==============================] - 0s 553us/step - loss: 1.0834e-04 - mse: 1.0834e-04 - val_loss: 1.3993e-04 - val_mse: 1.3993e-04\n",
      "Epoch 99/1000\n",
      "66/66 [==============================] - 0s 530us/step - loss: 9.7412e-05 - mse: 9.7412e-05 - val_loss: 9.7359e-05 - val_mse: 9.7359e-05\n",
      "Epoch 100/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 1.1984e-04 - mse: 1.1984e-04 - val_loss: 9.1574e-05 - val_mse: 9.1574e-05\n",
      "Epoch 101/1000\n",
      "66/66 [==============================] - 0s 527us/step - loss: 1.2836e-04 - mse: 1.2836e-04 - val_loss: 2.7669e-04 - val_mse: 2.7669e-04\n",
      "Epoch 102/1000\n",
      "66/66 [==============================] - 0s 501us/step - loss: 1.1677e-04 - mse: 1.1677e-04 - val_loss: 1.0891e-04 - val_mse: 1.0891e-04\n",
      "Epoch 103/1000\n",
      "66/66 [==============================] - 0s 487us/step - loss: 8.6034e-05 - mse: 8.6034e-05 - val_loss: 1.6226e-04 - val_mse: 1.6226e-04\n",
      "Epoch 104/1000\n",
      "66/66 [==============================] - 0s 511us/step - loss: 8.6897e-05 - mse: 8.6897e-05 - val_loss: 1.3602e-04 - val_mse: 1.3602e-04\n",
      "Epoch 105/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 9.6901e-05 - mse: 9.6901e-05 - val_loss: 1.0330e-04 - val_mse: 1.0330e-04\n",
      "Epoch 106/1000\n",
      "66/66 [==============================] - 0s 472us/step - loss: 1.0082e-04 - mse: 1.0082e-04 - val_loss: 1.7741e-04 - val_mse: 1.7741e-04\n",
      "Epoch 107/1000\n",
      "66/66 [==============================] - 0s 493us/step - loss: 5.7597e-05 - mse: 5.7597e-05 - val_loss: 1.0948e-04 - val_mse: 1.0948e-04\n",
      "Epoch 108/1000\n",
      "66/66 [==============================] - 0s 469us/step - loss: 8.0569e-05 - mse: 8.0569e-05 - val_loss: 1.4657e-04 - val_mse: 1.4657e-04\n",
      "Epoch 109/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 1.5329e-04 - mse: 1.5329e-04 - val_loss: 8.0117e-05 - val_mse: 8.0117e-05\n",
      "Epoch 110/1000\n",
      "66/66 [==============================] - 0s 464us/step - loss: 6.9640e-05 - mse: 6.9640e-05 - val_loss: 9.6784e-05 - val_mse: 9.6784e-05\n",
      "Epoch 111/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 1.2095e-04 - mse: 1.2095e-04 - val_loss: 6.1202e-05 - val_mse: 6.1202e-05\n",
      "Epoch 112/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.9720e-05 - mse: 4.9720e-05 - val_loss: 1.1176e-04 - val_mse: 1.1176e-04\n",
      "Epoch 113/1000\n",
      "66/66 [==============================] - 0s 433us/step - loss: 1.3240e-04 - mse: 1.3240e-04 - val_loss: 1.8305e-04 - val_mse: 1.8305e-04\n",
      "Epoch 114/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.1622e-04 - mse: 1.1622e-04 - val_loss: 9.4318e-05 - val_mse: 9.4318e-05\n",
      "Epoch 115/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.3647e-04 - mse: 1.3647e-04 - val_loss: 1.0804e-04 - val_mse: 1.0804e-04\n",
      "Epoch 116/1000\n",
      "66/66 [==============================] - 0s 396us/step - loss: 8.0397e-05 - mse: 8.0397e-05 - val_loss: 7.9613e-05 - val_mse: 7.9613e-05\n",
      "Epoch 117/1000\n",
      "66/66 [==============================] - 0s 414us/step - loss: 1.3811e-04 - mse: 1.3811e-04 - val_loss: 9.2300e-05 - val_mse: 9.2300e-05\n",
      "Epoch 118/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.3221e-04 - mse: 1.3221e-04 - val_loss: 8.8066e-05 - val_mse: 8.8066e-05\n",
      "Epoch 119/1000\n",
      "66/66 [==============================] - 0s 442us/step - loss: 5.4326e-05 - mse: 5.4326e-05 - val_loss: 8.1386e-05 - val_mse: 8.1386e-05\n",
      "Epoch 120/1000\n",
      "66/66 [==============================] - 0s 405us/step - loss: 6.9203e-05 - mse: 6.9203e-05 - val_loss: 9.6368e-05 - val_mse: 9.6368e-05\n",
      "Epoch 121/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 8.2417e-05 - mse: 8.2417e-05 - val_loss: 1.6300e-04 - val_mse: 1.6300e-04\n",
      "Epoch 122/1000\n",
      "66/66 [==============================] - 0s 408us/step - loss: 1.0535e-04 - mse: 1.0535e-04 - val_loss: 6.3127e-05 - val_mse: 6.3127e-05\n",
      "Epoch 123/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 7.0886e-05 - mse: 7.0886e-05 - val_loss: 9.0254e-05 - val_mse: 9.0254e-05\n",
      "Epoch 124/1000\n",
      "66/66 [==============================] - 0s 449us/step - loss: 1.3023e-04 - mse: 1.3023e-04 - val_loss: 3.2214e-04 - val_mse: 3.2214e-04\n",
      "Epoch 125/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.2005e-04 - mse: 1.2005e-04 - val_loss: 8.2636e-05 - val_mse: 8.2636e-05\n",
      "Epoch 126/1000\n",
      "66/66 [==============================] - 0s 486us/step - loss: 1.0169e-04 - mse: 1.0169e-04 - val_loss: 1.6565e-04 - val_mse: 1.6565e-04\n",
      "Epoch 127/1000\n",
      "66/66 [==============================] - 0s 419us/step - loss: 8.4631e-05 - mse: 8.4631e-05 - val_loss: 6.8875e-05 - val_mse: 6.8875e-05\n",
      "Epoch 128/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 6.3136e-05 - mse: 6.3136e-05 - val_loss: 1.1903e-04 - val_mse: 1.1903e-04\n",
      "Epoch 129/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 1.0493e-04 - mse: 1.0493e-04 - val_loss: 1.9458e-04 - val_mse: 1.9458e-04\n",
      "Epoch 130/1000\n",
      "66/66 [==============================] - 0s 493us/step - loss: 6.7321e-05 - mse: 6.7321e-05 - val_loss: 4.9634e-05 - val_mse: 4.9634e-05\n",
      "Epoch 131/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 8.2044e-05 - mse: 8.2044e-05 - val_loss: 1.0487e-04 - val_mse: 1.0487e-04\n",
      "Epoch 132/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 9.3650e-05 - mse: 9.3650e-05 - val_loss: 9.1444e-05 - val_mse: 9.1444e-05\n",
      "Epoch 133/1000\n",
      "66/66 [==============================] - 0s 421us/step - loss: 9.5198e-05 - mse: 9.5198e-05 - val_loss: 1.7004e-04 - val_mse: 1.7004e-04\n",
      "Epoch 134/1000\n",
      "66/66 [==============================] - 0s 428us/step - loss: 1.2007e-04 - mse: 1.2007e-04 - val_loss: 7.1919e-05 - val_mse: 7.1919e-05\n",
      "Epoch 135/1000\n",
      "66/66 [==============================] - 0s 438us/step - loss: 8.2192e-05 - mse: 8.2192e-05 - val_loss: 1.0852e-04 - val_mse: 1.0852e-04\n",
      "Epoch 136/1000\n",
      "66/66 [==============================] - 0s 438us/step - loss: 6.7226e-05 - mse: 6.7226e-05 - val_loss: 7.5881e-05 - val_mse: 7.5881e-05\n",
      "Epoch 137/1000\n",
      "66/66 [==============================] - 0s 442us/step - loss: 8.8308e-05 - mse: 8.8308e-05 - val_loss: 1.9413e-04 - val_mse: 1.9413e-04\n",
      "Epoch 138/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.0089e-04 - mse: 1.0089e-04 - val_loss: 9.1720e-05 - val_mse: 9.1720e-05\n",
      "Epoch 139/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 6.7004e-05 - mse: 6.7004e-05 - val_loss: 4.9640e-05 - val_mse: 4.9640e-05\n",
      "Epoch 140/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 1.1994e-04 - mse: 1.1994e-04 - val_loss: 5.8411e-05 - val_mse: 5.8411e-05\n",
      "Epoch 141/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 6.1474e-05 - mse: 6.1474e-05 - val_loss: 1.7706e-04 - val_mse: 1.7706e-04\n",
      "Epoch 142/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.2680e-04 - mse: 2.2680e-04 - val_loss: 2.6769e-04 - val_mse: 2.6769e-04\n",
      "Epoch 143/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 5.7137e-05 - mse: 5.7137e-05 - val_loss: 5.3686e-05 - val_mse: 5.3686e-05\n",
      "Epoch 144/1000\n",
      "66/66 [==============================] - 0s 530us/step - loss: 4.7968e-05 - mse: 4.7968e-05 - val_loss: 4.7514e-05 - val_mse: 4.7514e-05\n",
      "Epoch 145/1000\n",
      "66/66 [==============================] - 0s 456us/step - loss: 7.9153e-05 - mse: 7.9153e-05 - val_loss: 1.1292e-04 - val_mse: 1.1292e-04\n",
      "Epoch 146/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 5.6721e-05 - mse: 5.6721e-05 - val_loss: 2.1663e-04 - val_mse: 2.1663e-04\n",
      "Epoch 147/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 1.0834e-04 - mse: 1.0834e-04 - val_loss: 1.6285e-04 - val_mse: 1.6285e-04\n",
      "Epoch 148/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 6.0276e-05 - mse: 6.0276e-05 - val_loss: 7.7245e-05 - val_mse: 7.7245e-05\n",
      "Epoch 149/1000\n",
      "66/66 [==============================] - 0s 364us/step - loss: 8.0633e-05 - mse: 8.0633e-05 - val_loss: 9.1348e-05 - val_mse: 9.1348e-05\n",
      "Epoch 150/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.9137e-05 - mse: 4.9137e-05 - val_loss: 1.7445e-04 - val_mse: 1.7445e-04\n",
      "Epoch 151/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 8.8630e-05 - mse: 8.8630e-05 - val_loss: 8.3672e-05 - val_mse: 8.3672e-05\n",
      "Epoch 152/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.1039e-04 - mse: 1.1039e-04 - val_loss: 2.6114e-04 - val_mse: 2.6114e-04\n",
      "Epoch 153/1000\n",
      "66/66 [==============================] - 0s 463us/step - loss: 9.9054e-05 - mse: 9.9054e-05 - val_loss: 1.1467e-04 - val_mse: 1.1467e-04\n",
      "Epoch 154/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 6.2510e-05 - mse: 6.2510e-05 - val_loss: 1.6938e-04 - val_mse: 1.6938e-04\n",
      "Epoch 155/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 1.4391e-04 - mse: 1.4391e-04 - val_loss: 1.4316e-04 - val_mse: 1.4316e-04\n",
      "Epoch 156/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 4.2184e-05 - mse: 4.2184e-05 - val_loss: 4.6825e-05 - val_mse: 4.6825e-05\n",
      "Epoch 157/1000\n",
      "66/66 [==============================] - 0s 396us/step - loss: 4.8408e-05 - mse: 4.8408e-05 - val_loss: 1.0313e-04 - val_mse: 1.0313e-04\n",
      "Epoch 158/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 8.4565e-05 - mse: 8.4565e-05 - val_loss: 5.2738e-05 - val_mse: 5.2738e-05\n",
      "Epoch 159/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 6.4797e-05 - mse: 6.4797e-05 - val_loss: 6.4713e-05 - val_mse: 6.4713e-05\n",
      "Epoch 160/1000\n",
      "66/66 [==============================] - 0s 397us/step - loss: 6.7335e-05 - mse: 6.7335e-05 - val_loss: 7.1156e-05 - val_mse: 7.1156e-05\n",
      "Epoch 161/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 8.5621e-05 - mse: 8.5621e-05 - val_loss: 1.5662e-04 - val_mse: 1.5662e-04\n",
      "Epoch 162/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.1218e-04 - mse: 1.1218e-04 - val_loss: 2.2083e-04 - val_mse: 2.2083e-04\n",
      "Epoch 163/1000\n",
      "66/66 [==============================] - 0s 462us/step - loss: 4.8802e-05 - mse: 4.8802e-05 - val_loss: 5.4005e-05 - val_mse: 5.4005e-05\n",
      "Epoch 164/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 5.7193e-05 - mse: 5.7193e-05 - val_loss: 6.5017e-05 - val_mse: 6.5017e-05\n",
      "Epoch 165/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 7.4459e-05 - mse: 7.4459e-05 - val_loss: 7.9749e-05 - val_mse: 7.9749e-05\n",
      "Epoch 166/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.3242e-05 - mse: 4.3242e-05 - val_loss: 2.6264e-04 - val_mse: 2.6264e-04\n",
      "Epoch 167/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 8.5779e-05 - mse: 8.5779e-05 - val_loss: 8.2134e-05 - val_mse: 8.2134e-05\n",
      "Epoch 168/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.2023e-05 - mse: 4.2023e-05 - val_loss: 9.6406e-05 - val_mse: 9.6406e-05\n",
      "Epoch 169/1000\n",
      "66/66 [==============================] - 0s 561us/step - loss: 6.6251e-05 - mse: 6.6251e-05 - val_loss: 4.5362e-05 - val_mse: 4.5362e-05\n",
      "Epoch 170/1000\n",
      "66/66 [==============================] - 0s 518us/step - loss: 3.8109e-05 - mse: 3.8109e-05 - val_loss: 8.5710e-05 - val_mse: 8.5710e-05\n",
      "Epoch 171/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 5.4592e-05 - mse: 5.4592e-05 - val_loss: 1.1593e-04 - val_mse: 1.1593e-04\n",
      "Epoch 172/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 7.8547e-05 - mse: 7.8547e-05 - val_loss: 1.1876e-04 - val_mse: 1.1876e-04\n",
      "Epoch 173/1000\n",
      "66/66 [==============================] - 0s 411us/step - loss: 8.8851e-05 - mse: 8.8851e-05 - val_loss: 5.2004e-05 - val_mse: 5.2004e-05\n",
      "Epoch 174/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 5.1084e-05 - mse: 5.1084e-05 - val_loss: 3.7587e-05 - val_mse: 3.7587e-05\n",
      "Epoch 175/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 6.7213e-05 - mse: 6.7213e-05 - val_loss: 8.4157e-05 - val_mse: 8.4157e-05\n",
      "Epoch 176/1000\n",
      "66/66 [==============================] - 0s 493us/step - loss: 7.5502e-05 - mse: 7.5502e-05 - val_loss: 1.4860e-04 - val_mse: 1.4860e-04\n",
      "Epoch 177/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 5.1776e-05 - mse: 5.1776e-05 - val_loss: 7.1815e-05 - val_mse: 7.1815e-05\n",
      "Epoch 178/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 5.5150e-05 - mse: 5.5150e-05 - val_loss: 7.0791e-05 - val_mse: 7.0791e-05\n",
      "Epoch 179/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 5.3210e-05 - mse: 5.3210e-05 - val_loss: 7.5997e-05 - val_mse: 7.5997e-05\n",
      "Epoch 180/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 8.5031e-05 - mse: 8.5031e-05 - val_loss: 6.4488e-05 - val_mse: 6.4488e-05\n",
      "Epoch 181/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.7850e-05 - mse: 4.7850e-05 - val_loss: 4.0341e-05 - val_mse: 4.0341e-05\n",
      "Epoch 182/1000\n",
      "66/66 [==============================] - 0s 498us/step - loss: 4.5601e-05 - mse: 4.5601e-05 - val_loss: 1.2924e-04 - val_mse: 1.2924e-04\n",
      "Epoch 183/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 9.2510e-05 - mse: 9.2510e-05 - val_loss: 8.5114e-05 - val_mse: 8.5114e-05\n",
      "Epoch 184/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 6.4813e-05 - mse: 6.4813e-05 - val_loss: 1.5137e-04 - val_mse: 1.5137e-04\n",
      "Epoch 185/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.0060e-04 - mse: 1.0060e-04 - val_loss: 7.4698e-05 - val_mse: 7.4698e-05\n",
      "Epoch 186/1000\n",
      "66/66 [==============================] - 0s 426us/step - loss: 1.0177e-04 - mse: 1.0177e-04 - val_loss: 2.2451e-04 - val_mse: 2.2451e-04\n",
      "Epoch 187/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 6.8090e-05 - mse: 6.8090e-05 - val_loss: 1.1766e-04 - val_mse: 1.1766e-04\n",
      "Epoch 188/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 6.7845e-05 - mse: 6.7845e-05 - val_loss: 1.2669e-04 - val_mse: 1.2669e-04\n",
      "Epoch 189/1000\n",
      "66/66 [==============================] - 0s 428us/step - loss: 4.8987e-05 - mse: 4.8987e-05 - val_loss: 4.8800e-05 - val_mse: 4.8800e-05\n",
      "Epoch 190/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.7696e-05 - mse: 4.7696e-05 - val_loss: 9.3755e-05 - val_mse: 9.3755e-05\n",
      "Epoch 191/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 7.1338e-05 - mse: 7.1338e-05 - val_loss: 9.5612e-05 - val_mse: 9.5612e-05\n",
      "Epoch 192/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.0476e-04 - mse: 1.0476e-04 - val_loss: 1.8670e-04 - val_mse: 1.8670e-04\n",
      "Epoch 193/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 7.4531e-05 - mse: 7.4531e-05 - val_loss: 5.2778e-05 - val_mse: 5.2778e-05\n",
      "Epoch 194/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.3738e-05 - mse: 4.3738e-05 - val_loss: 4.4268e-05 - val_mse: 4.4268e-05\n",
      "Epoch 195/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 7.7023e-05 - mse: 7.7023e-05 - val_loss: 5.2376e-05 - val_mse: 5.2376e-05\n",
      "Epoch 196/1000\n",
      "66/66 [==============================] - 0s 414us/step - loss: 7.4580e-05 - mse: 7.4580e-05 - val_loss: 2.5506e-04 - val_mse: 2.5506e-04\n",
      "Epoch 197/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.0004e-04 - mse: 1.0004e-04 - val_loss: 8.4007e-05 - val_mse: 8.4007e-05\n",
      "Epoch 198/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.3656e-04 - mse: 1.3656e-04 - val_loss: 1.6395e-04 - val_mse: 1.6395e-04\n",
      "Epoch 199/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.6844e-05 - mse: 3.6844e-05 - val_loss: 3.8072e-05 - val_mse: 3.8072e-05\n",
      "Epoch 200/1000\n",
      "66/66 [==============================] - 0s 478us/step - loss: 5.2350e-05 - mse: 5.2350e-05 - val_loss: 8.5009e-05 - val_mse: 8.5009e-05\n",
      "Epoch 201/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 7.0367e-05 - mse: 7.0367e-05 - val_loss: 3.2293e-04 - val_mse: 3.2293e-04\n",
      "Epoch 202/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 9.1891e-05 - mse: 9.1891e-05 - val_loss: 3.6901e-05 - val_mse: 3.6901e-05\n",
      "Epoch 203/1000\n",
      "66/66 [==============================] - 0s 446us/step - loss: 4.3182e-05 - mse: 4.3182e-05 - val_loss: 4.5083e-05 - val_mse: 4.5083e-05\n",
      "Epoch 204/1000\n",
      "66/66 [==============================] - 0s 428us/step - loss: 6.8908e-05 - mse: 6.8908e-05 - val_loss: 8.8550e-05 - val_mse: 8.8550e-05\n",
      "Epoch 205/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.6986e-05 - mse: 3.6986e-05 - val_loss: 3.6881e-05 - val_mse: 3.6881e-05\n",
      "Epoch 206/1000\n",
      "66/66 [==============================] - 0s 418us/step - loss: 4.6292e-05 - mse: 4.6292e-05 - val_loss: 6.4462e-05 - val_mse: 6.4462e-05\n",
      "Epoch 207/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.3005e-04 - mse: 1.3005e-04 - val_loss: 6.0005e-05 - val_mse: 6.0005e-05\n",
      "Epoch 208/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.1732e-05 - mse: 2.1732e-05 - val_loss: 5.0851e-05 - val_mse: 5.0851e-05\n",
      "Epoch 209/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.8561e-05 - mse: 4.8561e-05 - val_loss: 3.8434e-05 - val_mse: 3.8434e-05\n",
      "Epoch 210/1000\n",
      "66/66 [==============================] - 0s 457us/step - loss: 4.3967e-05 - mse: 4.3967e-05 - val_loss: 9.8171e-05 - val_mse: 9.8171e-05\n",
      "Epoch 211/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 8.7910e-05 - mse: 8.7910e-05 - val_loss: 7.3669e-05 - val_mse: 7.3669e-05\n",
      "Epoch 212/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 7.3361e-05 - mse: 7.3361e-05 - val_loss: 5.3489e-05 - val_mse: 5.3489e-05\n",
      "Epoch 213/1000\n",
      "66/66 [==============================] - 0s 412us/step - loss: 8.7829e-05 - mse: 8.7829e-05 - val_loss: 1.4069e-04 - val_mse: 1.4069e-04\n",
      "Epoch 214/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 5.9663e-05 - mse: 5.9663e-05 - val_loss: 2.0754e-04 - val_mse: 2.0754e-04\n",
      "Epoch 215/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 5.7169e-05 - mse: 5.7169e-05 - val_loss: 5.3671e-05 - val_mse: 5.3671e-05\n",
      "Epoch 216/1000\n",
      "66/66 [==============================] - 0s 486us/step - loss: 4.1403e-05 - mse: 4.1403e-05 - val_loss: 3.5754e-05 - val_mse: 3.5754e-05\n",
      "Epoch 217/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 8.4856e-05 - mse: 8.4856e-05 - val_loss: 1.1246e-04 - val_mse: 1.1246e-04\n",
      "Epoch 218/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 8.2477e-05 - mse: 8.2477e-05 - val_loss: 9.4727e-05 - val_mse: 9.4727e-05\n",
      "Epoch 219/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 4.8357e-05 - mse: 4.8357e-05 - val_loss: 3.1316e-05 - val_mse: 3.1316e-05\n",
      "Epoch 220/1000\n",
      "66/66 [==============================] - 0s 426us/step - loss: 4.2568e-05 - mse: 4.2568e-05 - val_loss: 9.4702e-05 - val_mse: 9.4702e-05\n",
      "Epoch 221/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 6.4852e-05 - mse: 6.4852e-05 - val_loss: 1.2462e-04 - val_mse: 1.2462e-04\n",
      "Epoch 222/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 6.3809e-05 - mse: 6.3809e-05 - val_loss: 5.2658e-05 - val_mse: 5.2658e-05\n",
      "Epoch 223/1000\n",
      "66/66 [==============================] - 0s 412us/step - loss: 5.9683e-05 - mse: 5.9683e-05 - val_loss: 5.4113e-05 - val_mse: 5.4113e-05\n",
      "Epoch 224/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 5.5758e-05 - mse: 5.5758e-05 - val_loss: 5.0984e-05 - val_mse: 5.0984e-05\n",
      "Epoch 225/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.5460e-05 - mse: 4.5460e-05 - val_loss: 6.0906e-05 - val_mse: 6.0906e-05\n",
      "Epoch 226/1000\n",
      "66/66 [==============================] - 0s 411us/step - loss: 6.3237e-05 - mse: 6.3237e-05 - val_loss: 3.8543e-05 - val_mse: 3.8543e-05\n",
      "Epoch 227/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 5.5242e-05 - mse: 5.5242e-05 - val_loss: 7.6241e-05 - val_mse: 7.6241e-05\n",
      "Epoch 228/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 6.3601e-05 - mse: 6.3601e-05 - val_loss: 7.0852e-05 - val_mse: 7.0852e-05\n",
      "Epoch 229/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 8.1255e-05 - mse: 8.1255e-05 - val_loss: 1.0707e-04 - val_mse: 1.0707e-04\n",
      "Epoch 230/1000\n",
      "66/66 [==============================] - 0s 408us/step - loss: 3.2661e-05 - mse: 3.2661e-05 - val_loss: 6.8935e-05 - val_mse: 6.8935e-05\n",
      "Epoch 231/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 5.3892e-05 - mse: 5.3892e-05 - val_loss: 2.6123e-05 - val_mse: 2.6123e-05\n",
      "Epoch 232/1000\n",
      "66/66 [==============================] - 0s 405us/step - loss: 4.8630e-05 - mse: 4.8630e-05 - val_loss: 3.7207e-05 - val_mse: 3.7207e-05\n",
      "Epoch 233/1000\n",
      "66/66 [==============================] - 0s 416us/step - loss: 4.0164e-05 - mse: 4.0164e-05 - val_loss: 1.8851e-04 - val_mse: 1.8851e-04\n",
      "Epoch 234/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.1407e-04 - mse: 1.1407e-04 - val_loss: 5.7292e-05 - val_mse: 5.7292e-05\n",
      "Epoch 235/1000\n",
      "66/66 [==============================] - 0s 441us/step - loss: 4.3769e-05 - mse: 4.3769e-05 - val_loss: 5.1646e-05 - val_mse: 5.1646e-05\n",
      "Epoch 236/1000\n",
      "66/66 [==============================] - 0s 418us/step - loss: 5.0273e-05 - mse: 5.0273e-05 - val_loss: 5.8384e-05 - val_mse: 5.8384e-05\n",
      "Epoch 237/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 5.0188e-05 - mse: 5.0188e-05 - val_loss: 1.4353e-04 - val_mse: 1.4353e-04\n",
      "Epoch 238/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 7.0110e-05 - mse: 7.0110e-05 - val_loss: 1.6775e-04 - val_mse: 1.6775e-04\n",
      "Epoch 239/1000\n",
      "66/66 [==============================] - 0s 438us/step - loss: 5.8127e-05 - mse: 5.8127e-05 - val_loss: 1.6911e-04 - val_mse: 1.6911e-04\n",
      "Epoch 240/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 4.8096e-05 - mse: 4.8096e-05 - val_loss: 4.0147e-05 - val_mse: 4.0147e-05\n",
      "Epoch 241/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 6.5537e-05 - mse: 6.5537e-05 - val_loss: 3.9867e-05 - val_mse: 3.9867e-05\n",
      "Epoch 242/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 6.3091e-05 - mse: 6.3091e-05 - val_loss: 1.1064e-04 - val_mse: 1.1064e-04\n",
      "Epoch 243/1000\n",
      "66/66 [==============================] - 0s 442us/step - loss: 4.7991e-05 - mse: 4.7991e-05 - val_loss: 4.4949e-05 - val_mse: 4.4949e-05\n",
      "Epoch 244/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 3.3882e-05 - mse: 3.3882e-05 - val_loss: 7.7737e-05 - val_mse: 7.7737e-05\n",
      "Epoch 245/1000\n",
      "66/66 [==============================] - 0s 530us/step - loss: 4.7642e-05 - mse: 4.7642e-05 - val_loss: 8.7902e-05 - val_mse: 8.7902e-05\n",
      "Epoch 246/1000\n",
      "66/66 [==============================] - 0s 563us/step - loss: 6.5207e-05 - mse: 6.5207e-05 - val_loss: 8.3057e-05 - val_mse: 8.3057e-05\n",
      "Epoch 247/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 5.7522e-05 - mse: 5.7522e-05 - val_loss: 1.0798e-04 - val_mse: 1.0798e-04\n",
      "Epoch 248/1000\n",
      "66/66 [==============================] - 0s 621us/step - loss: 7.0091e-05 - mse: 7.0091e-05 - val_loss: 1.4849e-04 - val_mse: 1.4849e-04\n",
      "Epoch 249/1000\n",
      "66/66 [==============================] - 0s 502us/step - loss: 6.8057e-05 - mse: 6.8057e-05 - val_loss: 4.9576e-05 - val_mse: 4.9576e-05\n",
      "Epoch 250/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.8426e-05 - mse: 1.8426e-05 - val_loss: 5.6271e-05 - val_mse: 5.6271e-05\n",
      "Epoch 251/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.2883e-05 - mse: 4.2883e-05 - val_loss: 1.3551e-04 - val_mse: 1.3551e-04\n",
      "Epoch 252/1000\n",
      "66/66 [==============================] - 0s 401us/step - loss: 6.8806e-05 - mse: 6.8806e-05 - val_loss: 2.9244e-05 - val_mse: 2.9244e-05\n",
      "Epoch 253/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.8709e-05 - mse: 2.8709e-05 - val_loss: 5.6980e-05 - val_mse: 5.6980e-05\n",
      "Epoch 254/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 4.5787e-05 - mse: 4.5787e-05 - val_loss: 1.1817e-04 - val_mse: 1.1817e-04\n",
      "Epoch 255/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 7.7038e-05 - mse: 7.7038e-05 - val_loss: 2.8200e-05 - val_mse: 2.8200e-05\n",
      "Epoch 256/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.4775e-05 - mse: 3.4775e-05 - val_loss: 4.2508e-05 - val_mse: 4.2508e-05\n",
      "Epoch 257/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 7.2677e-05 - mse: 7.2677e-05 - val_loss: 7.2794e-05 - val_mse: 7.2794e-05\n",
      "Epoch 258/1000\n",
      "66/66 [==============================] - 0s 508us/step - loss: 3.7827e-05 - mse: 3.7827e-05 - val_loss: 2.9616e-05 - val_mse: 2.9616e-05\n",
      "Epoch 259/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 5.3634e-05 - mse: 5.3634e-05 - val_loss: 4.5799e-05 - val_mse: 4.5799e-05\n",
      "Epoch 260/1000\n",
      "66/66 [==============================] - 0s 530us/step - loss: 3.8824e-05 - mse: 3.8824e-05 - val_loss: 5.2687e-05 - val_mse: 5.2687e-05\n",
      "Epoch 261/1000\n",
      "66/66 [==============================] - 0s 519us/step - loss: 6.7826e-05 - mse: 6.7826e-05 - val_loss: 3.6882e-05 - val_mse: 3.6882e-05\n",
      "Epoch 262/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 3.2887e-05 - mse: 3.2887e-05 - val_loss: 4.6408e-05 - val_mse: 4.6408e-05\n",
      "Epoch 263/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 4.3005e-05 - mse: 4.3005e-05 - val_loss: 1.8721e-04 - val_mse: 1.8721e-04\n",
      "Epoch 264/1000\n",
      "66/66 [==============================] - 0s 413us/step - loss: 6.0824e-05 - mse: 6.0824e-05 - val_loss: 1.1367e-04 - val_mse: 1.1367e-04\n",
      "Epoch 265/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.3625e-05 - mse: 4.3625e-05 - val_loss: 9.1559e-05 - val_mse: 9.1559e-05\n",
      "Epoch 266/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 7.3303e-05 - mse: 7.3303e-05 - val_loss: 2.5414e-05 - val_mse: 2.5414e-05\n",
      "Epoch 267/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 2.3211e-05 - mse: 2.3211e-05 - val_loss: 9.3437e-05 - val_mse: 9.3437e-05\n",
      "Epoch 268/1000\n",
      "66/66 [==============================] - 0s 443us/step - loss: 4.3762e-05 - mse: 4.3762e-05 - val_loss: 2.8621e-05 - val_mse: 2.8621e-05\n",
      "Epoch 269/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 5.6878e-05 - mse: 5.6878e-05 - val_loss: 4.6113e-05 - val_mse: 4.6113e-05\n",
      "Epoch 270/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.6908e-05 - mse: 4.6908e-05 - val_loss: 3.9932e-05 - val_mse: 3.9932e-05\n",
      "Epoch 271/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 2.6982e-05 - mse: 2.6982e-05 - val_loss: 3.1343e-05 - val_mse: 3.1343e-05\n",
      "Epoch 272/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.9528e-05 - mse: 4.9528e-05 - val_loss: 3.4789e-05 - val_mse: 3.4789e-05\n",
      "Epoch 273/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 6.9589e-05 - mse: 6.9589e-05 - val_loss: 1.3412e-04 - val_mse: 1.3412e-04\n",
      "Epoch 274/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 5.8691e-05 - mse: 5.8691e-05 - val_loss: 6.6802e-05 - val_mse: 6.6802e-05\n",
      "Epoch 275/1000\n",
      "66/66 [==============================] - 0s 412us/step - loss: 6.3531e-05 - mse: 6.3531e-05 - val_loss: 7.9310e-05 - val_mse: 7.9310e-05\n",
      "Epoch 276/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 8.5796e-05 - mse: 8.5796e-05 - val_loss: 1.4662e-04 - val_mse: 1.4662e-04\n",
      "Epoch 277/1000\n",
      "66/66 [==============================] - 0s 546us/step - loss: 6.1910e-05 - mse: 6.1910e-05 - val_loss: 2.9376e-05 - val_mse: 2.9376e-05\n",
      "Epoch 278/1000\n",
      "66/66 [==============================] - 0s 546us/step - loss: 3.1275e-05 - mse: 3.1275e-05 - val_loss: 1.1494e-04 - val_mse: 1.1494e-04\n",
      "Epoch 279/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 4.6232e-05 - mse: 4.6232e-05 - val_loss: 4.4856e-05 - val_mse: 4.4856e-05\n",
      "Epoch 280/1000\n",
      "66/66 [==============================] - 0s 504us/step - loss: 5.5242e-05 - mse: 5.5242e-05 - val_loss: 1.0854e-04 - val_mse: 1.0854e-04\n",
      "Epoch 281/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 5.8066e-05 - mse: 5.8066e-05 - val_loss: 1.0157e-04 - val_mse: 1.0157e-04\n",
      "Epoch 282/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 5.7577e-05 - mse: 5.7577e-05 - val_loss: 3.4187e-05 - val_mse: 3.4187e-05\n",
      "Epoch 283/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 3.6729e-05 - mse: 3.6729e-05 - val_loss: 2.9980e-04 - val_mse: 2.9980e-04\n",
      "Epoch 284/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 6.5127e-05 - mse: 6.5127e-05 - val_loss: 4.2278e-05 - val_mse: 4.2278e-05\n",
      "Epoch 285/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 5.1790e-05 - mse: 5.1790e-05 - val_loss: 1.1113e-04 - val_mse: 1.1113e-04\n",
      "Epoch 286/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.1922e-05 - mse: 4.1922e-05 - val_loss: 1.0793e-04 - val_mse: 1.0793e-04\n",
      "Epoch 287/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 3.8029e-05 - mse: 3.8029e-05 - val_loss: 3.4655e-05 - val_mse: 3.4655e-05\n",
      "Epoch 288/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.8245e-05 - mse: 3.8245e-05 - val_loss: 4.7642e-05 - val_mse: 4.7642e-05\n",
      "Epoch 289/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 3.9067e-05 - mse: 3.9067e-05 - val_loss: 1.6089e-04 - val_mse: 1.6089e-04\n",
      "Epoch 290/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.3232e-05 - mse: 4.3232e-05 - val_loss: 5.1277e-05 - val_mse: 5.1277e-05\n",
      "Epoch 291/1000\n",
      "66/66 [==============================] - 0s 418us/step - loss: 5.8885e-05 - mse: 5.8885e-05 - val_loss: 5.0471e-05 - val_mse: 5.0471e-05\n",
      "Epoch 292/1000\n",
      "66/66 [==============================] - 0s 393us/step - loss: 4.1614e-05 - mse: 4.1614e-05 - val_loss: 8.3655e-05 - val_mse: 8.3655e-05\n",
      "Epoch 293/1000\n",
      "66/66 [==============================] - 0s 412us/step - loss: 3.3249e-05 - mse: 3.3249e-05 - val_loss: 1.3475e-04 - val_mse: 1.3475e-04\n",
      "Epoch 294/1000\n",
      "66/66 [==============================] - 0s 433us/step - loss: 6.2870e-05 - mse: 6.2870e-05 - val_loss: 3.2877e-05 - val_mse: 3.2877e-05\n",
      "Epoch 295/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.6670e-05 - mse: 3.6670e-05 - val_loss: 1.4439e-04 - val_mse: 1.4439e-04\n",
      "Epoch 296/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 3.9278e-05 - mse: 3.9278e-05 - val_loss: 2.2774e-05 - val_mse: 2.2774e-05\n",
      "Epoch 297/1000\n",
      "66/66 [==============================] - 0s 408us/step - loss: 4.2894e-05 - mse: 4.2894e-05 - val_loss: 3.0431e-05 - val_mse: 3.0431e-05\n",
      "Epoch 298/1000\n",
      "66/66 [==============================] - 0s 399us/step - loss: 5.1797e-05 - mse: 5.1797e-05 - val_loss: 9.7806e-05 - val_mse: 9.7806e-05\n",
      "Epoch 299/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.9104e-05 - mse: 4.9104e-05 - val_loss: 3.6047e-05 - val_mse: 3.6047e-05\n",
      "Epoch 300/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 2.9018e-05 - mse: 2.9018e-05 - val_loss: 6.7134e-05 - val_mse: 6.7134e-05\n",
      "Epoch 301/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 4.0414e-05 - mse: 4.0414e-05 - val_loss: 9.1875e-05 - val_mse: 9.1875e-05\n",
      "Epoch 302/1000\n",
      "66/66 [==============================] - 0s 411us/step - loss: 6.0808e-05 - mse: 6.0808e-05 - val_loss: 9.2067e-05 - val_mse: 9.2067e-05\n",
      "Epoch 303/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 5.3952e-05 - mse: 5.3952e-05 - val_loss: 9.1892e-05 - val_mse: 9.1892e-05\n",
      "Epoch 304/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 3.1958e-05 - mse: 3.1958e-05 - val_loss: 4.5031e-05 - val_mse: 4.5031e-05\n",
      "Epoch 305/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 3.6168e-05 - mse: 3.6168e-05 - val_loss: 3.6739e-05 - val_mse: 3.6739e-05\n",
      "Epoch 306/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 4.5910e-05 - mse: 4.5910e-05 - val_loss: 7.1718e-05 - val_mse: 7.1718e-05\n",
      "Epoch 307/1000\n",
      "66/66 [==============================] - 0s 533us/step - loss: 1.6229e-05 - mse: 1.6229e-05 - val_loss: 3.0042e-05 - val_mse: 3.0042e-05\n",
      "Epoch 308/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 4.3636e-05 - mse: 4.3636e-05 - val_loss: 7.5147e-05 - val_mse: 7.5147e-05\n",
      "Epoch 309/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.9173e-05 - mse: 4.9173e-05 - val_loss: 3.1716e-05 - val_mse: 3.1716e-05\n",
      "Epoch 310/1000\n",
      "66/66 [==============================] - 0s 463us/step - loss: 2.5997e-05 - mse: 2.5997e-05 - val_loss: 6.0591e-05 - val_mse: 6.0591e-05\n",
      "Epoch 311/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.0118e-05 - mse: 4.0118e-05 - val_loss: 9.1028e-05 - val_mse: 9.1028e-05\n",
      "Epoch 312/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 5.6219e-05 - mse: 5.6219e-05 - val_loss: 5.8168e-05 - val_mse: 5.8168e-05\n",
      "Epoch 313/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 4.6604e-05 - mse: 4.6604e-05 - val_loss: 3.7916e-05 - val_mse: 3.7916e-05\n",
      "Epoch 314/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 4.1863e-05 - mse: 4.1863e-05 - val_loss: 3.5501e-05 - val_mse: 3.5501e-05\n",
      "Epoch 315/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.1130e-05 - mse: 2.1130e-05 - val_loss: 3.8088e-05 - val_mse: 3.8088e-05\n",
      "Epoch 316/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.2103e-05 - mse: 4.2103e-05 - val_loss: 4.2843e-05 - val_mse: 4.2843e-05\n",
      "Epoch 317/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 5.1713e-05 - mse: 5.1713e-05 - val_loss: 5.7294e-05 - val_mse: 5.7294e-05\n",
      "Epoch 318/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.7049e-05 - mse: 2.7049e-05 - val_loss: 2.7036e-05 - val_mse: 2.7036e-05\n",
      "Epoch 319/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 5.6033e-05 - mse: 5.6033e-05 - val_loss: 3.1628e-05 - val_mse: 3.1628e-05\n",
      "Epoch 320/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 3.9024e-05 - mse: 3.9024e-05 - val_loss: 3.9001e-05 - val_mse: 3.9001e-05\n",
      "Epoch 321/1000\n",
      "66/66 [==============================] - 0s 413us/step - loss: 2.8442e-05 - mse: 2.8442e-05 - val_loss: 3.1137e-05 - val_mse: 3.1137e-05\n",
      "Epoch 322/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.6601e-05 - mse: 4.6601e-05 - val_loss: 1.0583e-04 - val_mse: 1.0583e-04\n",
      "Epoch 323/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.1330e-05 - mse: 4.1330e-05 - val_loss: 2.4025e-05 - val_mse: 2.4025e-05\n",
      "Epoch 324/1000\n",
      "66/66 [==============================] - 0s 476us/step - loss: 3.1540e-05 - mse: 3.1540e-05 - val_loss: 7.9858e-05 - val_mse: 7.9858e-05\n",
      "Epoch 325/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 8.0744e-05 - mse: 8.0744e-05 - val_loss: 6.6233e-05 - val_mse: 6.6233e-05\n",
      "Epoch 326/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.7859e-05 - mse: 4.7859e-05 - val_loss: 5.3617e-05 - val_mse: 5.3617e-05\n",
      "Epoch 327/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 4.6045e-05 - mse: 4.6045e-05 - val_loss: 7.5376e-05 - val_mse: 7.5376e-05\n",
      "Epoch 328/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 7.4910e-05 - mse: 7.4910e-05 - val_loss: 1.3490e-04 - val_mse: 1.3490e-04\n",
      "Epoch 329/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 8.0789e-05 - mse: 8.0789e-05 - val_loss: 5.4854e-05 - val_mse: 5.4854e-05\n",
      "Epoch 330/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 2.5038e-05 - mse: 2.5038e-05 - val_loss: 5.3578e-05 - val_mse: 5.3578e-05\n",
      "Epoch 331/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.5574e-05 - mse: 4.5574e-05 - val_loss: 2.8739e-05 - val_mse: 2.8739e-05\n",
      "Epoch 332/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 4.4283e-05 - mse: 4.4283e-05 - val_loss: 7.0460e-05 - val_mse: 7.0460e-05\n",
      "Epoch 333/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 6.8507e-05 - mse: 6.8507e-05 - val_loss: 5.7808e-05 - val_mse: 5.7808e-05\n",
      "Epoch 334/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 4.7616e-05 - mse: 4.7616e-05 - val_loss: 2.2094e-04 - val_mse: 2.2094e-04\n",
      "Epoch 335/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 6.5209e-05 - mse: 6.5209e-05 - val_loss: 3.2641e-05 - val_mse: 3.2641e-05\n",
      "Epoch 336/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 4.0717e-05 - mse: 4.0717e-05 - val_loss: 5.5149e-05 - val_mse: 5.5149e-05\n",
      "Epoch 337/1000\n",
      "66/66 [==============================] - 0s 758us/step - loss: 5.7550e-05 - mse: 5.7550e-05 - val_loss: 1.9229e-05 - val_mse: 1.9229e-05\n",
      "Epoch 338/1000\n",
      "66/66 [==============================] - 0s 834us/step - loss: 3.3176e-05 - mse: 3.3176e-05 - val_loss: 5.1412e-05 - val_mse: 5.1412e-05\n",
      "Epoch 339/1000\n",
      "66/66 [==============================] - 0s 621us/step - loss: 6.7515e-05 - mse: 6.7515e-05 - val_loss: 6.9798e-05 - val_mse: 6.9798e-05\n",
      "Epoch 340/1000\n",
      "66/66 [==============================] - 0s 530us/step - loss: 4.6589e-05 - mse: 4.6589e-05 - val_loss: 2.9110e-05 - val_mse: 2.9110e-05\n",
      "Epoch 341/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 2.9643e-05 - mse: 2.9643e-05 - val_loss: 6.1959e-05 - val_mse: 6.1959e-05\n",
      "Epoch 342/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.6476e-05 - mse: 3.6476e-05 - val_loss: 7.2949e-05 - val_mse: 7.2949e-05\n",
      "Epoch 343/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 3.7186e-05 - mse: 3.7186e-05 - val_loss: 3.3497e-05 - val_mse: 3.3497e-05\n",
      "Epoch 344/1000\n",
      "66/66 [==============================] - 0s 539us/step - loss: 5.7504e-05 - mse: 5.7504e-05 - val_loss: 1.0262e-04 - val_mse: 1.0262e-04\n",
      "Epoch 345/1000\n",
      "66/66 [==============================] - 0s 652us/step - loss: 6.7337e-05 - mse: 6.7337e-05 - val_loss: 3.2433e-05 - val_mse: 3.2433e-05\n",
      "Epoch 346/1000\n",
      "66/66 [==============================] - 0s 705us/step - loss: 3.1544e-05 - mse: 3.1544e-05 - val_loss: 5.3021e-05 - val_mse: 5.3021e-05\n",
      "Epoch 347/1000\n",
      "66/66 [==============================] - 0s 606us/step - loss: 5.3384e-05 - mse: 5.3384e-05 - val_loss: 7.7037e-05 - val_mse: 7.7037e-05\n",
      "Epoch 348/1000\n",
      "66/66 [==============================] - 0s 636us/step - loss: 2.5074e-05 - mse: 2.5074e-05 - val_loss: 2.8884e-05 - val_mse: 2.8884e-05\n",
      "Epoch 349/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.3221e-05 - mse: 3.3221e-05 - val_loss: 4.5279e-05 - val_mse: 4.5279e-05\n",
      "Epoch 350/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 5.0413e-05 - mse: 5.0413e-05 - val_loss: 6.2057e-05 - val_mse: 6.2057e-05\n",
      "Epoch 351/1000\n",
      "66/66 [==============================] - 0s 456us/step - loss: 3.7119e-05 - mse: 3.7119e-05 - val_loss: 1.5491e-04 - val_mse: 1.5491e-04\n",
      "Epoch 352/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 4.6034e-05 - mse: 4.6034e-05 - val_loss: 3.5380e-05 - val_mse: 3.5380e-05\n",
      "Epoch 353/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 5.0136e-05 - mse: 5.0136e-05 - val_loss: 7.7297e-05 - val_mse: 7.7297e-05\n",
      "Epoch 354/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.8651e-05 - mse: 3.8651e-05 - val_loss: 4.0365e-05 - val_mse: 4.0365e-05\n",
      "Epoch 355/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 4.8555e-05 - mse: 4.8555e-05 - val_loss: 8.7187e-05 - val_mse: 8.7187e-05\n",
      "Epoch 356/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 3.1755e-05 - mse: 3.1755e-05 - val_loss: 4.8156e-05 - val_mse: 4.8156e-05\n",
      "Epoch 357/1000\n",
      "66/66 [==============================] - 0s 576us/step - loss: 4.1686e-05 - mse: 4.1686e-05 - val_loss: 8.8916e-05 - val_mse: 8.8916e-05\n",
      "Epoch 358/1000\n",
      "66/66 [==============================] - 0s 531us/step - loss: 3.3337e-05 - mse: 3.3337e-05 - val_loss: 3.8741e-05 - val_mse: 3.8741e-05\n",
      "Epoch 359/1000\n",
      "66/66 [==============================] - 0s 667us/step - loss: 4.6940e-05 - mse: 4.6940e-05 - val_loss: 6.1944e-05 - val_mse: 6.1944e-05\n",
      "Epoch 360/1000\n",
      "66/66 [==============================] - 0s 700us/step - loss: 3.0555e-05 - mse: 3.0555e-05 - val_loss: 2.0590e-05 - val_mse: 2.0590e-05\n",
      "Epoch 361/1000\n",
      "66/66 [==============================] - 0s 652us/step - loss: 3.1614e-05 - mse: 3.1614e-05 - val_loss: 3.1926e-05 - val_mse: 3.1926e-05\n",
      "Epoch 362/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 3.7880e-05 - mse: 3.7880e-05 - val_loss: 3.1366e-05 - val_mse: 3.1366e-05\n",
      "Epoch 363/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 1.7555e-05 - mse: 1.7555e-05 - val_loss: 2.8977e-05 - val_mse: 2.8977e-05\n",
      "Epoch 364/1000\n",
      "66/66 [==============================] - 0s 530us/step - loss: 3.3984e-05 - mse: 3.3984e-05 - val_loss: 6.4496e-05 - val_mse: 6.4496e-05\n",
      "Epoch 365/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 4.1341e-05 - mse: 4.1341e-05 - val_loss: 6.7876e-05 - val_mse: 6.7876e-05\n",
      "Epoch 366/1000\n",
      "66/66 [==============================] - 0s 387us/step - loss: 3.8381e-05 - mse: 3.8381e-05 - val_loss: 6.6160e-05 - val_mse: 6.6160e-05\n",
      "Epoch 367/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.3609e-05 - mse: 3.3609e-05 - val_loss: 8.6619e-05 - val_mse: 8.6619e-05\n",
      "Epoch 368/1000\n",
      "66/66 [==============================] - 0s 530us/step - loss: 6.3134e-05 - mse: 6.3134e-05 - val_loss: 2.7082e-05 - val_mse: 2.7082e-05\n",
      "Epoch 369/1000\n",
      "66/66 [==============================] - 0s 537us/step - loss: 1.6999e-05 - mse: 1.6999e-05 - val_loss: 1.6895e-05 - val_mse: 1.6895e-05\n",
      "Epoch 370/1000\n",
      "66/66 [==============================] - 0s 403us/step - loss: 3.7826e-05 - mse: 3.7826e-05 - val_loss: 8.9755e-05 - val_mse: 8.9755e-05\n",
      "Epoch 371/1000\n",
      "66/66 [==============================] - 0s 387us/step - loss: 3.2086e-05 - mse: 3.2086e-05 - val_loss: 3.5786e-05 - val_mse: 3.5786e-05\n",
      "Epoch 372/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 3.3600e-05 - mse: 3.3600e-05 - val_loss: 2.9238e-05 - val_mse: 2.9238e-05\n",
      "Epoch 373/1000\n",
      "66/66 [==============================] - 0s 425us/step - loss: 3.8423e-05 - mse: 3.8423e-05 - val_loss: 3.8519e-05 - val_mse: 3.8519e-05\n",
      "Epoch 374/1000\n",
      "66/66 [==============================] - 0s 423us/step - loss: 3.2973e-05 - mse: 3.2973e-05 - val_loss: 5.5845e-05 - val_mse: 5.5845e-05\n",
      "Epoch 375/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 5.2003e-05 - mse: 5.2003e-05 - val_loss: 9.0326e-05 - val_mse: 9.0326e-05\n",
      "Epoch 376/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.8423e-05 - mse: 4.8423e-05 - val_loss: 3.0788e-05 - val_mse: 3.0788e-05\n",
      "Epoch 377/1000\n",
      "66/66 [==============================] - 0s 451us/step - loss: 3.3187e-05 - mse: 3.3187e-05 - val_loss: 2.8216e-05 - val_mse: 2.8216e-05\n",
      "Epoch 378/1000\n",
      "66/66 [==============================] - 0s 698us/step - loss: 4.3412e-05 - mse: 4.3412e-05 - val_loss: 2.5806e-05 - val_mse: 2.5806e-05\n",
      "Epoch 379/1000\n",
      "66/66 [==============================] - 0s 652us/step - loss: 1.6388e-05 - mse: 1.6388e-05 - val_loss: 5.4206e-05 - val_mse: 5.4206e-05\n",
      "Epoch 380/1000\n",
      "66/66 [==============================] - 0s 606us/step - loss: 3.2464e-05 - mse: 3.2464e-05 - val_loss: 7.7665e-05 - val_mse: 7.7665e-05\n",
      "Epoch 381/1000\n",
      "66/66 [==============================] - 0s 652us/step - loss: 3.5590e-05 - mse: 3.5590e-05 - val_loss: 2.2103e-05 - val_mse: 2.2103e-05\n",
      "Epoch 382/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 3.2207e-05 - mse: 3.2207e-05 - val_loss: 3.5475e-05 - val_mse: 3.5475e-05\n",
      "Epoch 383/1000\n",
      "66/66 [==============================] - 0s 531us/step - loss: 3.6085e-05 - mse: 3.6085e-05 - val_loss: 2.5181e-05 - val_mse: 2.5181e-05\n",
      "Epoch 384/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.7317e-05 - mse: 3.7317e-05 - val_loss: 5.0887e-05 - val_mse: 5.0887e-05\n",
      "Epoch 385/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 6.5025e-05 - mse: 6.5025e-05 - val_loss: 5.9512e-05 - val_mse: 5.9512e-05\n",
      "Epoch 386/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 3.7675e-05 - mse: 3.7675e-05 - val_loss: 4.2012e-05 - val_mse: 4.2012e-05\n",
      "Epoch 387/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 5.9939e-05 - mse: 5.9939e-05 - val_loss: 3.6459e-04 - val_mse: 3.6459e-04\n",
      "Epoch 388/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.6139e-05 - mse: 4.6139e-05 - val_loss: 3.9163e-05 - val_mse: 3.9163e-05\n",
      "Epoch 389/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.1955e-05 - mse: 2.1955e-05 - val_loss: 4.6619e-05 - val_mse: 4.6619e-05\n",
      "Epoch 390/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 5.5722e-05 - mse: 5.5722e-05 - val_loss: 3.7029e-05 - val_mse: 3.7029e-05\n",
      "Epoch 391/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.7638e-05 - mse: 2.7638e-05 - val_loss: 3.4043e-05 - val_mse: 3.4043e-05\n",
      "Epoch 392/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 4.5263e-05 - mse: 4.5263e-05 - val_loss: 3.6507e-05 - val_mse: 3.6507e-05\n",
      "Epoch 393/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 4.4602e-05 - mse: 4.4602e-05 - val_loss: 2.2090e-05 - val_mse: 2.2090e-05\n",
      "Epoch 394/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.1090e-05 - mse: 2.1090e-05 - val_loss: 3.9481e-05 - val_mse: 3.9481e-05\n",
      "Epoch 395/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.3203e-05 - mse: 4.3203e-05 - val_loss: 4.4736e-05 - val_mse: 4.4736e-05\n",
      "Epoch 396/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.7745e-05 - mse: 4.7745e-05 - val_loss: 3.2872e-05 - val_mse: 3.2872e-05\n",
      "Epoch 397/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 2.3702e-05 - mse: 2.3702e-05 - val_loss: 2.9600e-05 - val_mse: 2.9600e-05\n",
      "Epoch 398/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.4010e-05 - mse: 4.4010e-05 - val_loss: 6.5007e-05 - val_mse: 6.5007e-05\n",
      "Epoch 399/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.3706e-05 - mse: 4.3706e-05 - val_loss: 6.3773e-05 - val_mse: 6.3773e-05\n",
      "Epoch 400/1000\n",
      "66/66 [==============================] - 0s 387us/step - loss: 5.0045e-05 - mse: 5.0045e-05 - val_loss: 3.0176e-05 - val_mse: 3.0176e-05\n",
      "Epoch 401/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 3.3175e-05 - mse: 3.3175e-05 - val_loss: 9.2303e-05 - val_mse: 9.2303e-05\n",
      "Epoch 402/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 7.8427e-05 - mse: 7.8427e-05 - val_loss: 2.1998e-05 - val_mse: 2.1998e-05\n",
      "Epoch 403/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.9041e-05 - mse: 4.9041e-05 - val_loss: 2.5813e-05 - val_mse: 2.5813e-05\n",
      "Epoch 404/1000\n",
      "66/66 [==============================] - 0s 462us/step - loss: 1.6131e-05 - mse: 1.6131e-05 - val_loss: 3.5443e-05 - val_mse: 3.5443e-05\n",
      "Epoch 405/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 3.6368e-05 - mse: 3.6368e-05 - val_loss: 2.0583e-05 - val_mse: 2.0583e-05\n",
      "Epoch 406/1000\n",
      "66/66 [==============================] - 0s 561us/step - loss: 3.3901e-05 - mse: 3.3901e-05 - val_loss: 1.6703e-05 - val_mse: 1.6703e-05\n",
      "Epoch 407/1000\n",
      "66/66 [==============================] - 0s 493us/step - loss: 2.8265e-05 - mse: 2.8265e-05 - val_loss: 5.3983e-05 - val_mse: 5.3983e-05\n",
      "Epoch 408/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 4.6910e-05 - mse: 4.6910e-05 - val_loss: 3.3850e-05 - val_mse: 3.3850e-05\n",
      "Epoch 409/1000\n",
      "66/66 [==============================] - 0s 644us/step - loss: 2.2859e-05 - mse: 2.2859e-05 - val_loss: 1.6327e-05 - val_mse: 1.6327e-05\n",
      "Epoch 410/1000\n",
      "66/66 [==============================] - 0s 560us/step - loss: 2.4393e-05 - mse: 2.4393e-05 - val_loss: 2.9657e-05 - val_mse: 2.9657e-05\n",
      "Epoch 411/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.5689e-05 - mse: 3.5689e-05 - val_loss: 7.7325e-05 - val_mse: 7.7325e-05\n",
      "Epoch 412/1000\n",
      "66/66 [==============================] - 0s 433us/step - loss: 2.2292e-05 - mse: 2.2292e-05 - val_loss: 4.1154e-05 - val_mse: 4.1154e-05\n",
      "Epoch 413/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 3.4262e-05 - mse: 3.4262e-05 - val_loss: 3.2567e-05 - val_mse: 3.2567e-05\n",
      "Epoch 414/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 5.5182e-05 - mse: 5.5182e-05 - val_loss: 6.3559e-05 - val_mse: 6.3559e-05\n",
      "Epoch 415/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.2227e-05 - mse: 3.2227e-05 - val_loss: 2.9446e-05 - val_mse: 2.9446e-05\n",
      "Epoch 416/1000\n",
      "66/66 [==============================] - 0s 462us/step - loss: 2.3385e-05 - mse: 2.3385e-05 - val_loss: 2.6658e-05 - val_mse: 2.6658e-05\n",
      "Epoch 417/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.4715e-05 - mse: 3.4715e-05 - val_loss: 5.5082e-05 - val_mse: 5.5082e-05\n",
      "Epoch 418/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 7.8500e-05 - mse: 7.8500e-05 - val_loss: 5.5954e-05 - val_mse: 5.5954e-05\n",
      "Epoch 419/1000\n",
      "66/66 [==============================] - 0s 434us/step - loss: 2.4550e-05 - mse: 2.4550e-05 - val_loss: 4.3328e-05 - val_mse: 4.3328e-05\n",
      "Epoch 420/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 5.4403e-05 - mse: 5.4403e-05 - val_loss: 4.6833e-05 - val_mse: 4.6833e-05\n",
      "Epoch 421/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.6929e-05 - mse: 2.6929e-05 - val_loss: 4.0895e-05 - val_mse: 4.0895e-05\n",
      "Epoch 422/1000\n",
      "66/66 [==============================] - 0s 387us/step - loss: 2.6682e-05 - mse: 2.6682e-05 - val_loss: 5.5207e-05 - val_mse: 5.5207e-05\n",
      "Epoch 423/1000\n",
      "66/66 [==============================] - 0s 441us/step - loss: 4.9210e-05 - mse: 4.9210e-05 - val_loss: 4.7918e-05 - val_mse: 4.7918e-05\n",
      "Epoch 424/1000\n",
      "66/66 [==============================] - 0s 501us/step - loss: 4.1772e-05 - mse: 4.1772e-05 - val_loss: 1.5256e-05 - val_mse: 1.5256e-05\n",
      "Epoch 425/1000\n",
      "66/66 [==============================] - 0s 458us/step - loss: 2.7902e-05 - mse: 2.7902e-05 - val_loss: 6.8097e-05 - val_mse: 6.8097e-05\n",
      "Epoch 426/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.0055e-05 - mse: 3.0055e-05 - val_loss: 3.6797e-05 - val_mse: 3.6797e-05\n",
      "Epoch 427/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.6278e-05 - mse: 2.6278e-05 - val_loss: 1.7580e-05 - val_mse: 1.7580e-05\n",
      "Epoch 428/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.6081e-05 - mse: 4.6081e-05 - val_loss: 2.3816e-05 - val_mse: 2.3816e-05\n",
      "Epoch 429/1000\n",
      "66/66 [==============================] - 0s 462us/step - loss: 2.1574e-05 - mse: 2.1574e-05 - val_loss: 4.6609e-05 - val_mse: 4.6609e-05\n",
      "Epoch 430/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 2.6992e-05 - mse: 2.6992e-05 - val_loss: 1.6230e-05 - val_mse: 1.6230e-05\n",
      "Epoch 431/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.2925e-05 - mse: 4.2925e-05 - val_loss: 7.4516e-05 - val_mse: 7.4516e-05\n",
      "Epoch 432/1000\n",
      "66/66 [==============================] - 0s 404us/step - loss: 5.2810e-05 - mse: 5.2810e-05 - val_loss: 1.7482e-05 - val_mse: 1.7482e-05\n",
      "Epoch 433/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.0659e-05 - mse: 2.0659e-05 - val_loss: 4.8253e-05 - val_mse: 4.8253e-05\n",
      "Epoch 434/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 6.1268e-05 - mse: 6.1268e-05 - val_loss: 3.7691e-05 - val_mse: 3.7691e-05\n",
      "Epoch 435/1000\n",
      "66/66 [==============================] - 0s 448us/step - loss: 2.8914e-05 - mse: 2.8914e-05 - val_loss: 1.5925e-05 - val_mse: 1.5925e-05\n",
      "Epoch 436/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 8.8269e-05 - mse: 8.8269e-05 - val_loss: 6.7334e-05 - val_mse: 6.7334e-05\n",
      "Epoch 437/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 3.4736e-05 - mse: 3.4736e-05 - val_loss: 3.2764e-05 - val_mse: 3.2764e-05\n",
      "Epoch 438/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 4.5918e-05 - mse: 4.5918e-05 - val_loss: 4.6452e-05 - val_mse: 4.6452e-05\n",
      "Epoch 439/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 4.2885e-05 - mse: 4.2885e-05 - val_loss: 3.4377e-05 - val_mse: 3.4377e-05\n",
      "Epoch 440/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 2.8271e-05 - mse: 2.8271e-05 - val_loss: 4.8213e-05 - val_mse: 4.8213e-05\n",
      "Epoch 441/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.8530e-05 - mse: 2.8530e-05 - val_loss: 7.0766e-05 - val_mse: 7.0766e-05\n",
      "Epoch 442/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 3.2649e-05 - mse: 3.2649e-05 - val_loss: 4.1430e-05 - val_mse: 4.1430e-05\n",
      "Epoch 443/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 6.3103e-05 - mse: 6.3103e-05 - val_loss: 1.2846e-04 - val_mse: 1.2846e-04\n",
      "Epoch 444/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.0807e-05 - mse: 3.0807e-05 - val_loss: 4.3783e-05 - val_mse: 4.3783e-05\n",
      "Epoch 445/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 3.1073e-05 - mse: 3.1073e-05 - val_loss: 4.3524e-05 - val_mse: 4.3524e-05\n",
      "Epoch 446/1000\n",
      "66/66 [==============================] - 0s 546us/step - loss: 2.6873e-05 - mse: 2.6873e-05 - val_loss: 1.3862e-05 - val_mse: 1.3862e-05\n",
      "Epoch 447/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.4846e-05 - mse: 3.4846e-05 - val_loss: 1.6043e-04 - val_mse: 1.6043e-04\n",
      "Epoch 448/1000\n",
      "66/66 [==============================] - 0s 449us/step - loss: 4.0249e-05 - mse: 4.0249e-05 - val_loss: 1.5858e-05 - val_mse: 1.5858e-05\n",
      "Epoch 449/1000\n",
      "66/66 [==============================] - 0s 453us/step - loss: 2.3490e-05 - mse: 2.3490e-05 - val_loss: 2.0443e-05 - val_mse: 2.0443e-05\n",
      "Epoch 450/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 3.5294e-05 - mse: 3.5294e-05 - val_loss: 5.5988e-05 - val_mse: 5.5988e-05\n",
      "Epoch 451/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 4.5717e-05 - mse: 4.5717e-05 - val_loss: 3.1123e-05 - val_mse: 3.1123e-05\n",
      "Epoch 452/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.7698e-05 - mse: 3.7698e-05 - val_loss: 3.1622e-05 - val_mse: 3.1622e-05\n",
      "Epoch 453/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 1.9604e-05 - mse: 1.9604e-05 - val_loss: 5.5682e-05 - val_mse: 5.5682e-05\n",
      "Epoch 454/1000\n",
      "66/66 [==============================] - 0s 487us/step - loss: 4.8079e-05 - mse: 4.8079e-05 - val_loss: 4.3334e-05 - val_mse: 4.3334e-05\n",
      "Epoch 455/1000\n",
      "66/66 [==============================] - 0s 473us/step - loss: 2.8204e-05 - mse: 2.8204e-05 - val_loss: 3.6508e-05 - val_mse: 3.6508e-05\n",
      "Epoch 456/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 4.6759e-05 - mse: 4.6759e-05 - val_loss: 2.5426e-05 - val_mse: 2.5426e-05\n",
      "Epoch 457/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 1.4968e-05 - mse: 1.4968e-05 - val_loss: 6.7463e-05 - val_mse: 6.7463e-05\n",
      "Epoch 458/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 5.7988e-05 - mse: 5.7988e-05 - val_loss: 5.6050e-05 - val_mse: 5.6050e-05\n",
      "Epoch 459/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.4134e-05 - mse: 3.4134e-05 - val_loss: 4.6278e-05 - val_mse: 4.6278e-05\n",
      "Epoch 460/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 4.1072e-05 - mse: 4.1072e-05 - val_loss: 5.7704e-05 - val_mse: 5.7704e-05\n",
      "Epoch 461/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 1.9513e-05 - mse: 1.9513e-05 - val_loss: 1.0794e-05 - val_mse: 1.0794e-05\n",
      "Epoch 462/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 2.9913e-05 - mse: 2.9913e-05 - val_loss: 3.0244e-05 - val_mse: 3.0244e-05\n",
      "Epoch 463/1000\n",
      "66/66 [==============================] - 0s 472us/step - loss: 2.5126e-05 - mse: 2.5126e-05 - val_loss: 5.0532e-05 - val_mse: 5.0532e-05\n",
      "Epoch 464/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 3.3855e-05 - mse: 3.3855e-05 - val_loss: 5.8546e-05 - val_mse: 5.8546e-05\n",
      "Epoch 465/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 4.8127e-05 - mse: 4.8127e-05 - val_loss: 1.4233e-05 - val_mse: 1.4233e-05\n",
      "Epoch 466/1000\n",
      "66/66 [==============================] - 0s 466us/step - loss: 1.1348e-05 - mse: 1.1348e-05 - val_loss: 6.2023e-05 - val_mse: 6.2023e-05\n",
      "Epoch 467/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 4.4009e-05 - mse: 4.4009e-05 - val_loss: 6.8518e-05 - val_mse: 6.8518e-05\n",
      "Epoch 468/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.2838e-05 - mse: 3.2838e-05 - val_loss: 2.8430e-05 - val_mse: 2.8430e-05\n",
      "Epoch 469/1000\n",
      "66/66 [==============================] - 0s 448us/step - loss: 6.6967e-05 - mse: 6.6967e-05 - val_loss: 2.8040e-05 - val_mse: 2.8040e-05\n",
      "Epoch 470/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 1.5751e-05 - mse: 1.5751e-05 - val_loss: 1.6131e-05 - val_mse: 1.6131e-05\n",
      "Epoch 471/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.1690e-05 - mse: 2.1690e-05 - val_loss: 2.9150e-05 - val_mse: 2.9150e-05\n",
      "Epoch 472/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.3693e-05 - mse: 4.3693e-05 - val_loss: 2.4868e-05 - val_mse: 2.4868e-05\n",
      "Epoch 473/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 4.4648e-05 - mse: 4.4648e-05 - val_loss: 4.3088e-05 - val_mse: 4.3088e-05\n",
      "Epoch 474/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 3.5881e-05 - mse: 3.5881e-05 - val_loss: 2.4249e-05 - val_mse: 2.4249e-05\n",
      "Epoch 475/1000\n",
      "66/66 [==============================] - 0s 425us/step - loss: 2.8534e-05 - mse: 2.8534e-05 - val_loss: 4.7161e-05 - val_mse: 4.7161e-05\n",
      "Epoch 476/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 5.5647e-05 - mse: 5.5647e-05 - val_loss: 4.7540e-05 - val_mse: 4.7540e-05\n",
      "Epoch 477/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 4.0047e-05 - mse: 4.0047e-05 - val_loss: 1.8945e-05 - val_mse: 1.8945e-05\n",
      "Epoch 478/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.1853e-05 - mse: 4.1853e-05 - val_loss: 1.1276e-04 - val_mse: 1.1276e-04\n",
      "Epoch 479/1000\n",
      "66/66 [==============================] - 0s 413us/step - loss: 2.3413e-05 - mse: 2.3413e-05 - val_loss: 1.5398e-05 - val_mse: 1.5398e-05\n",
      "Epoch 480/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.9085e-05 - mse: 2.9085e-05 - val_loss: 4.9097e-05 - val_mse: 4.9097e-05\n",
      "Epoch 481/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 3.6789e-05 - mse: 3.6789e-05 - val_loss: 5.5813e-05 - val_mse: 5.5813e-05\n",
      "Epoch 482/1000\n",
      "66/66 [==============================] - 0s 431us/step - loss: 1.8929e-05 - mse: 1.8929e-05 - val_loss: 1.8873e-05 - val_mse: 1.8873e-05\n",
      "Epoch 483/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.6206e-05 - mse: 4.6206e-05 - val_loss: 4.4095e-05 - val_mse: 4.4095e-05\n",
      "Epoch 484/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 1.6013e-05 - mse: 1.6013e-05 - val_loss: 3.9243e-05 - val_mse: 3.9243e-05\n",
      "Epoch 485/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 2.6356e-05 - mse: 2.6356e-05 - val_loss: 4.2372e-05 - val_mse: 4.2372e-05\n",
      "Epoch 486/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.5112e-05 - mse: 3.5112e-05 - val_loss: 5.8858e-05 - val_mse: 5.8858e-05\n",
      "Epoch 487/1000\n",
      "66/66 [==============================] - 0s 456us/step - loss: 3.2520e-05 - mse: 3.2520e-05 - val_loss: 1.2034e-05 - val_mse: 1.2034e-05\n",
      "Epoch 488/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 2.6970e-05 - mse: 2.6970e-05 - val_loss: 2.0179e-05 - val_mse: 2.0179e-05\n",
      "Epoch 489/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 5.4566e-05 - mse: 5.4566e-05 - val_loss: 3.7486e-05 - val_mse: 3.7486e-05\n",
      "Epoch 490/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.3062e-05 - mse: 3.3062e-05 - val_loss: 1.9061e-05 - val_mse: 1.9061e-05\n",
      "Epoch 491/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 3.9913e-05 - mse: 3.9913e-05 - val_loss: 4.4863e-05 - val_mse: 4.4863e-05\n",
      "Epoch 492/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 2.4235e-05 - mse: 2.4235e-05 - val_loss: 2.6808e-05 - val_mse: 2.6808e-05\n",
      "Epoch 493/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 4.0903e-05 - mse: 4.0903e-05 - val_loss: 6.3299e-05 - val_mse: 6.3299e-05\n",
      "Epoch 494/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.5782e-05 - mse: 2.5782e-05 - val_loss: 1.7546e-05 - val_mse: 1.7546e-05\n",
      "Epoch 495/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 4.4306e-05 - mse: 4.4306e-05 - val_loss: 5.7237e-05 - val_mse: 5.7237e-05\n",
      "Epoch 496/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 3.2517e-05 - mse: 3.2517e-05 - val_loss: 2.9608e-05 - val_mse: 2.9608e-05\n",
      "Epoch 497/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 3.3467e-05 - mse: 3.3467e-05 - val_loss: 5.7811e-05 - val_mse: 5.7811e-05\n",
      "Epoch 498/1000\n",
      "66/66 [==============================] - 0s 448us/step - loss: 4.5948e-05 - mse: 4.5948e-05 - val_loss: 3.8823e-05 - val_mse: 3.8823e-05\n",
      "Epoch 499/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 1.8761e-05 - mse: 1.8761e-05 - val_loss: 1.6483e-05 - val_mse: 1.6483e-05\n",
      "Epoch 500/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.6834e-05 - mse: 3.6834e-05 - val_loss: 1.0144e-04 - val_mse: 1.0144e-04\n",
      "Epoch 501/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 3.6085e-05 - mse: 3.6085e-05 - val_loss: 2.9072e-05 - val_mse: 2.9072e-05\n",
      "Epoch 502/1000\n",
      "66/66 [==============================] - 0s 456us/step - loss: 2.5820e-05 - mse: 2.5820e-05 - val_loss: 5.3380e-05 - val_mse: 5.3380e-05\n",
      "Epoch 503/1000\n",
      "66/66 [==============================] - 0s 453us/step - loss: 2.5970e-05 - mse: 2.5970e-05 - val_loss: 2.1791e-05 - val_mse: 2.1791e-05\n",
      "Epoch 504/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.4033e-05 - mse: 2.4033e-05 - val_loss: 3.1162e-05 - val_mse: 3.1162e-05\n",
      "Epoch 505/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 2.0381e-05 - mse: 2.0381e-05 - val_loss: 3.8380e-05 - val_mse: 3.8380e-05\n",
      "Epoch 506/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.2407e-05 - mse: 3.2407e-05 - val_loss: 2.8013e-05 - val_mse: 2.8013e-05\n",
      "Epoch 507/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 2.4821e-05 - mse: 2.4821e-05 - val_loss: 4.5890e-05 - val_mse: 4.5890e-05\n",
      "Epoch 508/1000\n",
      "66/66 [==============================] - 0s 425us/step - loss: 3.2318e-05 - mse: 3.2318e-05 - val_loss: 9.1406e-05 - val_mse: 9.1406e-05\n",
      "Epoch 509/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 4.7925e-05 - mse: 4.7925e-05 - val_loss: 3.5521e-05 - val_mse: 3.5521e-05\n",
      "Epoch 510/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.7733e-05 - mse: 1.7733e-05 - val_loss: 3.2262e-05 - val_mse: 3.2262e-05\n",
      "Epoch 511/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 2.1842e-05 - mse: 2.1842e-05 - val_loss: 5.3342e-05 - val_mse: 5.3342e-05\n",
      "Epoch 512/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.3312e-05 - mse: 4.3312e-05 - val_loss: 9.3970e-05 - val_mse: 9.3970e-05\n",
      "Epoch 513/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.9908e-05 - mse: 1.9908e-05 - val_loss: 2.8237e-05 - val_mse: 2.8237e-05\n",
      "Epoch 514/1000\n",
      "66/66 [==============================] - 0s 411us/step - loss: 3.4926e-05 - mse: 3.4926e-05 - val_loss: 4.7558e-05 - val_mse: 4.7558e-05\n",
      "Epoch 515/1000\n",
      "66/66 [==============================] - 0s 433us/step - loss: 2.6141e-05 - mse: 2.6141e-05 - val_loss: 4.2146e-05 - val_mse: 4.2146e-05\n",
      "Epoch 516/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.9608e-05 - mse: 2.9608e-05 - val_loss: 3.7905e-05 - val_mse: 3.7905e-05\n",
      "Epoch 517/1000\n",
      "66/66 [==============================] - 0s 546us/step - loss: 2.9952e-05 - mse: 2.9952e-05 - val_loss: 9.8396e-06 - val_mse: 9.8396e-06\n",
      "Epoch 518/1000\n",
      "66/66 [==============================] - 0s 501us/step - loss: 2.4188e-05 - mse: 2.4188e-05 - val_loss: 4.2618e-05 - val_mse: 4.2618e-05\n",
      "Epoch 519/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.6666e-05 - mse: 2.6666e-05 - val_loss: 5.0566e-05 - val_mse: 5.0566e-05\n",
      "Epoch 520/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 2.3134e-05 - mse: 2.3134e-05 - val_loss: 6.3295e-05 - val_mse: 6.3295e-05\n",
      "Epoch 521/1000\n",
      "66/66 [==============================] - 0s 501us/step - loss: 2.7037e-05 - mse: 2.7037e-05 - val_loss: 2.7842e-05 - val_mse: 2.7842e-05\n",
      "Epoch 522/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 2.1486e-05 - mse: 2.1486e-05 - val_loss: 4.0139e-05 - val_mse: 4.0139e-05\n",
      "Epoch 523/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 1.7159e-05 - mse: 1.7159e-05 - val_loss: 2.4569e-05 - val_mse: 2.4569e-05\n",
      "Epoch 524/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 4.7004e-05 - mse: 4.7004e-05 - val_loss: 1.6088e-05 - val_mse: 1.6088e-05\n",
      "Epoch 525/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.4485e-05 - mse: 1.4485e-05 - val_loss: 3.9421e-05 - val_mse: 3.9421e-05\n",
      "Epoch 526/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.9621e-05 - mse: 2.9621e-05 - val_loss: 1.7334e-05 - val_mse: 1.7334e-05\n",
      "Epoch 527/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 4.2345e-05 - mse: 4.2345e-05 - val_loss: 1.5829e-04 - val_mse: 1.5829e-04\n",
      "Epoch 528/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 6.2908e-05 - mse: 6.2908e-05 - val_loss: 3.0058e-05 - val_mse: 3.0058e-05\n",
      "Epoch 529/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.1943e-05 - mse: 2.1943e-05 - val_loss: 3.5756e-05 - val_mse: 3.5756e-05\n",
      "Epoch 530/1000\n",
      "66/66 [==============================] - 0s 463us/step - loss: 4.9719e-05 - mse: 4.9719e-05 - val_loss: 7.1325e-05 - val_mse: 7.1325e-05\n",
      "Epoch 531/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 3.4405e-05 - mse: 3.4405e-05 - val_loss: 1.2769e-05 - val_mse: 1.2769e-05\n",
      "Epoch 532/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 3.4170e-05 - mse: 3.4170e-05 - val_loss: 5.0413e-05 - val_mse: 5.0413e-05\n",
      "Epoch 533/1000\n",
      "66/66 [==============================] - 0s 462us/step - loss: 2.3935e-05 - mse: 2.3935e-05 - val_loss: 4.5015e-05 - val_mse: 4.5015e-05\n",
      "Epoch 534/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.7224e-05 - mse: 3.7224e-05 - val_loss: 2.8795e-05 - val_mse: 2.8795e-05\n",
      "Epoch 535/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.2360e-05 - mse: 2.2360e-05 - val_loss: 3.7897e-05 - val_mse: 3.7897e-05\n",
      "Epoch 536/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 2.2979e-05 - mse: 2.2979e-05 - val_loss: 2.6739e-05 - val_mse: 2.6739e-05\n",
      "Epoch 537/1000\n",
      "66/66 [==============================] - 0s 474us/step - loss: 3.5483e-05 - mse: 3.5483e-05 - val_loss: 1.4644e-05 - val_mse: 1.4644e-05\n",
      "Epoch 538/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.0467e-05 - mse: 2.0467e-05 - val_loss: 4.8364e-05 - val_mse: 4.8364e-05\n",
      "Epoch 539/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 3.9624e-05 - mse: 3.9624e-05 - val_loss: 4.2378e-05 - val_mse: 4.2378e-05\n",
      "Epoch 540/1000\n",
      "66/66 [==============================] - 0s 504us/step - loss: 2.5929e-05 - mse: 2.5929e-05 - val_loss: 2.4410e-05 - val_mse: 2.4410e-05\n",
      "Epoch 541/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.7533e-05 - mse: 2.7533e-05 - val_loss: 1.8384e-05 - val_mse: 1.8384e-05\n",
      "Epoch 542/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 3.1932e-05 - mse: 3.1932e-05 - val_loss: 1.0132e-04 - val_mse: 1.0132e-04\n",
      "Epoch 543/1000\n",
      "66/66 [==============================] - 0s 459us/step - loss: 3.4150e-05 - mse: 3.4150e-05 - val_loss: 1.2404e-05 - val_mse: 1.2404e-05\n",
      "Epoch 544/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 2.7777e-05 - mse: 2.7777e-05 - val_loss: 2.0006e-05 - val_mse: 2.0006e-05\n",
      "Epoch 545/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.5863e-05 - mse: 2.5863e-05 - val_loss: 2.0041e-05 - val_mse: 2.0041e-05\n",
      "Epoch 546/1000\n",
      "66/66 [==============================] - 0s 508us/step - loss: 2.4224e-05 - mse: 2.4224e-05 - val_loss: 9.7608e-06 - val_mse: 9.7608e-06\n",
      "Epoch 547/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.6047e-05 - mse: 1.6047e-05 - val_loss: 9.8773e-05 - val_mse: 9.8773e-05\n",
      "Epoch 548/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 3.5635e-05 - mse: 3.5635e-05 - val_loss: 1.2389e-05 - val_mse: 1.2389e-05\n",
      "Epoch 549/1000\n",
      "66/66 [==============================] - 0s 448us/step - loss: 3.5254e-05 - mse: 3.5254e-05 - val_loss: 3.2968e-05 - val_mse: 3.2968e-05\n",
      "Epoch 550/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 1.6783e-05 - mse: 1.6783e-05 - val_loss: 3.9132e-05 - val_mse: 3.9132e-05\n",
      "Epoch 551/1000\n",
      "66/66 [==============================] - 0s 469us/step - loss: 2.2974e-05 - mse: 2.2974e-05 - val_loss: 4.0158e-05 - val_mse: 4.0158e-05\n",
      "Epoch 552/1000\n",
      "66/66 [==============================] - 0s 482us/step - loss: 2.6796e-05 - mse: 2.6796e-05 - val_loss: 2.9110e-05 - val_mse: 2.9110e-05\n",
      "Epoch 553/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 2.7008e-05 - mse: 2.7008e-05 - val_loss: 4.4516e-05 - val_mse: 4.4516e-05\n",
      "Epoch 554/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.9350e-05 - mse: 3.9350e-05 - val_loss: 4.1213e-05 - val_mse: 4.1213e-05\n",
      "Epoch 555/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.5699e-05 - mse: 4.5699e-05 - val_loss: 1.2773e-04 - val_mse: 1.2773e-04\n",
      "Epoch 556/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.2354e-05 - mse: 2.2354e-05 - val_loss: 4.6555e-05 - val_mse: 4.6555e-05\n",
      "Epoch 557/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 2.5114e-05 - mse: 2.5114e-05 - val_loss: 3.4624e-05 - val_mse: 3.4624e-05\n",
      "Epoch 558/1000\n",
      "66/66 [==============================] - 0s 427us/step - loss: 1.5935e-05 - mse: 1.5935e-05 - val_loss: 2.2626e-05 - val_mse: 2.2626e-05\n",
      "Epoch 559/1000\n",
      "66/66 [==============================] - 0s 427us/step - loss: 2.5727e-05 - mse: 2.5727e-05 - val_loss: 3.0104e-05 - val_mse: 3.0104e-05\n",
      "Epoch 560/1000\n",
      "66/66 [==============================] - 0s 425us/step - loss: 3.4165e-05 - mse: 3.4165e-05 - val_loss: 5.5157e-05 - val_mse: 5.5157e-05\n",
      "Epoch 561/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 6.2049e-05 - mse: 6.2049e-05 - val_loss: 1.1463e-04 - val_mse: 1.1463e-04\n",
      "Epoch 562/1000\n",
      "66/66 [==============================] - 0s 508us/step - loss: 5.2835e-05 - mse: 5.2835e-05 - val_loss: 5.0509e-05 - val_mse: 5.0509e-05\n",
      "Epoch 563/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.1882e-05 - mse: 3.1882e-05 - val_loss: 1.0023e-04 - val_mse: 1.0023e-04\n",
      "Epoch 564/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 3.8410e-05 - mse: 3.8410e-05 - val_loss: 1.8896e-05 - val_mse: 1.8896e-05\n",
      "Epoch 565/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 2.7745e-05 - mse: 2.7745e-05 - val_loss: 1.2186e-05 - val_mse: 1.2186e-05\n",
      "Epoch 566/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 2.0076e-05 - mse: 2.0076e-05 - val_loss: 2.6127e-05 - val_mse: 2.6127e-05\n",
      "Epoch 567/1000\n",
      "66/66 [==============================] - 0s 468us/step - loss: 2.0093e-05 - mse: 2.0093e-05 - val_loss: 9.8175e-05 - val_mse: 9.8175e-05\n",
      "Epoch 568/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 4.5131e-05 - mse: 4.5131e-05 - val_loss: 3.7589e-05 - val_mse: 3.7589e-05\n",
      "Epoch 569/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.1770e-05 - mse: 3.1770e-05 - val_loss: 5.7315e-05 - val_mse: 5.7315e-05\n",
      "Epoch 570/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 3.2413e-05 - mse: 3.2413e-05 - val_loss: 3.4603e-05 - val_mse: 3.4603e-05\n",
      "Epoch 571/1000\n",
      "66/66 [==============================] - 0s 463us/step - loss: 3.3566e-05 - mse: 3.3566e-05 - val_loss: 3.8371e-05 - val_mse: 3.8371e-05\n",
      "Epoch 572/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 4.9432e-05 - mse: 4.9432e-05 - val_loss: 1.1844e-04 - val_mse: 1.1844e-04\n",
      "Epoch 573/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 2.3660e-05 - mse: 2.3660e-05 - val_loss: 2.2062e-05 - val_mse: 2.2062e-05\n",
      "Epoch 574/1000\n",
      "66/66 [==============================] - 0s 420us/step - loss: 4.3047e-05 - mse: 4.3047e-05 - val_loss: 3.2313e-05 - val_mse: 3.2313e-05\n",
      "Epoch 575/1000\n",
      "66/66 [==============================] - 0s 423us/step - loss: 3.8152e-05 - mse: 3.8152e-05 - val_loss: 1.3774e-05 - val_mse: 1.3774e-05\n",
      "Epoch 576/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.6190e-05 - mse: 2.6190e-05 - val_loss: 4.6691e-05 - val_mse: 4.6691e-05\n",
      "Epoch 577/1000\n",
      "66/66 [==============================] - 0s 380us/step - loss: 2.8780e-05 - mse: 2.8780e-05 - val_loss: 7.5052e-05 - val_mse: 7.5052e-05\n",
      "Epoch 578/1000\n",
      "66/66 [==============================] - 0s 380us/step - loss: 2.4595e-05 - mse: 2.4595e-05 - val_loss: 1.5138e-05 - val_mse: 1.5138e-05\n",
      "Epoch 579/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.4228e-05 - mse: 2.4228e-05 - val_loss: 6.8226e-05 - val_mse: 6.8226e-05\n",
      "Epoch 580/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.3440e-05 - mse: 2.3440e-05 - val_loss: 1.7983e-05 - val_mse: 1.7983e-05\n",
      "Epoch 581/1000\n",
      "66/66 [==============================] - 0s 395us/step - loss: 2.6135e-05 - mse: 2.6135e-05 - val_loss: 2.5912e-05 - val_mse: 2.5912e-05\n",
      "Epoch 582/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.4976e-05 - mse: 2.4976e-05 - val_loss: 4.0982e-05 - val_mse: 4.0982e-05\n",
      "Epoch 583/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.7306e-05 - mse: 2.7306e-05 - val_loss: 1.5595e-05 - val_mse: 1.5595e-05\n",
      "Epoch 584/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 3.3092e-05 - mse: 3.3092e-05 - val_loss: 3.5537e-05 - val_mse: 3.5537e-05\n",
      "Epoch 585/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 2.0578e-05 - mse: 2.0578e-05 - val_loss: 3.8398e-05 - val_mse: 3.8398e-05\n",
      "Epoch 586/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.5148e-05 - mse: 3.5148e-05 - val_loss: 2.9165e-05 - val_mse: 2.9165e-05\n",
      "Epoch 587/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.3786e-05 - mse: 3.3786e-05 - val_loss: 2.3009e-05 - val_mse: 2.3009e-05\n",
      "Epoch 588/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 3.1833e-05 - mse: 3.1833e-05 - val_loss: 7.6227e-05 - val_mse: 7.6227e-05\n",
      "Epoch 589/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 4.8118e-05 - mse: 4.8118e-05 - val_loss: 5.3649e-05 - val_mse: 5.3649e-05\n",
      "Epoch 590/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.3284e-05 - mse: 4.3284e-05 - val_loss: 2.6080e-05 - val_mse: 2.6080e-05\n",
      "Epoch 591/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 9.6355e-06 - mse: 9.6355e-06 - val_loss: 5.0020e-05 - val_mse: 5.0020e-05\n",
      "Epoch 592/1000\n",
      "66/66 [==============================] - 0s 413us/step - loss: 3.5190e-05 - mse: 3.5190e-05 - val_loss: 2.3892e-05 - val_mse: 2.3892e-05\n",
      "Epoch 593/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.8926e-05 - mse: 1.8926e-05 - val_loss: 9.8846e-06 - val_mse: 9.8846e-06\n",
      "Epoch 594/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.5437e-05 - mse: 2.5437e-05 - val_loss: 1.0759e-04 - val_mse: 1.0759e-04\n",
      "Epoch 595/1000\n",
      "66/66 [==============================] - 0s 395us/step - loss: 5.1592e-05 - mse: 5.1592e-05 - val_loss: 3.0166e-05 - val_mse: 3.0166e-05\n",
      "Epoch 596/1000\n",
      "66/66 [==============================] - 0s 438us/step - loss: 2.6693e-05 - mse: 2.6693e-05 - val_loss: 6.5835e-05 - val_mse: 6.5835e-05\n",
      "Epoch 597/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.2594e-05 - mse: 3.2594e-05 - val_loss: 2.3825e-05 - val_mse: 2.3825e-05\n",
      "Epoch 598/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 3.9995e-05 - mse: 3.9995e-05 - val_loss: 5.5041e-05 - val_mse: 5.5041e-05\n",
      "Epoch 599/1000\n",
      "66/66 [==============================] - 0s 388us/step - loss: 4.2252e-05 - mse: 4.2252e-05 - val_loss: 3.1966e-05 - val_mse: 3.1966e-05\n",
      "Epoch 600/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 3.1267e-05 - mse: 3.1267e-05 - val_loss: 1.1914e-05 - val_mse: 1.1914e-05\n",
      "Epoch 601/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 1.7860e-05 - mse: 1.7860e-05 - val_loss: 3.2390e-05 - val_mse: 3.2390e-05\n",
      "Epoch 602/1000\n",
      "66/66 [==============================] - 0s 426us/step - loss: 3.0888e-05 - mse: 3.0888e-05 - val_loss: 6.2081e-05 - val_mse: 6.2081e-05\n",
      "Epoch 603/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 4.2571e-05 - mse: 4.2571e-05 - val_loss: 3.2988e-05 - val_mse: 3.2988e-05\n",
      "Epoch 604/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.5837e-05 - mse: 2.5837e-05 - val_loss: 2.3561e-05 - val_mse: 2.3561e-05\n",
      "Epoch 605/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 2.7625e-05 - mse: 2.7625e-05 - val_loss: 9.0254e-05 - val_mse: 9.0254e-05\n",
      "Epoch 606/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 2.3613e-05 - mse: 2.3613e-05 - val_loss: 2.4407e-05 - val_mse: 2.4407e-05\n",
      "Epoch 607/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 1.5945e-05 - mse: 1.5945e-05 - val_loss: 1.2707e-05 - val_mse: 1.2707e-05\n",
      "Epoch 608/1000\n",
      "66/66 [==============================] - 0s 397us/step - loss: 2.8510e-05 - mse: 2.8510e-05 - val_loss: 2.2297e-05 - val_mse: 2.2297e-05\n",
      "Epoch 609/1000\n",
      "66/66 [==============================] - 0s 381us/step - loss: 3.4460e-05 - mse: 3.4460e-05 - val_loss: 2.6672e-05 - val_mse: 2.6672e-05\n",
      "Epoch 610/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 1.9145e-05 - mse: 1.9145e-05 - val_loss: 4.2306e-05 - val_mse: 4.2306e-05\n",
      "Epoch 611/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.5744e-05 - mse: 3.5744e-05 - val_loss: 2.1475e-05 - val_mse: 2.1475e-05\n",
      "Epoch 612/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.7382e-05 - mse: 1.7382e-05 - val_loss: 3.7821e-05 - val_mse: 3.7821e-05\n",
      "Epoch 613/1000\n",
      "66/66 [==============================] - 0s 398us/step - loss: 2.4930e-05 - mse: 2.4930e-05 - val_loss: 3.4493e-05 - val_mse: 3.4493e-05\n",
      "Epoch 614/1000\n",
      "66/66 [==============================] - 0s 393us/step - loss: 2.6018e-05 - mse: 2.6018e-05 - val_loss: 6.1840e-05 - val_mse: 6.1840e-05\n",
      "Epoch 615/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 3.7985e-05 - mse: 3.7985e-05 - val_loss: 2.5885e-05 - val_mse: 2.5885e-05\n",
      "Epoch 616/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.2932e-05 - mse: 3.2932e-05 - val_loss: 1.3240e-05 - val_mse: 1.3240e-05\n",
      "Epoch 617/1000\n",
      "66/66 [==============================] - 0s 380us/step - loss: 1.9217e-05 - mse: 1.9217e-05 - val_loss: 4.0792e-05 - val_mse: 4.0792e-05\n",
      "Epoch 618/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 5.2151e-05 - mse: 5.2151e-05 - val_loss: 3.2301e-05 - val_mse: 3.2301e-05\n",
      "Epoch 619/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.1785e-05 - mse: 2.1785e-05 - val_loss: 3.6378e-05 - val_mse: 3.6378e-05\n",
      "Epoch 620/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 1.4876e-05 - mse: 1.4876e-05 - val_loss: 1.9152e-05 - val_mse: 1.9152e-05\n",
      "Epoch 621/1000\n",
      "66/66 [==============================] - 0s 403us/step - loss: 3.6992e-05 - mse: 3.6992e-05 - val_loss: 4.0195e-05 - val_mse: 4.0195e-05\n",
      "Epoch 622/1000\n",
      "66/66 [==============================] - 0s 393us/step - loss: 3.8251e-05 - mse: 3.8251e-05 - val_loss: 1.5385e-04 - val_mse: 1.5385e-04\n",
      "Epoch 623/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.4757e-05 - mse: 3.4757e-05 - val_loss: 2.7542e-05 - val_mse: 2.7542e-05\n",
      "Epoch 624/1000\n",
      "66/66 [==============================] - 0s 375us/step - loss: 1.1824e-05 - mse: 1.1824e-05 - val_loss: 2.2528e-05 - val_mse: 2.2528e-05\n",
      "Epoch 625/1000\n",
      "66/66 [==============================] - 0s 393us/step - loss: 3.0752e-05 - mse: 3.0752e-05 - val_loss: 4.8044e-05 - val_mse: 4.8044e-05\n",
      "Epoch 626/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.1974e-05 - mse: 2.1974e-05 - val_loss: 3.1598e-05 - val_mse: 3.1598e-05\n",
      "Epoch 627/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.9909e-05 - mse: 4.9909e-05 - val_loss: 7.2352e-05 - val_mse: 7.2352e-05\n",
      "Epoch 628/1000\n",
      "66/66 [==============================] - 0s 397us/step - loss: 3.8747e-05 - mse: 3.8747e-05 - val_loss: 5.0672e-05 - val_mse: 5.0672e-05\n",
      "Epoch 629/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 2.2447e-05 - mse: 2.2447e-05 - val_loss: 8.1296e-06 - val_mse: 8.1296e-06\n",
      "Epoch 630/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.2130e-05 - mse: 2.2130e-05 - val_loss: 3.6675e-05 - val_mse: 3.6675e-05\n",
      "Epoch 631/1000\n",
      "66/66 [==============================] - 0s 398us/step - loss: 2.7419e-05 - mse: 2.7419e-05 - val_loss: 8.6394e-06 - val_mse: 8.6394e-06\n",
      "Epoch 632/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.4789e-05 - mse: 1.4789e-05 - val_loss: 7.3137e-05 - val_mse: 7.3137e-05\n",
      "Epoch 633/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 3.5731e-05 - mse: 3.5731e-05 - val_loss: 1.6506e-05 - val_mse: 1.6506e-05\n",
      "Epoch 634/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.1227e-05 - mse: 2.1227e-05 - val_loss: 1.3488e-05 - val_mse: 1.3488e-05\n",
      "Epoch 635/1000\n",
      "66/66 [==============================] - 0s 443us/step - loss: 2.3210e-05 - mse: 2.3210e-05 - val_loss: 2.2205e-05 - val_mse: 2.2205e-05\n",
      "Epoch 636/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 5.3536e-05 - mse: 5.3536e-05 - val_loss: 6.4463e-05 - val_mse: 6.4463e-05\n",
      "Epoch 637/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 2.1931e-05 - mse: 2.1931e-05 - val_loss: 2.0747e-05 - val_mse: 2.0747e-05\n",
      "Epoch 638/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 2.9419e-05 - mse: 2.9419e-05 - val_loss: 2.4891e-05 - val_mse: 2.4891e-05\n",
      "Epoch 639/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 3.0522e-05 - mse: 3.0522e-05 - val_loss: 3.4747e-05 - val_mse: 3.4747e-05\n",
      "Epoch 640/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 3.1231e-05 - mse: 3.1231e-05 - val_loss: 2.9437e-05 - val_mse: 2.9437e-05\n",
      "Epoch 641/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 3.5116e-05 - mse: 3.5116e-05 - val_loss: 4.6217e-05 - val_mse: 4.6217e-05\n",
      "Epoch 642/1000\n",
      "66/66 [==============================] - 0s 387us/step - loss: 2.9113e-05 - mse: 2.9113e-05 - val_loss: 7.8645e-05 - val_mse: 7.8645e-05\n",
      "Epoch 643/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.4366e-05 - mse: 4.4366e-05 - val_loss: 2.0722e-05 - val_mse: 2.0722e-05\n",
      "Epoch 644/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 2.9674e-05 - mse: 2.9674e-05 - val_loss: 9.0369e-06 - val_mse: 9.0369e-06\n",
      "Epoch 645/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.6753e-05 - mse: 1.6753e-05 - val_loss: 3.5066e-05 - val_mse: 3.5066e-05\n",
      "Epoch 646/1000\n",
      "66/66 [==============================] - 0s 388us/step - loss: 2.7573e-05 - mse: 2.7573e-05 - val_loss: 1.6703e-05 - val_mse: 1.6703e-05\n",
      "Epoch 647/1000\n",
      "66/66 [==============================] - 0s 393us/step - loss: 2.5732e-05 - mse: 2.5732e-05 - val_loss: 2.4406e-05 - val_mse: 2.4406e-05\n",
      "Epoch 648/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 3.3798e-05 - mse: 3.3798e-05 - val_loss: 6.6005e-05 - val_mse: 6.6005e-05\n",
      "Epoch 649/1000\n",
      "66/66 [==============================] - 0s 387us/step - loss: 2.6654e-05 - mse: 2.6654e-05 - val_loss: 1.6381e-05 - val_mse: 1.6381e-05\n",
      "Epoch 650/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.8682e-05 - mse: 1.8682e-05 - val_loss: 1.0285e-04 - val_mse: 1.0285e-04\n",
      "Epoch 651/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 5.7938e-05 - mse: 5.7938e-05 - val_loss: 2.1171e-05 - val_mse: 2.1171e-05\n",
      "Epoch 652/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 1.8047e-05 - mse: 1.8047e-05 - val_loss: 1.7224e-05 - val_mse: 1.7224e-05\n",
      "Epoch 653/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.8753e-05 - mse: 1.8753e-05 - val_loss: 1.9876e-05 - val_mse: 1.9876e-05\n",
      "Epoch 654/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.2316e-05 - mse: 2.2316e-05 - val_loss: 1.1911e-04 - val_mse: 1.1911e-04\n",
      "Epoch 655/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.0815e-05 - mse: 4.0815e-05 - val_loss: 7.7685e-05 - val_mse: 7.7685e-05\n",
      "Epoch 656/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.0145e-05 - mse: 4.0145e-05 - val_loss: 1.1335e-05 - val_mse: 1.1335e-05\n",
      "Epoch 657/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.4458e-05 - mse: 3.4458e-05 - val_loss: 1.4645e-04 - val_mse: 1.4645e-04\n",
      "Epoch 658/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 4.3857e-05 - mse: 4.3857e-05 - val_loss: 2.1919e-05 - val_mse: 2.1919e-05\n",
      "Epoch 659/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.2634e-05 - mse: 2.2634e-05 - val_loss: 6.8855e-05 - val_mse: 6.8855e-05\n",
      "Epoch 660/1000\n",
      "66/66 [==============================] - 0s 465us/step - loss: 3.1101e-05 - mse: 3.1101e-05 - val_loss: 2.8444e-05 - val_mse: 2.8444e-05\n",
      "Epoch 661/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.8812e-05 - mse: 2.8812e-05 - val_loss: 3.1646e-05 - val_mse: 3.1646e-05\n",
      "Epoch 662/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.3298e-05 - mse: 2.3298e-05 - val_loss: 3.3708e-05 - val_mse: 3.3708e-05\n",
      "Epoch 663/1000\n",
      "66/66 [==============================] - 0s 372us/step - loss: 2.6859e-05 - mse: 2.6859e-05 - val_loss: 1.0034e-05 - val_mse: 1.0034e-05\n",
      "Epoch 664/1000\n",
      "66/66 [==============================] - 0s 380us/step - loss: 1.9935e-05 - mse: 1.9935e-05 - val_loss: 2.3251e-05 - val_mse: 2.3251e-05\n",
      "Epoch 665/1000\n",
      "66/66 [==============================] - 0s 408us/step - loss: 4.1355e-05 - mse: 4.1355e-05 - val_loss: 8.8137e-06 - val_mse: 8.8137e-06\n",
      "Epoch 666/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.9004e-05 - mse: 1.9004e-05 - val_loss: 2.5258e-05 - val_mse: 2.5258e-05\n",
      "Epoch 667/1000\n",
      "66/66 [==============================] - 0s 449us/step - loss: 3.3216e-05 - mse: 3.3216e-05 - val_loss: 7.1165e-05 - val_mse: 7.1165e-05\n",
      "Epoch 668/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.7163e-05 - mse: 4.7163e-05 - val_loss: 4.9396e-05 - val_mse: 4.9396e-05\n",
      "Epoch 669/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.1826e-05 - mse: 1.1826e-05 - val_loss: 1.4121e-05 - val_mse: 1.4121e-05\n",
      "Epoch 670/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.6763e-05 - mse: 2.6763e-05 - val_loss: 1.0646e-05 - val_mse: 1.0646e-05\n",
      "Epoch 671/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 2.2331e-05 - mse: 2.2331e-05 - val_loss: 2.3235e-05 - val_mse: 2.3235e-05\n",
      "Epoch 672/1000\n",
      "66/66 [==============================] - 0s 408us/step - loss: 1.3143e-05 - mse: 1.3143e-05 - val_loss: 4.7568e-05 - val_mse: 4.7568e-05\n",
      "Epoch 673/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.2718e-05 - mse: 1.2718e-05 - val_loss: 3.5588e-05 - val_mse: 3.5588e-05\n",
      "Epoch 674/1000\n",
      "66/66 [==============================] - 0s 411us/step - loss: 4.1861e-05 - mse: 4.1861e-05 - val_loss: 2.1190e-05 - val_mse: 2.1190e-05\n",
      "Epoch 675/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.5774e-05 - mse: 1.5774e-05 - val_loss: 1.1152e-05 - val_mse: 1.1152e-05\n",
      "Epoch 676/1000\n",
      "66/66 [==============================] - 0s 412us/step - loss: 1.1142e-05 - mse: 1.1142e-05 - val_loss: 2.4778e-05 - val_mse: 2.4778e-05\n",
      "Epoch 677/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.0541e-05 - mse: 3.0541e-05 - val_loss: 3.0078e-05 - val_mse: 3.0078e-05\n",
      "Epoch 678/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.3388e-05 - mse: 1.3388e-05 - val_loss: 2.4983e-05 - val_mse: 2.4983e-05\n",
      "Epoch 679/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.1420e-05 - mse: 2.1420e-05 - val_loss: 2.7673e-05 - val_mse: 2.7673e-05\n",
      "Epoch 680/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 3.7948e-05 - mse: 3.7948e-05 - val_loss: 5.4512e-05 - val_mse: 5.4512e-05\n",
      "Epoch 681/1000\n",
      "66/66 [==============================] - 0s 412us/step - loss: 1.8736e-05 - mse: 1.8736e-05 - val_loss: 2.2628e-05 - val_mse: 2.2628e-05\n",
      "Epoch 682/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.8469e-05 - mse: 1.8469e-05 - val_loss: 3.4152e-05 - val_mse: 3.4152e-05\n",
      "Epoch 683/1000\n",
      "66/66 [==============================] - 0s 423us/step - loss: 2.9095e-05 - mse: 2.9095e-05 - val_loss: 2.4161e-05 - val_mse: 2.4161e-05\n",
      "Epoch 684/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.1482e-05 - mse: 2.1482e-05 - val_loss: 3.7523e-05 - val_mse: 3.7523e-05\n",
      "Epoch 685/1000\n",
      "66/66 [==============================] - 0s 475us/step - loss: 3.9567e-05 - mse: 3.9567e-05 - val_loss: 2.3566e-05 - val_mse: 2.3566e-05\n",
      "Epoch 686/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.6196e-05 - mse: 1.6196e-05 - val_loss: 1.7296e-05 - val_mse: 1.7296e-05\n",
      "Epoch 687/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.9296e-05 - mse: 2.9296e-05 - val_loss: 1.2808e-05 - val_mse: 1.2808e-05\n",
      "Epoch 688/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 1.7156e-05 - mse: 1.7156e-05 - val_loss: 1.0366e-04 - val_mse: 1.0366e-04\n",
      "Epoch 689/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 5.1189e-05 - mse: 5.1189e-05 - val_loss: 2.7124e-05 - val_mse: 2.7124e-05\n",
      "Epoch 690/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 2.2266e-05 - mse: 2.2266e-05 - val_loss: 4.4610e-05 - val_mse: 4.4610e-05\n",
      "Epoch 691/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 1.5042e-05 - mse: 1.5042e-05 - val_loss: 1.9413e-05 - val_mse: 1.9413e-05\n",
      "Epoch 692/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 6.2236e-05 - mse: 6.2236e-05 - val_loss: 8.6871e-05 - val_mse: 8.6871e-05\n",
      "Epoch 693/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.3586e-05 - mse: 3.3586e-05 - val_loss: 1.1934e-05 - val_mse: 1.1934e-05\n",
      "Epoch 694/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.2044e-05 - mse: 1.2044e-05 - val_loss: 4.6015e-05 - val_mse: 4.6015e-05\n",
      "Epoch 695/1000\n",
      "66/66 [==============================] - 0s 386us/step - loss: 3.5224e-05 - mse: 3.5224e-05 - val_loss: 9.4798e-06 - val_mse: 9.4798e-06\n",
      "Epoch 696/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 1.9850e-05 - mse: 1.9850e-05 - val_loss: 2.6485e-05 - val_mse: 2.6485e-05\n",
      "Epoch 697/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 1.7497e-05 - mse: 1.7497e-05 - val_loss: 5.1750e-05 - val_mse: 5.1750e-05\n",
      "Epoch 698/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.7438e-05 - mse: 1.7438e-05 - val_loss: 2.4645e-05 - val_mse: 2.4645e-05\n",
      "Epoch 699/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.2790e-05 - mse: 2.2790e-05 - val_loss: 2.3572e-05 - val_mse: 2.3572e-05\n",
      "Epoch 700/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.9961e-05 - mse: 1.9961e-05 - val_loss: 2.1562e-05 - val_mse: 2.1562e-05\n",
      "Epoch 701/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.7500e-05 - mse: 3.7500e-05 - val_loss: 6.0851e-05 - val_mse: 6.0851e-05\n",
      "Epoch 702/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 3.1150e-05 - mse: 3.1150e-05 - val_loss: 5.4199e-05 - val_mse: 5.4199e-05\n",
      "Epoch 703/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.8528e-05 - mse: 1.8528e-05 - val_loss: 1.6183e-05 - val_mse: 1.6183e-05\n",
      "Epoch 704/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.5912e-05 - mse: 1.5912e-05 - val_loss: 1.2984e-05 - val_mse: 1.2984e-05\n",
      "Epoch 705/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 3.9584e-05 - mse: 3.9584e-05 - val_loss: 2.6998e-05 - val_mse: 2.6998e-05\n",
      "Epoch 706/1000\n",
      "66/66 [==============================] - 0s 398us/step - loss: 2.1645e-05 - mse: 2.1645e-05 - val_loss: 3.3086e-05 - val_mse: 3.3086e-05\n",
      "Epoch 707/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.1056e-05 - mse: 4.1056e-05 - val_loss: 1.9329e-05 - val_mse: 1.9329e-05\n",
      "Epoch 708/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.7838e-05 - mse: 1.7838e-05 - val_loss: 6.7231e-05 - val_mse: 6.7231e-05\n",
      "Epoch 709/1000\n",
      "66/66 [==============================] - 0s 457us/step - loss: 2.8647e-05 - mse: 2.8647e-05 - val_loss: 1.5288e-05 - val_mse: 1.5288e-05\n",
      "Epoch 710/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 5.9107e-05 - mse: 5.9107e-05 - val_loss: 5.2162e-05 - val_mse: 5.2162e-05\n",
      "Epoch 711/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.3301e-05 - mse: 2.3301e-05 - val_loss: 8.0038e-06 - val_mse: 8.0038e-06\n",
      "Epoch 712/1000\n",
      "66/66 [==============================] - 0s 402us/step - loss: 3.4730e-05 - mse: 3.4730e-05 - val_loss: 8.1789e-05 - val_mse: 8.1789e-05\n",
      "Epoch 713/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.6379e-05 - mse: 2.6379e-05 - val_loss: 1.1443e-05 - val_mse: 1.1443e-05\n",
      "Epoch 714/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 2.8033e-05 - mse: 2.8033e-05 - val_loss: 5.5179e-05 - val_mse: 5.5179e-05\n",
      "Epoch 715/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.4992e-05 - mse: 1.4992e-05 - val_loss: 2.7265e-05 - val_mse: 2.7265e-05\n",
      "Epoch 716/1000\n",
      "66/66 [==============================] - 0s 372us/step - loss: 2.7213e-05 - mse: 2.7213e-05 - val_loss: 5.7490e-05 - val_mse: 5.7490e-05\n",
      "Epoch 717/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.5535e-05 - mse: 1.5535e-05 - val_loss: 5.5185e-05 - val_mse: 5.5185e-05\n",
      "Epoch 718/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 4.8688e-05 - mse: 4.8688e-05 - val_loss: 4.1240e-05 - val_mse: 4.1240e-05\n",
      "Epoch 719/1000\n",
      "66/66 [==============================] - 0s 379us/step - loss: 1.3618e-05 - mse: 1.3618e-05 - val_loss: 1.2825e-05 - val_mse: 1.2825e-05\n",
      "Epoch 720/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 1.4000e-05 - mse: 1.4000e-05 - val_loss: 5.5878e-05 - val_mse: 5.5878e-05\n",
      "Epoch 721/1000\n",
      "66/66 [==============================] - 0s 395us/step - loss: 3.0955e-05 - mse: 3.0955e-05 - val_loss: 6.4406e-05 - val_mse: 6.4406e-05\n",
      "Epoch 722/1000\n",
      "66/66 [==============================] - 0s 394us/step - loss: 1.5344e-05 - mse: 1.5344e-05 - val_loss: 1.7748e-05 - val_mse: 1.7748e-05\n",
      "Epoch 723/1000\n",
      "66/66 [==============================] - 0s 405us/step - loss: 2.3621e-05 - mse: 2.3621e-05 - val_loss: 2.5506e-05 - val_mse: 2.5506e-05\n",
      "Epoch 724/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.6676e-05 - mse: 1.6676e-05 - val_loss: 2.7856e-05 - val_mse: 2.7856e-05\n",
      "Epoch 725/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 2.6232e-05 - mse: 2.6232e-05 - val_loss: 5.9624e-05 - val_mse: 5.9624e-05\n",
      "Epoch 726/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.8991e-05 - mse: 3.8991e-05 - val_loss: 2.9158e-05 - val_mse: 2.9158e-05\n",
      "Epoch 727/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 1.8721e-05 - mse: 1.8721e-05 - val_loss: 1.0281e-04 - val_mse: 1.0281e-04\n",
      "Epoch 728/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.7059e-05 - mse: 3.7059e-05 - val_loss: 2.0210e-05 - val_mse: 2.0210e-05\n",
      "Epoch 729/1000\n",
      "66/66 [==============================] - 0s 411us/step - loss: 1.2978e-05 - mse: 1.2978e-05 - val_loss: 2.5387e-05 - val_mse: 2.5387e-05\n",
      "Epoch 730/1000\n",
      "66/66 [==============================] - 0s 456us/step - loss: 2.1224e-05 - mse: 2.1224e-05 - val_loss: 2.2111e-05 - val_mse: 2.2111e-05\n",
      "Epoch 731/1000\n",
      "66/66 [==============================] - 0s 486us/step - loss: 2.7018e-05 - mse: 2.7018e-05 - val_loss: 6.4946e-05 - val_mse: 6.4946e-05\n",
      "Epoch 732/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.3527e-05 - mse: 3.3527e-05 - val_loss: 1.1905e-04 - val_mse: 1.1905e-04\n",
      "Epoch 733/1000\n",
      "66/66 [==============================] - 0s 475us/step - loss: 2.4173e-05 - mse: 2.4173e-05 - val_loss: 3.5965e-05 - val_mse: 3.5965e-05\n",
      "Epoch 734/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 3.5136e-05 - mse: 3.5136e-05 - val_loss: 2.0962e-05 - val_mse: 2.0962e-05\n",
      "Epoch 735/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.9899e-05 - mse: 1.9899e-05 - val_loss: 1.3428e-04 - val_mse: 1.3428e-04\n",
      "Epoch 736/1000\n",
      "66/66 [==============================] - 0s 433us/step - loss: 2.8118e-05 - mse: 2.8118e-05 - val_loss: 1.6403e-05 - val_mse: 1.6403e-05\n",
      "Epoch 737/1000\n",
      "66/66 [==============================] - 0s 408us/step - loss: 1.9261e-05 - mse: 1.9261e-05 - val_loss: 1.6476e-05 - val_mse: 1.6476e-05\n",
      "Epoch 738/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.3375e-05 - mse: 1.3375e-05 - val_loss: 5.4753e-05 - val_mse: 5.4753e-05\n",
      "Epoch 739/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.1063e-05 - mse: 3.1063e-05 - val_loss: 2.9433e-05 - val_mse: 2.9433e-05\n",
      "Epoch 740/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 1.4521e-05 - mse: 1.4521e-05 - val_loss: 1.7962e-05 - val_mse: 1.7962e-05\n",
      "Epoch 741/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.3484e-05 - mse: 2.3484e-05 - val_loss: 4.6624e-05 - val_mse: 4.6624e-05\n",
      "Epoch 742/1000\n",
      "66/66 [==============================] - 0s 438us/step - loss: 2.2818e-05 - mse: 2.2818e-05 - val_loss: 5.5342e-05 - val_mse: 5.5342e-05\n",
      "Epoch 743/1000\n",
      "66/66 [==============================] - 0s 441us/step - loss: 3.0449e-05 - mse: 3.0449e-05 - val_loss: 2.0320e-05 - val_mse: 2.0320e-05\n",
      "Epoch 744/1000\n",
      "66/66 [==============================] - 0s 484us/step - loss: 4.0353e-05 - mse: 4.0353e-05 - val_loss: 3.8748e-05 - val_mse: 3.8748e-05\n",
      "Epoch 745/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.9157e-05 - mse: 3.9157e-05 - val_loss: 5.0225e-05 - val_mse: 5.0225e-05\n",
      "Epoch 746/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.7569e-05 - mse: 3.7569e-05 - val_loss: 2.6205e-05 - val_mse: 2.6205e-05\n",
      "Epoch 747/1000\n",
      "66/66 [==============================] - 0s 545us/step - loss: 7.4127e-06 - mse: 7.4127e-06 - val_loss: 7.7990e-06 - val_mse: 7.7990e-06\n",
      "Epoch 748/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.1134e-05 - mse: 2.1134e-05 - val_loss: 1.6633e-05 - val_mse: 1.6633e-05\n",
      "Epoch 749/1000\n",
      "66/66 [==============================] - 0s 448us/step - loss: 2.5634e-05 - mse: 2.5634e-05 - val_loss: 2.1176e-05 - val_mse: 2.1176e-05\n",
      "Epoch 750/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 1.5009e-05 - mse: 1.5009e-05 - val_loss: 9.6228e-05 - val_mse: 9.6228e-05\n",
      "Epoch 751/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 4.1547e-05 - mse: 4.1547e-05 - val_loss: 1.4030e-04 - val_mse: 1.4030e-04\n",
      "Epoch 752/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 3.0722e-05 - mse: 3.0722e-05 - val_loss: 1.0403e-05 - val_mse: 1.0403e-05\n",
      "Epoch 753/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 1.1940e-05 - mse: 1.1940e-05 - val_loss: 7.8774e-05 - val_mse: 7.8774e-05\n",
      "Epoch 754/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 4.9240e-05 - mse: 4.9240e-05 - val_loss: 3.5919e-05 - val_mse: 3.5919e-05\n",
      "Epoch 755/1000\n",
      "66/66 [==============================] - 0s 456us/step - loss: 1.4345e-05 - mse: 1.4345e-05 - val_loss: 3.1782e-05 - val_mse: 3.1782e-05\n",
      "Epoch 756/1000\n",
      "66/66 [==============================] - 0s 456us/step - loss: 2.1633e-05 - mse: 2.1633e-05 - val_loss: 3.7428e-05 - val_mse: 3.7428e-05\n",
      "Epoch 757/1000\n",
      "66/66 [==============================] - 0s 438us/step - loss: 2.8272e-05 - mse: 2.8272e-05 - val_loss: 5.7202e-05 - val_mse: 5.7202e-05\n",
      "Epoch 758/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 2.1717e-05 - mse: 2.1717e-05 - val_loss: 1.6257e-05 - val_mse: 1.6257e-05\n",
      "Epoch 759/1000\n",
      "66/66 [==============================] - 0s 516us/step - loss: 1.9725e-05 - mse: 1.9725e-05 - val_loss: 1.0835e-05 - val_mse: 1.0835e-05\n",
      "Epoch 760/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 1.2169e-05 - mse: 1.2169e-05 - val_loss: 1.3031e-05 - val_mse: 1.3031e-05\n",
      "Epoch 761/1000\n",
      "66/66 [==============================] - 0s 464us/step - loss: 2.0451e-05 - mse: 2.0451e-05 - val_loss: 2.5416e-05 - val_mse: 2.5416e-05\n",
      "Epoch 762/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.2123e-05 - mse: 2.2123e-05 - val_loss: 3.1861e-05 - val_mse: 3.1861e-05\n",
      "Epoch 763/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.7446e-05 - mse: 1.7446e-05 - val_loss: 1.3771e-05 - val_mse: 1.3771e-05\n",
      "Epoch 764/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 2.5982e-05 - mse: 2.5982e-05 - val_loss: 3.2212e-05 - val_mse: 3.2212e-05\n",
      "Epoch 765/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 2.7872e-05 - mse: 2.7872e-05 - val_loss: 1.8847e-05 - val_mse: 1.8847e-05\n",
      "Epoch 766/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.3396e-05 - mse: 2.3396e-05 - val_loss: 6.0241e-05 - val_mse: 6.0241e-05\n",
      "Epoch 767/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 2.8640e-05 - mse: 2.8640e-05 - val_loss: 5.7626e-05 - val_mse: 5.7626e-05\n",
      "Epoch 768/1000\n",
      "66/66 [==============================] - 0s 441us/step - loss: 2.1471e-05 - mse: 2.1471e-05 - val_loss: 1.6349e-05 - val_mse: 1.6349e-05\n",
      "Epoch 769/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.1752e-05 - mse: 1.1752e-05 - val_loss: 2.3967e-05 - val_mse: 2.3967e-05\n",
      "Epoch 770/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 4.4684e-05 - mse: 4.4684e-05 - val_loss: 4.5773e-05 - val_mse: 4.5773e-05\n",
      "Epoch 771/1000\n",
      "66/66 [==============================] - 0s 395us/step - loss: 2.6113e-05 - mse: 2.6113e-05 - val_loss: 1.6912e-05 - val_mse: 1.6912e-05\n",
      "Epoch 772/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.1861e-05 - mse: 3.1861e-05 - val_loss: 4.6678e-05 - val_mse: 4.6678e-05\n",
      "Epoch 773/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 3.3059e-05 - mse: 3.3059e-05 - val_loss: 3.3201e-05 - val_mse: 3.3201e-05\n",
      "Epoch 774/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 1.7977e-05 - mse: 1.7977e-05 - val_loss: 4.6469e-05 - val_mse: 4.6469e-05\n",
      "Epoch 775/1000\n",
      "66/66 [==============================] - 0s 410us/step - loss: 2.0375e-05 - mse: 2.0375e-05 - val_loss: 7.9100e-05 - val_mse: 7.9100e-05\n",
      "Epoch 776/1000\n",
      "66/66 [==============================] - 0s 425us/step - loss: 6.3838e-05 - mse: 6.3838e-05 - val_loss: 1.9313e-05 - val_mse: 1.9313e-05\n",
      "Epoch 777/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 1.3844e-05 - mse: 1.3844e-05 - val_loss: 2.2198e-05 - val_mse: 2.2198e-05\n",
      "Epoch 778/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.1046e-05 - mse: 2.1046e-05 - val_loss: 2.2992e-05 - val_mse: 2.2992e-05\n",
      "Epoch 779/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 2.3747e-05 - mse: 2.3747e-05 - val_loss: 1.7743e-05 - val_mse: 1.7743e-05\n",
      "Epoch 780/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.2920e-05 - mse: 1.2920e-05 - val_loss: 1.7166e-05 - val_mse: 1.7166e-05\n",
      "Epoch 781/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 2.1582e-05 - mse: 2.1582e-05 - val_loss: 5.4408e-05 - val_mse: 5.4408e-05\n",
      "Epoch 782/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 3.4687e-05 - mse: 3.4687e-05 - val_loss: 1.7335e-05 - val_mse: 1.7335e-05\n",
      "Epoch 783/1000\n",
      "66/66 [==============================] - 0s 453us/step - loss: 1.9158e-05 - mse: 1.9158e-05 - val_loss: 4.7303e-05 - val_mse: 4.7303e-05\n",
      "Epoch 784/1000\n",
      "66/66 [==============================] - 0s 449us/step - loss: 2.7517e-05 - mse: 2.7517e-05 - val_loss: 2.3392e-05 - val_mse: 2.3392e-05\n",
      "Epoch 785/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.4004e-05 - mse: 2.4004e-05 - val_loss: 2.6512e-05 - val_mse: 2.6512e-05\n",
      "Epoch 786/1000\n",
      "66/66 [==============================] - 0s 456us/step - loss: 2.6713e-05 - mse: 2.6713e-05 - val_loss: 2.9192e-05 - val_mse: 2.9192e-05\n",
      "Epoch 787/1000\n",
      "66/66 [==============================] - 0s 510us/step - loss: 3.4723e-05 - mse: 3.4723e-05 - val_loss: 6.0572e-05 - val_mse: 6.0572e-05\n",
      "Epoch 788/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.1522e-05 - mse: 3.1522e-05 - val_loss: 3.7308e-05 - val_mse: 3.7308e-05\n",
      "Epoch 789/1000\n",
      "66/66 [==============================] - 0s 441us/step - loss: 2.5684e-05 - mse: 2.5684e-05 - val_loss: 4.2867e-05 - val_mse: 4.2867e-05\n",
      "Epoch 790/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 2.5719e-05 - mse: 2.5719e-05 - val_loss: 4.1990e-05 - val_mse: 4.1990e-05\n",
      "Epoch 791/1000\n",
      "66/66 [==============================] - 0s 409us/step - loss: 1.6294e-05 - mse: 1.6294e-05 - val_loss: 2.5562e-05 - val_mse: 2.5562e-05\n",
      "Epoch 792/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.3027e-05 - mse: 2.3027e-05 - val_loss: 3.4167e-05 - val_mse: 3.4167e-05\n",
      "Epoch 793/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 1.7333e-05 - mse: 1.7333e-05 - val_loss: 1.2234e-05 - val_mse: 1.2234e-05\n",
      "Epoch 794/1000\n",
      "66/66 [==============================] - 0s 427us/step - loss: 2.4385e-05 - mse: 2.4385e-05 - val_loss: 8.1906e-05 - val_mse: 8.1906e-05\n",
      "Epoch 795/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.1053e-05 - mse: 3.1053e-05 - val_loss: 4.1110e-05 - val_mse: 4.1110e-05\n",
      "Epoch 796/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 2.3384e-05 - mse: 2.3384e-05 - val_loss: 2.4446e-05 - val_mse: 2.4446e-05\n",
      "Epoch 797/1000\n",
      "66/66 [==============================] - 0s 432us/step - loss: 1.9617e-05 - mse: 1.9617e-05 - val_loss: 9.3300e-05 - val_mse: 9.3300e-05\n",
      "Epoch 798/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.5535e-05 - mse: 2.5535e-05 - val_loss: 1.4461e-05 - val_mse: 1.4461e-05\n",
      "Epoch 799/1000\n",
      "66/66 [==============================] - 0s 531us/step - loss: 1.9186e-05 - mse: 1.9186e-05 - val_loss: 4.5169e-05 - val_mse: 4.5169e-05\n",
      "Epoch 800/1000\n",
      "66/66 [==============================] - 0s 477us/step - loss: 3.4636e-05 - mse: 3.4636e-05 - val_loss: 6.5616e-05 - val_mse: 6.5616e-05\n",
      "Epoch 801/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 2.4757e-05 - mse: 2.4757e-05 - val_loss: 3.6226e-05 - val_mse: 3.6226e-05\n",
      "Epoch 802/1000\n",
      "66/66 [==============================] - 0s 427us/step - loss: 1.6197e-05 - mse: 1.6197e-05 - val_loss: 1.4751e-05 - val_mse: 1.4751e-05\n",
      "Epoch 803/1000\n",
      "66/66 [==============================] - 0s 434us/step - loss: 1.3105e-05 - mse: 1.3105e-05 - val_loss: 1.4512e-05 - val_mse: 1.4512e-05\n",
      "Epoch 804/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 1.4876e-05 - mse: 1.4876e-05 - val_loss: 5.5342e-05 - val_mse: 5.5342e-05\n",
      "Epoch 805/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 2.6397e-05 - mse: 2.6397e-05 - val_loss: 4.8644e-05 - val_mse: 4.8644e-05\n",
      "Epoch 806/1000\n",
      "66/66 [==============================] - 0s 462us/step - loss: 4.2269e-05 - mse: 4.2269e-05 - val_loss: 1.4257e-05 - val_mse: 1.4257e-05\n",
      "Epoch 807/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 1.1348e-05 - mse: 1.1348e-05 - val_loss: 2.3884e-05 - val_mse: 2.3884e-05\n",
      "Epoch 808/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 2.1458e-05 - mse: 2.1458e-05 - val_loss: 3.1341e-05 - val_mse: 3.1341e-05\n",
      "Epoch 809/1000\n",
      "66/66 [==============================] - 0s 488us/step - loss: 3.5487e-05 - mse: 3.5487e-05 - val_loss: 5.0108e-05 - val_mse: 5.0108e-05\n",
      "Epoch 810/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.0481e-05 - mse: 3.0481e-05 - val_loss: 2.2795e-05 - val_mse: 2.2795e-05\n",
      "Epoch 811/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 2.1305e-05 - mse: 2.1305e-05 - val_loss: 4.3935e-05 - val_mse: 4.3935e-05\n",
      "Epoch 812/1000\n",
      "66/66 [==============================] - 0s 442us/step - loss: 1.3688e-05 - mse: 1.3688e-05 - val_loss: 1.3301e-05 - val_mse: 1.3301e-05\n",
      "Epoch 813/1000\n",
      "66/66 [==============================] - 0s 519us/step - loss: 3.3525e-05 - mse: 3.3525e-05 - val_loss: 2.7945e-05 - val_mse: 2.7945e-05\n",
      "Epoch 814/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.1184e-05 - mse: 3.1184e-05 - val_loss: 3.2278e-05 - val_mse: 3.2278e-05\n",
      "Epoch 815/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 1.1028e-05 - mse: 1.1028e-05 - val_loss: 1.0772e-05 - val_mse: 1.0772e-05\n",
      "Epoch 816/1000\n",
      "66/66 [==============================] - 0s 417us/step - loss: 2.2619e-05 - mse: 2.2619e-05 - val_loss: 1.7879e-05 - val_mse: 1.7879e-05\n",
      "Epoch 817/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 2.9932e-05 - mse: 2.9932e-05 - val_loss: 3.2138e-05 - val_mse: 3.2138e-05\n",
      "Epoch 818/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.6002e-05 - mse: 3.6002e-05 - val_loss: 7.2700e-05 - val_mse: 7.2700e-05\n",
      "Epoch 819/1000\n",
      "66/66 [==============================] - 0s 418us/step - loss: 2.8129e-05 - mse: 2.8129e-05 - val_loss: 2.0924e-05 - val_mse: 2.0924e-05\n",
      "Epoch 820/1000\n",
      "66/66 [==============================] - 0s 454us/step - loss: 1.2530e-05 - mse: 1.2530e-05 - val_loss: 1.8057e-05 - val_mse: 1.8057e-05\n",
      "Epoch 821/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 3.0046e-05 - mse: 3.0046e-05 - val_loss: 8.4323e-05 - val_mse: 8.4323e-05\n",
      "Epoch 822/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 3.8060e-05 - mse: 3.8060e-05 - val_loss: 2.4797e-05 - val_mse: 2.4797e-05\n",
      "Epoch 823/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 2.1146e-05 - mse: 2.1146e-05 - val_loss: 2.0208e-05 - val_mse: 2.0208e-05\n",
      "Epoch 824/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 1.4052e-05 - mse: 1.4052e-05 - val_loss: 1.1387e-05 - val_mse: 1.1387e-05\n",
      "Epoch 825/1000\n",
      "66/66 [==============================] - 0s 433us/step - loss: 2.1777e-05 - mse: 2.1777e-05 - val_loss: 3.5337e-05 - val_mse: 3.5337e-05\n",
      "Epoch 826/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 1.5906e-05 - mse: 1.5906e-05 - val_loss: 2.0971e-05 - val_mse: 2.0971e-05\n",
      "Epoch 827/1000\n",
      "66/66 [==============================] - 0s 455us/step - loss: 1.2230e-05 - mse: 1.2230e-05 - val_loss: 3.3877e-05 - val_mse: 3.3877e-05\n",
      "Epoch 828/1000\n",
      "66/66 [==============================] - 0s 447us/step - loss: 2.1413e-05 - mse: 2.1413e-05 - val_loss: 1.6118e-05 - val_mse: 1.6118e-05\n",
      "Epoch 829/1000\n",
      "66/66 [==============================] - 0s 471us/step - loss: 1.7336e-05 - mse: 1.7336e-05 - val_loss: 4.2268e-05 - val_mse: 4.2268e-05\n",
      "Epoch 830/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 4.4504e-05 - mse: 4.4504e-05 - val_loss: 1.6904e-05 - val_mse: 1.6904e-05\n",
      "Epoch 831/1000\n",
      "66/66 [==============================] - 0s 449us/step - loss: 1.9359e-05 - mse: 1.9359e-05 - val_loss: 7.9345e-05 - val_mse: 7.9345e-05\n",
      "Epoch 832/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 3.2715e-05 - mse: 3.2715e-05 - val_loss: 1.7353e-05 - val_mse: 1.7353e-05\n",
      "Epoch 833/1000\n",
      "66/66 [==============================] - 0s 470us/step - loss: 2.1723e-05 - mse: 2.1723e-05 - val_loss: 1.0974e-05 - val_mse: 1.0974e-05\n",
      "Epoch 834/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 1.1825e-05 - mse: 1.1825e-05 - val_loss: 2.3101e-05 - val_mse: 2.3101e-05\n",
      "Epoch 835/1000\n",
      "66/66 [==============================] - 0s 463us/step - loss: 2.4789e-05 - mse: 2.4789e-05 - val_loss: 3.0482e-05 - val_mse: 3.0482e-05\n",
      "Epoch 836/1000\n",
      "66/66 [==============================] - 0s 424us/step - loss: 5.1723e-05 - mse: 5.1723e-05 - val_loss: 6.8738e-05 - val_mse: 6.8738e-05\n",
      "Epoch 837/1000\n",
      "66/66 [==============================] - 0s 440us/step - loss: 3.3737e-05 - mse: 3.3737e-05 - val_loss: 2.3051e-05 - val_mse: 2.3051e-05\n",
      "Epoch 838/1000\n",
      "66/66 [==============================] - 0s 472us/step - loss: 1.1095e-05 - mse: 1.1095e-05 - val_loss: 1.9719e-05 - val_mse: 1.9719e-05\n",
      "Epoch 839/1000\n",
      "66/66 [==============================] - 0s 439us/step - loss: 1.7062e-05 - mse: 1.7062e-05 - val_loss: 2.4771e-05 - val_mse: 2.4771e-05\n",
      "Epoch 840/1000\n",
      "66/66 [==============================] - 0s 472us/step - loss: 2.1401e-05 - mse: 2.1401e-05 - val_loss: 3.4592e-05 - val_mse: 3.4592e-05\n",
      "Epoch 841/1000\n",
      "66/66 [==============================] - 0s 463us/step - loss: 2.0946e-05 - mse: 2.0946e-05 - val_loss: 4.0879e-05 - val_mse: 4.0879e-05\n",
      "Epoch 842/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 2.3122e-05 - mse: 2.3122e-05 - val_loss: 3.6277e-05 - val_mse: 3.6277e-05\n",
      "Epoch 843/1000\n",
      "66/66 [==============================] - 0s 500us/step - loss: 3.5384e-05 - mse: 3.5384e-05 - val_loss: 6.1431e-05 - val_mse: 6.1431e-05\n",
      "Epoch 844/1000\n",
      "66/66 [==============================] - 0s 502us/step - loss: 2.1425e-05 - mse: 2.1425e-05 - val_loss: 1.5561e-05 - val_mse: 1.5561e-05\n",
      "Epoch 845/1000\n",
      "66/66 [==============================] - 0s 485us/step - loss: 2.9745e-05 - mse: 2.9745e-05 - val_loss: 3.3447e-05 - val_mse: 3.3447e-05\n",
      "Epoch 846/1000\n",
      "66/66 [==============================] - 0s 515us/step - loss: 3.3430e-05 - mse: 3.3430e-05 - val_loss: 3.1688e-05 - val_mse: 3.1688e-05\n",
      "Epoch 847/1000\n",
      "66/66 [==============================] - 0s 442us/step - loss: 2.6790e-05 - mse: 2.6790e-05 - val_loss: 3.8615e-05 - val_mse: 3.8615e-05\n",
      "Epoch 00847: early stopping\n",
      "3.444398692928157e-05\n",
      "0.09561985518554723\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build model and Predict\n",
    "B_N_hidden_nodes=\n",
    "B_learning_rate=\n",
    "B_Batch_size=\n",
    "B_epochs=\n",
    "B_momentum=\n",
    "B_nesterov=\n",
    "ANN, predict_test_scal,predict_test, history = get_result(model_type, Proc_X_train, Proc_Y_train, Proc_X_test,\n",
    "                                      B_N_hidden_nodes,B_learning_rate, B_Batch_size, B_epochs,B_momentum, B_nesterov)\n",
    "Y_test_scal = scaler_Y.transform(Y_test.values.reshape(-1,1)).reshape(-1, get_N_output(Y_test))\n",
    "MSE_R_scal = MSE(Y_test_scal, predict_test_scal)\n",
    "MSE_R = MSE(Y_test.values, predict_test)\n",
    "print(MSE_R_scal)\n",
    "print(MSE_R)\n",
    "plot_history(history)\n",
    "# save the model\n",
    "# ANN.save(r'data_analysis/saved_model/ANN.h5')\n",
    "# pickle.dump(scaler_X, open('scaler_X.pkl','wb'))\n",
    "# pickle.dump(scaler_Y, open('scaler_Y.pkl','wb'))\n",
    "\n",
    "#for i in range(len(predict_test)):\n",
    "#get_plot(Y_test.iloc[i, :], predict_test[i], i)\n",
    "\n",
    "pred_results = []\n",
    "sim_results = []\n",
    "for i in range(len(predict_test)):\n",
    "    for j in range(len(predict_test[0])):\n",
    "        sim_results.append(Y_test.iloc[i, j])\n",
    "        pred_results.append(predict_test[i][j])\n",
    "\n",
    "max_results = max(max(pred_results,sim_results))\n",
    "min_results = min(min(pred_results,sim_results))\n",
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot([min_results, max_results], [min_results, max_results],'b', linewidth=3)\n",
    "ax.scatter(pred_results, sim_results,s=10, c='r')\n",
    "ax.set_xlabel(\"Surrogate model predicted residual stress (MPa)\", fontsize=24)\n",
    "ax.set_ylabel(\"Finite element simulated residual stress (MPa)\", fontsize=24)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f6bbdb-8f77-492f-8c3e-4bc11e2f817a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
